...menustart

 - [Lexical Analysis](#c9137c1c04dbfe3f1e2cd3a2c6f56ddf)
	 - [3.1 The Role of the Lexical Analyzer](#503dc2cfa99223a98a1fa20ddd7aa67a)
		 - [3.1.1 Lexical Analysis Versus Parsing](#cb56a27f159fc337bddaa026a491cfa6)
		 - [3.1.2 Tokens, Patterns, and Lexemes](#4f54dc373bb9d6a8db7b4534ebcae01f)
		 - [3.1.3 Attributes for Tokens](#b8c4a7ce665bbd5521e7531b5f56049d)
		 - [3.1.4 Lexical Errors](#d401b0f8d4a56f8357ac6c0e053efe83)
	 - [3.2 Input Buffering](#48d05be5115ab5c35d376d2eae488b78)
		 - [3.2.1 Buffer Pairs](#b05d3f12b9ea3730efff70e2ca88648a)
		 - [3.2.2 Sentinels](#fff01af6a53288aa02eb09337c31967f)
	 - [3.3 Speci cation of Tokens](#385328dcd658028123f7add4eb75d737)
		 - [3.3.1 Strings and Languages](#74e49ac16ef34367b51793af0f048840)
		 - [3.3.2 Operations on Languages](#4cbd840ba5cb383f70147e16ae7a05de)
		 - [3.3.3 Regular Expressions](#fb4a5b3852381b6aadac0887d6ff4ae2)
		 - [3.3.4 Regular Definitions](#9a75776dfb73fcfebabbb99bf3422851)
		 - [3.3.5 Extensions of Regular Expressions](#7cedf7c26570a4666ecad1a2eb8730a0)
	 - [3.4 Recognition of Tokens](#ebea52fcade7fe28010bd99d30398b5d)

...menuend



<h2 id="c9137c1c04dbfe3f1e2cd3a2c6f56ddf"></h2>
# Lexical Analysis

 - We begin the study of lexical-analyzer generators by introducing regular expressions. 
 - We show how this notation can be transformed, 
 	- first into nondeterministic automata and then into deterministic automata. 
 - The latter two notations can be used as input to a "driver", 
 	- that is, code which simulates these automata and uses them as a guide to determining the next token.
 - This driver and the specification of the automaton form the nucleus of the lexical analyzer.

---

<h2 id="503dc2cfa99223a98a1fa20ddd7aa67a"></h2>
## 3.1 The Role of the Lexical Analyzer

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.1.png)

lexical analyzer may perform certain other tasks besides identi cation of lexemes.

 - stripping out comments and *whitespace*
 - correlating error messages generated by the compiler with the source program. 

In some compilers, the lexical analyzer makes a copy of the source program with the error messages inserted at the appropriate positions.  If the source program uses a macro-preprocessor, the expansion of macros may also be performed by the lexical analyzer.

Sometimes, lexical analyzers are divided into a cascade of two processes:

 - a) *Scanning* consists of the simple processes that do not require tokenization of the input, 
 	- such as deletion of comments and compaction of consecutive whitespace characters into one.
 - b) *Lexical analysis* proper is the more complex portion, where the scanner produces the sequence of tokens as output.

---

<h2 id="cb56a27f159fc337bddaa026a491cfa6"></h2>
### 3.1.1 Lexical Analysis Versus Parsing

There are a number of reasons why the analysis portion of a compiler is normally separated into lexical analysis and parsing (syntax analysis) phases.

 1. Simplicity of design is the most important consideration. 
 	- The separation of lexical and syntactic analysis often allows us to simplify at least one of these tasks. 
 2. Compiler efficiency is improved. 
 	- A separate lexical analyzer allows us to apply specialized techniques that serve only the lexical task, not the job of parsing. In addition, specialized buffering techniques for reading input characters can speed up the compiler significantly.
 3. Compiler portability is enhanced. 
 	- Input-device-specific peculiarities can be restricted to the lexical analyzer.

---

<h2 id="4f54dc373bb9d6a8db7b4534ebcae01f"></h2>
### 3.1.2 Tokens, Patterns, and Lexemes

When discussing lexical analysis, we use three related but distinct terms:

 - A *token* is a pair consisting of a token name and an optional attribute value. 
 	- The token name is an abstract symbol representing a kind of lexical unit, e.g., a keyword, or an identifier. 
 - A *pattern* is a description of the form that the lexemes of a token may take. 
 	- In the case of a keyword as a token, the pattern is just the sequence of characters that form the keyword. For identifiers and some other tokens, the pattern is a more complex structure that is matched by many strings.
 - A *lexeme* is a sequence of characters in the source program that matches the pattern for a token and is identified by the lexical analyzer as an instance of that token.


TOKEN | INFORMAL DESCRIPTION | SAMPLE LEXEMES
--- | --- | ---
**if** | characters i,f 	| if
**else** | characters e,l,s,e 	| else
**comparison** | < or > or <= or >= or == or != |	<=, !=
**id** | letter followed by letters and digits |  pi, score, D2
**number** | any numeric constant |	3.14159 , 6.02e23
**literal** | anything but ", surrounded by "'s  | "core dumped"

Figure 3.2: Examples of tokens

Example 3.1: 

```c
	printf("Total = %d\ ", score);
```

 - printf and score are lexemes matching the pattern for token **id**
 - "Total = %d\n" is a lexeme matching **literal**.

In many programming languages, the following classes cover most or all of the tokens:

 - One token for ***each keyword***. 
 	- The pattern for a keyword is the same as the keyword itself. 
 - Tokens for the ***operators***, either individually or in classes such as the token comparison mentioned in Fig. 3.2.
 - One token representing all ***identifiers***.
 - One or more tokens representing ***constants***, such as numbers and literal strings .
 - Tokens for each ***punctuation symbol***, such as left and right parentheses, comma, and semicolon.

---

<h2 id="b8c4a7ce665bbd5521e7531b5f56049d"></h2>
### 3.1.3 Attributes for Tokens

When more than one lexeme can match a pattern, the lexical analyzer must provide additional information about the par¬≠ticular lexeme that matched, for the subsequent compiler phases. 

For example, the pattern for token **number** matches both 0 and 1, but it is extremely important for the code generator to know which lexeme was found in the source program. 

Thus, in many cases the lexical analyzer returns to the parser not only a token name, but an attribute value that describes the lexeme represented by the tokenl

The token name in¬≠fluences parsing decisions, while the attribute value influences translation of tokens after the parse.

We shall assume that tokens have at most one associated attribute, although this attribute may have a structure that combines several pieces of information. 

The most important example is the token **id**, where we need to associate with the token a great deal of information. 

Normally, information about an identi¬≠fier -- e.g., its lexeme, its type, and the location at which it is first found  -- is kept in the symbol table. Thus, the appropriate attribute value for an identifier is a pointer to the symbol-table entry for that identifier.

Example 3.2 : The token names and associated attribute values for the For¬≠ tran statement

```
	E = M * C ** 2
```

are written below as a sequence of pairs.

```
<id, pointer to symbol-table entry for E> 
< assign_op>
<id, pointer to symbol-table entry for M> 
<mult_op>
<id, pointer to symbol-table entry for C> 
<exp_op>
<number, integer value 2>
```

 - in certain pairs, especially operators, punctuation, and keywords, there is no need for an attribute value. 
 - In this example, the token **number** has been given an integer-valued attribute. 
 	- In practice, a typical compiler would instead store a character string representing the constant and use as an attribute value for **number** a pointer to that string. 

---

<h2 id="d401b0f8d4a56f8357ac6c0e053efe83"></h2>
### 3.1.4 Lexical Errors

It is hard for a lexical analyzer to tell, without the aid of other components, that there is a source-code error. 

For instance, if the string **fi** is encountered for the first time in a C program in the context:

```c
	fi ( a == f(x)) ...
```

a lexical analyzer cannot tell whether **fi** is a misspelling of the keyword **if** or an undeclared function identifier. Since fi is a valid lexeme for the token **id**, the lexical analyzer must return the token id to the parser and let some other phase of the compiler handle an error.

However, suppose a situation arises in which the lexical analyzer is unable to proceed because none of the patterns for tokens matches any prefix of the remaining input. 

The simplest recovery strategy is "panic mode" recovery. We delete successive characters from the remaining input, until the lexical analyzer can find a well-formed token at the beginning of what input is left. 

> ‰ªéÂâ©‰∏ãÁöÑËæìÂÖ•‰∏≠,ËøûÁª≠Âà†Èô§Â≠óÁ¨¶ÔºåÁõ¥Âà∞ lexical analyzer ÊâæÂà∞‰∏Ä‰∏™ÂêàÈÄÇÁöÑ token

This recovery technique may confuse the parser, but in an interactive computing environment it may be quite adequate.

Other possible error-recovery actions are:

 1. Delete one character from the remaining input.
 2. Insert a missing character into the remaining input.
 3. Replace a character by another character.
 4. Transpose two adjacent characters.

Transformations like these may be tried in an attempt to repair the input. The simplest such strategy is to see whether a prefix of the remaining input can be transformed into a valid lexeme by a single transformation. This strategy makes sense, since in practice most lexical errors involve a single character. A more general correction strategy is to find the smallest number of transforma¬≠tions needed to convert the source program into one that consists only of valid lexemes, but this approach is considered too expensive in practice to be worth the effort.

---

<h2 id="48d05be5115ab5c35d376d2eae488b78"></h2>
## 3.2 Input Buffering

We often have to look one or more characters beyond the next lexeme before we can be sure we have the right lexeme.

We shall introduce a two-buffer scheme that handles large lookaheads safely. We then consider an improvement involving "sentinels" that saves time checking for the ends of buffers.

---

<h2 id="b05d3f12b9ea3730efff70e2ca88648a"></h2>
### 3.2.1 Buffer Pairs

Specialized buffering techniques have been developed to reduce the amount of overhead required to process a single input character.  An impor¬≠tant scheme involves two buffers that are alternately reloaded, as suggested in Fig. 3.3.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.3.png)

Each buffer is of the same size N , and N is usually the size of a disk block, e.g., 4096 bytes. 

Using one system read command we can read N characters into a buffer, rather than using one system call per character.  

If fewer than N characters remain in the input file, then a special character, represented by **eof**, marks the end of the source file and is different from any possible character of the source program.

Two pointers to the input are maintained:

 1. Pointer **lexemeBegin**
 	- marks the beginning of the current lexeme, whose extent we are attempting to determine.
 2. Pointer **forward** 
 	- scans ahead until a pattern match is found.

Once the next lexeme is determined, **forward** is set to the character at its right end. Then, after the lexeme is recorded as an attribute value of a token returned to the parser, lexemeBegin is set to the character immediately after the lexeme just found. 

Advancing **forward** requires that we first test whether we have reached the end of one of the buffers, and if so, we must reload the other buffer from the input, and move forward to the beginning of the newly loaded buffer. As long as we never need to look so far ahead of the actual lexeme that the sum of the lexeme's length plus the distance we look ahead is greater than N, we shall never overwrite the lexeme in its buffer before determining it.

---

<h2 id="fff01af6a53288aa02eb09337c31967f"></h2>
### 3.2.2 Sentinels

We must check, each time we advance forward, that we have not moved off one of the buffers; if we do, then we must also reload the other buffer. Thus, for each character read, we make two tests: 

 - one for the end of the buffer, 
 - and one to determine what character is read. 

We can combine the buffer-end test with the test for the current character if we extend each buffer to hold a sentinel character at the end. The sentinel is a special character that cannot be part of the source program, and a natural choice is the character **eof**.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.4.png)

Figure 3.4 shows the same arrangement as Fig. 3.3, but with the sentinels added. 

Note that **eof** retains its use as a marker for the end of the entire input. Any **eof** that appears other than at the end of a buffer means that the input is at an end.


```java
switch ( *forward++ ) { 
case eof:
	if (forward is at end of first buffer ) { 
		reload second buffer;
		forward = beginning of second buffer;
	}else if (forward is at end of second buffer ) { 
		reload first buffer;
		forward = beginning of first buffer;
	}else /* eof within a buffer marks the end of input */ 
		terminate lexical analysis;
	break;
Cases for the other characters
}
```

Figure 3.5: Loo head code with sentinels

--- 

<h2 id="385328dcd658028123f7add4eb75d737"></h2>
## 3.3 Speci cation of Tokens

Regular expressions are an important notation for specifying lexeme patterns. 

While they cannot express all possible patterns, regular expressions are very effective in specifying those types of patterns that we actually need for tokens.



---

<h2 id="74e49ac16ef34367b51793af0f048840"></h2>
### 3.3.1 Strings and Languages

An *alphabet* is any finite set of symbols. Typical examples of symbols are let¬≠ters, digits, and punctuation. 

The set {0, 1} is the binary alphabet. ASCII is an important example of an alphabet; it is used in many software systems. 

Unicode, which includes approximately 100,000 characters from alphabets around the world, is another important example of an alphabet.

A ***string*** over an alphabet is a finite sequence of symbols drawn from that alphabet. In language theory, the terms "sentence" and "word" are often used as synonyms for "string". The length of a string s, usually written |s| . For example, ***banana*** is a string of length six. The ***empty string***, denoted Œµ, is the string of length zero.

A *language* is any countable set of strings over some fixed alphabet. This definition is very broad. 

Abstract languages like ‚àÖ, the empty set, or {Œµ}, the set containing only the empty string, are languages under this definition. 

> A *subsequence* of s is any string formed by deleting zero or more ***not necessarily consecutive*** positions of s. For example, baan is a subsequence of banana.

If x and y are strings, then the concatenation of x and y, denoted xy, is the string formed by appending y to x. For example, if x = dog and y = house, then xy = doghouse. The empty string is the identity under concatenation; that is , for any string s, Œµs = sŒµ = s.

If we think of concatenation as a product, we can define the "exponentiation" of strings as follows. Define s‚Å∞ to be Œµ, and for all i>0, define s‚Å± to be s‚Å±‚Åª¬πs. 

Since Œµs = s, it follows that s¬π=s. Then s¬≤=ss, s¬≥=sss, and so on.

---

<h2 id="4cbd840ba5cb383f70147e16ae7a05de"></h2>
### 3.3.2 Operations on Languages

In lexical analysis, the most important operations on languages are union, con¬≠catenation, and closure, which are defined formally in Fig. 3.6. 

OPERATION | DEFINITION AND NOTATION
--- | ---
Union of L and M | L ‚à™ M = { s \| s is in L  or s is in M }
Concatenation of L and M | LM = { st \| s is in L  and t is in M }
Kleene closure of L | L<sup>*</sup>  = ‚à™<sup>‚àû</sup>`·µ¢‚Çå‚ÇÄ L‚Å±`
Positive closure of L | L‚Å∫  = ‚à™<sup>‚àû</sup>`·µ¢‚Çå‚ÇÅ L‚Å±`


---

<h2 id="fb4a5b3852381b6aadac0887d6ff4ae2"></h2>
### 3.3.3 Regular Expressions

We describe the language of C identifiers by:

```
letter_ ( letter_ | digit )*
```

 - letter_ is for any letter or the underscore

The regular expressions are built recursively out of smaller regular expres¬≠sions, using the rules described below. 

Each regular expression r denotes a language L(r), which is also defined recursively from the languages denoted by r's subexpressions. Here are the rules that define the regular expressions over some alphabet Œ£ and the languages that those expressions denote.

BASIS: There are two rules that form the basis:

 1. Œµ is a regular expression, and L(Œµ) is {Œµ}, that is, the language whose sole member is the empty string.
 2. If *a* is a symbol in Œ£ ,then **a** is a regular expression, and L(**a**) = {*a*}, that is, the language with one string, of length one, with *a* in its one position. Note that by convention, we use italics for symbols, and boldface for their corresponding regular expression.

INDUCTION: There are four parts to the induction whereby larger regular expressions are built from smaller ones. 

Suppose **r** and **s** are regular expressions denoting languages L(*r*) and L(*s*), respectively.

 1. (*r*)|(*s*) is a regular expression , denoting the language L(*r*) U L(*s*).
 2. (*r*)(*s*) is a regular expression  , denoting the language L(*r*)L(*s*).
 3. (*r*)<sup>\*</sup> is a regular expression denoting (L(*r*))<sup>\*</sup>.
 4. (*r*) is a regular expression denoting L(*r*). 
 	- This last rule says that we can add additional pairs of parentheses around expressions without changing the language they denote.

As defined, regular expressions often contain unnecessary pairs of paren¬≠theses. We may drop certain pairs of parentheses if we adopt the conventions that :

 - a) The unary operator * has highest precedence and is left associative.
 - b) Concatenation has second highest precedence and is left associative.
 - c) | has lowest precedence and is left associative.

Under these conventions, for example, we may replace the regular expression `(a)|((b)*(c))` by `a|b*c`. 

A language that can be defined by a regular expression is called a *regular set*. 

> *regular set* is a language

If two regular expressions r and s denote the same regular set, we say they are *equivalent* and write r = s. For instance, `(a|b) = (b|a)`.

LAW | DESCRIPTION
--- | ---
r\|s = s\|r  |  \| is commutative
r\|(s\|t) = (r\|s)\|t  | \| is associative
r(st) = (rs)t | Concatenation is associative
r(s\|t) = rs\|rt; (s\|t)r = sr\|tr | Concatenation distributes over \|
Œµr = rŒµ = r | Œµ is the identity for concatenation  
r<sup>* </sup> = (r\|Œµ)<sup>* </sup> | Œµ is guaranteed in a closure
r<sup>\*\*</sup> = r<sup>* </sup> | * is idempotent

Figure 3.7: Algebraic laws for regular expressions

---

<h2 id="9a75776dfb73fcfebabbb99bf3422851"></h2>
### 3.3.4 Regular Definitions

For notational convenience, we may wish to give names to certain regular ex¬≠pressions and use those names in subsequent expressions, as if the names were themselves symbols.

If Œ£ is an alphabet of basic symbols, then a *regular definition* is a sequence of definitions of the form:

```
	d‚ÇÅ ‚Üí r‚ÇÅ
	d‚ÇÇ ‚Üí r‚ÇÇ 
	  ...
	dùíè ‚Üí rùòØ
```

where:

 1. Each d·µ¢ is a new symbol, not in Œ£ and not the same as any other of the d's, and
 2. Each r·µ¢ is a regular expression over the alphabet Œ£ U{ d‚ÇÅ,d‚ÇÇ,...,d·µ¢‚Çã‚ÇÅ }¬∑

By restricting r·µ¢ to Œ£ and the previously defined d's, we avoid recursive defini¬≠tions, and we can construct a regular expression over Œ£ alone, for each r·µ¢. 

We do so by first replacing uses of d‚ÇÅ in r‚ÇÇ (which cannot use any of the d's except for d‚ÇÅ ) , then replacing uses of d‚ÇÅ and d‚ÇÇ in r‚ÇÉ by r‚ÇÅ and (the substituted) r‚ÇÇ , and so on. Finally,in rùòØ we replace each d·µ¢, for i=1,2,...,n-1, by the substituted version of r·µ¢ , each of which has only symbols of Œ£ .

Example3.5: Here is a regular definition for the language of C identifiers.

```
	letter_ ‚Üí A | B | ... | Z | a | b | ... | z | _
	  digit ‚Üí 0 | 1 | ... | 9
	  	 id ‚Üí letter_( letter_ | digit )*   
```

Example 3.6 : Unsigned numbers (integer or floating point) are strings such as 5280, 0.01234, 6.336E4, or 1.89E-4

```
	         digit ‚Üí 0 | 1 | ... | 9
        	digits ‚Üí digit digit*
  optionalFraction ‚Üí . digits | Œµ
  optionalExponent ‚Üí ( E ( +|-|Œµ ) digits ) | Œµ
  			number ‚Üí digits optionalFraction optionalExponent
```

 - this specification does not match `1.` 

---

<h2 id="7cedf7c26570a4666ecad1a2eb8730a0"></h2>
### 3.3.5 Extensions of Regular Expressions

 1. *One or more instances*. `+`
 	- The operator + has the same precedence and associativity as the operator \*
 	- Two useful algebraic laws, r\* = r+ | Œµ,   and r+ = rr\* = r\*r
 2. *Ze  or one instance*.   `?`
 	- r? is equivalent to r | Œµ
 	- L(r?) = L(r) U {c}
 	- The ? operator has the same precedence and associativity as \* and +.
 3. *Character classes*.
 	- A regular expression a‚ÇÅ|a‚ÇÇ|...|aùòØ , can be replaced by the shorthand [ a‚ÇÅa‚ÇÇ...aùòØ ]
 	- More importantly, when a‚ÇÅ,a‚ÇÇ,...,aùòØ form a *logical se¬≠quence*, we can replace them by a‚ÇÅ-aùòØ, that is, just the first and last separated by a hyphen '-'. 
 	- Thus, [abc] is shorthand for a|b|c, and [a-z] is shorthand for a|b|...|z.

Example 3.7 : Using these shorthands, we can rewrite the regular definition of Example 3.5 as:

```
	letter_ ‚Üí [A-Za-z_]
	  digit ‚Üí [0-9]
	     id ‚Üí letter_( letter | digit)*  
```

```
	 digit ‚Üí [0-9]
	digits ‚Üí digit+
	number ‚Üí digits (. digits)? ( E[+-]? digits)?
```

---

<h2 id="ebea52fcade7fe28010bd99d30398b5d"></h2>
## 3.4 Recognition of Tokens

 - study how to take the patterns for all the needed tokens 
 - and build a piece of code that examines the input string and finds a prefix that is a lexeme matching one of the patterns

Our discussion will make use of the following running example.

```
stmt ‚Üí if expr then stmt
	 | if expr then stmt else stmt 
	 | Œµ
expr ‚Üí term relop term
	 | term
term ‚Üí id
	 | number
```

Figure 3.10: A grammar for branching statements

Example 3.8 : The grammar fragment of Fig. 3.10 describes a simple form of branching statements and conditional expressions. This syntax is similar to that of the language Pascal, in that **then** appears explicitly after conditions.

For **relop**, we use the comparison operators of languages like Pascal or SQL, where = is "equals" and <> is "not equals", because it presents an interesting structure of lexemes.

The terminals of the grammar, which are **if**, **then**, **else**, **relop**, **id**, and **number** , are the names of tokens as far as the lexical analyzer is concerned. 

The patterns for these tokens are described using regular definitions, as in Fig. 3.11 . The patterns for ***id*** and ***number*** are similar to what we saw in Example 3.7.

```
	digit ‚Üí [0-9]
   digits ‚Üí digit+
   number ‚Üí digits (. digits)? (E [+-]? digits)?
   letter ‚Üí [A-Za-z]
   	   id ‚Üí letter( letter | digits)*
   	   if ‚Üí if
   	 then ‚Üí then
   	 else ‚Üí else
   	relop ‚Üí < | > | <= | >= | = | <>
```

Figure 3.11: Patterns for tokens of Example 3.8

To simplify matters, we make the common assumption that keywords are also *reserved words*: that is, they are not identifiers, even though their lexemes match the pattern for identifiers.

In addition, we assign the lexical analyzer the job of stripping out white¬≠ space, by recognizing the "token" *ws* defined by:

```
	ws ‚Üí ( black | tab | newline)+
```

Token *ws* is different from the other tokens in that,  we do not return it to the parser.

Our goal for the lexical analyzer is summarized in Fig. 3.12. 

LEXEMES | TOKEN NAME | ATTRIBUTE VALUE
--- | --- | ---
Any ws | - | -
if | **if** | -
then | **then** | -
else | **else** | -
Any id | **id** | Pointer to table entry
Any number | **number** | Pointer to table entry
< | **relop** | LT
<= | **relop** | LE
= | **relop** | EQ
<> | **relop** | NE
> | **relop** | GT
>= | **relop** | GE

Figure 3.12: Tokens, their patterns, and attribute values

---

### 3.4.1 Transition Diagrams

As an intermediate step in the construction of a lexical analyzer, we first convert patterns into stylized flowcharts, called "*transition diagrams*". 

In this section, we perform the conversion from regular-expression patterns to transition dia¬≠grams by hand, but in Section 3.6, we shall see that there is a mechanical way to construct these diagrams from collections of regular expressions.

*Transition diagrams* have a collection of nodes or circles, called *states*. Each state represents a condition that could occur during the process of scanning the input looking for a lexeme that matches one of several patterns. We may think of a state as summarizing all we need to know about what characters we have seen between the *lexemeBegin* pointer and the *forward* pointer (as in the situation of Fig. 3.3).

*Edges* are directed from one state of the transition diagram to another. Each edge is *labeled* by a symbol or set of symbols. If we are in some state s, and the next input symbol is a, we look for an edge out of state s labeled by a (and perhaps by other symbols, as well).   

If we find such an edge, we advance the *forward* pointer and enter the state of the transition diagram to which that edge leads. We shall assume that all our transition diagrams are *deterministic*, meaning that there is never more than one edge out of a given state with a given symbol among its labels. 

Starting in Section 3.5, we shall relax the condition of determinism, making life much easier for the designer of a lexical analyzer, although trickier for the implementer. Some important conventions about transition diagrams are:

 1. Certain states are said to be *accepting*, of *final*. 
 	- These states indicate that a lexeme has been found, although the actual lexeme may not consist of all positions between the *lexemeBegin* and *forward* pointers. We always indicate an accepting state by a double circle, and if there is an action to be taken -- typically returning a token and an attribute value to the parser -- we shall attach that action to the accepting state.
 2. In addition, if it is necessary to retract the *forward* pointer one position (i.e., the lexeme does not include the symbol that got us to the accepting state) , then we shall additionally place a \* near that accepting state. 
 	- In our example, it is never necessary to retract forward by more than one position, but if it were, we could attach any number of \* 's to the accepting state.
 

 



...menustart

 - [Lexical Analysis](#c9137c1c04dbfe3f1e2cd3a2c6f56ddf)
	 - [3.1 The Role of the Lexical Analyzer](#503dc2cfa99223a98a1fa20ddd7aa67a)
		 - [3.1.1 Lexical Analysis Versus Parsing](#cb56a27f159fc337bddaa026a491cfa6)
		 - [3.1.2 Tokens, Patterns, and Lexemes](#4f54dc373bb9d6a8db7b4534ebcae01f)
		 - [3.1.3 Attributes for Tokens](#b8c4a7ce665bbd5521e7531b5f56049d)
		 - [3.1.4 Lexical Errors](#d401b0f8d4a56f8357ac6c0e053efe83)
	 - [3.2 Input Buffering](#48d05be5115ab5c35d376d2eae488b78)
		 - [3.2.1 Buffer Pairs](#b05d3f12b9ea3730efff70e2ca88648a)
		 - [3.2.2 Sentinels](#fff01af6a53288aa02eb09337c31967f)
	 - [3.3 Speci cation of Tokens](#385328dcd658028123f7add4eb75d737)
		 - [3.3.1 Strings and Languages](#74e49ac16ef34367b51793af0f048840)
		 - [3.3.2 Operations on Languages](#4cbd840ba5cb383f70147e16ae7a05de)
		 - [3.3.3 Regular Expressions](#fb4a5b3852381b6aadac0887d6ff4ae2)
		 - [3.3.4 Regular Definitions](#9a75776dfb73fcfebabbb99bf3422851)
		 - [3.3.5 Extensions of Regular Expressions](#7cedf7c26570a4666ecad1a2eb8730a0)
	 - [3.4 Recognition of Tokens](#ebea52fcade7fe28010bd99d30398b5d)
		 - [3.4.1 Transition Diagrams](#0503745a655577520a59327358161a9f)
		 - [3.4.2 Recognition of Reserved Words and Identifiers](#ac3ab235083c381748fdce42a4c1bb54)
		 - [3.4.3 Completion of the Running Example](#97611c49db1511feaedefc22ac6ada49)
		 - [3.4.4 Architecture of a Transition-Diagram-Based Lexical Analyzer](#d0fb312e375ee37b45a48c9c487d131a)
	 - [3.5 The Lexical-Analyzer Generator Lex](#921b7eb0390c2aa2b73abf6edb74ab44)
		 - [3.5.1 Use of Lex](#d96ef03e4ef48fad67844a05f42082c8)
		 - [3.5.2 Structure of Lex Programs](#657b471d853e0944613b1fc41f7988bd)
		 - [3.5.3 Conflict Resolution in Lex](#cab9b67a202a7627adfcf2587ce702df)
		 - [3.5.4 The Lookahead Operator](#f57723412a7038cb7392678e61a02b76)
	 - [3.6 Finite Automata](#adab9833ab2dcc6edab8a39432584e49)
		 - [3.6.1 Nondeterministic Finite Automata](#a46bbbffe97f741d7522f9d7eba5483b)
		 - [3.6.2 Transition Tables](#2ebef3028849a4dc0909d534069b2316)
		 - [3.6.3 Acceptance of Input Strings by Automata](#7de19e880b9825779ec50e33458727d7)
		 - [3.6.4 Deterministic Finite Automata](#4870f1974b0dd964c769b59b181d7226)

...menuend



<h2 id="c9137c1c04dbfe3f1e2cd3a2c6f56ddf"></h2>
# Lexical Analysis

 - We begin the study of lexical-analyzer generators by introducing regular expressions. 
 - We show how this notation can be transformed, 
 	- first into nondeterministic automata and then into deterministic automata. 
 - The latter two notations can be used as input to a "driver", 
 	- that is, code which simulates these automata and uses them as a guide to determining the next token.
 - This driver and the specification of the automaton form the nucleus of the lexical analyzer.

---

<h2 id="503dc2cfa99223a98a1fa20ddd7aa67a"></h2>
## 3.1 The Role of the Lexical Analyzer

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.1.png)

lexical analyzer may perform certain other tasks besides identi cation of lexemes.

 - stripping out comments and *whitespace*
 - correlating error messages generated by the compiler with the source program. 

In some compilers, the lexical analyzer makes a copy of the source program with the error messages inserted at the appropriate positions.  If the source program uses a macro-preprocessor, the expansion of macros may also be performed by the lexical analyzer.

Sometimes, lexical analyzers are divided into a cascade of two processes:

 - a) *Scanning* consists of the simple processes that do not require tokenization of the input, 
 	- such as deletion of comments and compaction of consecutive whitespace characters into one.
 - b) *Lexical analysis* proper is the more complex portion, where the scanner produces the sequence of tokens as output.

---

<h2 id="cb56a27f159fc337bddaa026a491cfa6"></h2>
### 3.1.1 Lexical Analysis Versus Parsing

There are a number of reasons why the analysis portion of a compiler is normally separated into lexical analysis and parsing (syntax analysis) phases.

 1. Simplicity of design is the most important consideration. 
 	- The separation of lexical and syntactic analysis often allows us to simplify at least one of these tasks. 
 2. Compiler efficiency is improved. 
 	- A separate lexical analyzer allows us to apply specialized techniques that serve only the lexical task, not the job of parsing. In addition, specialized buffering techniques for reading input characters can speed up the compiler significantly.
 3. Compiler portability is enhanced. 
 	- Input-device-specific peculiarities can be restricted to the lexical analyzer.

---

<h2 id="4f54dc373bb9d6a8db7b4534ebcae01f"></h2>
### 3.1.2 Tokens, Patterns, and Lexemes

When discussing lexical analysis, we use three related but distinct terms:

 - A *token* is a pair consisting of a token name and an optional attribute value. 
 	- The token name is an abstract symbol representing a kind of lexical unit, e.g., a keyword, or an identifier. 
 - A *pattern* is a description of the form that the lexemes of a token may take. 
 	- In the case of a keyword as a token, the pattern is just the sequence of characters that form the keyword. For identifiers and some other tokens, the pattern is a more complex structure that is matched by many strings.
 - A *lexeme* is a sequence of characters in the source program that matches the pattern for a token and is identified by the lexical analyzer as an instance of that token.


TOKEN | INFORMAL DESCRIPTION | SAMPLE LEXEMES
--- | --- | ---
**if** | characters i,f 	| if
**else** | characters e,l,s,e 	| else
**comparison** | < or > or <= or >= or == or != |	<=, !=
**id** | letter followed by letters and digits |  pi, score, D2
**number** | any numeric constant |	3.14159 , 6.02e23
**literal** | anything but ", surrounded by "'s  | "core dumped"

Figure 3.2: Examples of tokens

Example 3.1: 

```c
	printf("Total = %d\ ", score);
```

 - printf and score are lexemes matching the pattern for token **id**
 - "Total = %d\n" is a lexeme matching **literal**.

In many programming languages, the following classes cover most or all of the tokens:

 - One token for ***each keyword***. 
 	- The pattern for a keyword is the same as the keyword itself. 
 - Tokens for the ***operators***, either individually or in classes such as the token comparison mentioned in Fig. 3.2.
 - One token representing all ***identifiers***.
 - One or more tokens representing ***constants***, such as numbers and literal strings .
 - Tokens for each ***punctuation symbol***, such as left and right parentheses, comma, and semicolon.

---

<h2 id="b8c4a7ce665bbd5521e7531b5f56049d"></h2>
### 3.1.3 Attributes for Tokens

When more than one lexeme can match a pattern, the lexical analyzer must provide additional information about the par¬≠ticular lexeme that matched, for the subsequent compiler phases. 

For example, the pattern for token **number** matches both 0 and 1, but it is extremely important for the code generator to know which lexeme was found in the source program. 

Thus, in many cases the lexical analyzer returns to the parser not only a token name, but an attribute value that describes the lexeme represented by the tokenl

The token name in¬≠fluences parsing decisions, while the attribute value influences translation of tokens after the parse.

We shall assume that tokens have at most one associated attribute, although this attribute may have a structure that combines several pieces of information. 

The most important example is the token **id**, where we need to associate with the token a great deal of information. 

Normally, information about an identi¬≠fier -- e.g., its lexeme, its type, and the location at which it is first found  -- is kept in the symbol table. Thus, the appropriate attribute value for an identifier is a pointer to the symbol-table entry for that identifier.

Example 3.2 : The token names and associated attribute values for the For¬≠ tran statement

```
	E = M * C ** 2
```

are written below as a sequence of pairs.

```
<id, pointer to symbol-table entry for E> 
< assign_op>
<id, pointer to symbol-table entry for M> 
<mult_op>
<id, pointer to symbol-table entry for C> 
<exp_op>
<number, integer value 2>
```

 - in certain pairs, especially operators, punctuation, and keywords, there is no need for an attribute value. 
 - In this example, the token **number** has been given an integer-valued attribute. 
 	- In practice, a typical compiler would instead store a character string representing the constant and use as an attribute value for **number** a pointer to that string. 

---

<h2 id="d401b0f8d4a56f8357ac6c0e053efe83"></h2>
### 3.1.4 Lexical Errors

It is hard for a lexical analyzer to tell, without the aid of other components, that there is a source-code error. 

For instance, if the string **fi** is encountered for the first time in a C program in the context:

```c
	fi ( a == f(x)) ...
```

a lexical analyzer cannot tell whether **fi** is a misspelling of the keyword **if** or an undeclared function identifier. Since fi is a valid lexeme for the token **id**, the lexical analyzer must return the token id to the parser and let some other phase of the compiler handle an error.

However, suppose a situation arises in which the lexical analyzer is unable to proceed because none of the patterns for tokens matches any prefix of the remaining input. 

The simplest recovery strategy is "panic mode" recovery. We delete successive characters from the remaining input, until the lexical analyzer can find a well-formed token at the beginning of what input is left. 

> ‰ªéÂâ©‰∏ãÁöÑËæìÂÖ•‰∏≠,ËøûÁª≠Âà†Èô§Â≠óÁ¨¶ÔºåÁõ¥Âà∞ lexical analyzer ÊâæÂà∞‰∏Ä‰∏™ÂêàÈÄÇÁöÑ token

This recovery technique may confuse the parser, but in an interactive computing environment it may be quite adequate.

Other possible error-recovery actions are:

 1. Delete one character from the remaining input.
 2. Insert a missing character into the remaining input.
 3. Replace a character by another character.
 4. Transpose two adjacent characters.

Transformations like these may be tried in an attempt to repair the input. The simplest such strategy is to see whether a prefix of the remaining input can be transformed into a valid lexeme by a single transformation. This strategy makes sense, since in practice most lexical errors involve a single character. A more general correction strategy is to find the smallest number of transforma¬≠tions needed to convert the source program into one that consists only of valid lexemes, but this approach is considered too expensive in practice to be worth the effort.

---

<h2 id="48d05be5115ab5c35d376d2eae488b78"></h2>
## 3.2 Input Buffering

We often have to look one or more characters beyond the next lexeme before we can be sure we have the right lexeme.

We shall introduce a two-buffer scheme that handles large lookaheads safely. We then consider an improvement involving "sentinels" that saves time checking for the ends of buffers.

---

<h2 id="b05d3f12b9ea3730efff70e2ca88648a"></h2>
### 3.2.1 Buffer Pairs

Specialized buffering techniques have been developed to reduce the amount of overhead required to process a single input character.  An impor¬≠tant scheme involves two buffers that are alternately reloaded, as suggested in Fig. 3.3.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.3.png)

Each buffer is of the same size N , and N is usually the size of a disk block, e.g., 4096 bytes. 

Using one system read command we can read N characters into a buffer, rather than using one system call per character.  

If fewer than N characters remain in the input file, then a special character, represented by **eof**, marks the end of the source file and is different from any possible character of the source program.

Two pointers to the input are maintained:

 1. Pointer **lexemeBegin**
 	- marks the beginning of the current lexeme, whose extent we are attempting to determine.
 2. Pointer **forward** 
 	- scans ahead until a pattern match is found.

Once the next lexeme is determined, **forward** is set to the character at its right end. Then, after the lexeme is recorded as an attribute value of a token returned to the parser, lexemeBegin is set to the character immediately after the lexeme just found. 

Advancing **forward** requires that we first test whether we have reached the end of one of the buffers, and if so, we must reload the other buffer from the input, and move forward to the beginning of the newly loaded buffer. As long as we never need to look so far ahead of the actual lexeme that the sum of the lexeme's length plus the distance we look ahead is greater than N, we shall never overwrite the lexeme in its buffer before determining it.

---

<h2 id="fff01af6a53288aa02eb09337c31967f"></h2>
### 3.2.2 Sentinels

We must check, each time we advance forward, that we have not moved off one of the buffers; if we do, then we must also reload the other buffer. Thus, for each character read, we make two tests: 

 - one for the end of the buffer, 
 - and one to determine what character is read. 

We can combine the buffer-end test with the test for the current character if we extend each buffer to hold a sentinel character at the end. The sentinel is a special character that cannot be part of the source program, and a natural choice is the character **eof**.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.4.png)

Figure 3.4 shows the same arrangement as Fig. 3.3, but with the sentinels added. 

Note that **eof** retains its use as a marker for the end of the entire input. Any **eof** that appears other than at the end of a buffer means that the input is at an end.


```java
switch ( *forward++ ) { 
case eof:
	if (forward is at end of first buffer ) { 
		reload second buffer;
		forward = beginning of second buffer;
	}else if (forward is at end of second buffer ) { 
		reload first buffer;
		forward = beginning of first buffer;
	}else /* eof within a buffer marks the end of input */ 
		terminate lexical analysis;
	break;
Cases for the other characters
}
```

Figure 3.5: Loo head code with sentinels

--- 

<h2 id="385328dcd658028123f7add4eb75d737"></h2>
## 3.3 Speci cation of Tokens

Regular expressions are an important notation for specifying lexeme patterns. 

While they cannot express all possible patterns, regular expressions are very effective in specifying those types of patterns that we actually need for tokens.



---

<h2 id="74e49ac16ef34367b51793af0f048840"></h2>
### 3.3.1 Strings and Languages

An *alphabet* is any finite set of symbols. Typical examples of symbols are let¬≠ters, digits, and punctuation. 

The set {0, 1} is the binary alphabet. ASCII is an important example of an alphabet; it is used in many software systems. 

Unicode, which includes approximately 100,000 characters from alphabets around the world, is another important example of an alphabet.

A ***string*** over an alphabet is a finite sequence of symbols drawn from that alphabet. In language theory, the terms "sentence" and "word" are often used as synonyms for "string". The length of a string s, usually written |s| . For example, ***banana*** is a string of length six. The ***empty string***, denoted Œµ, is the string of length zero.

A *language* is any countable set of strings over some fixed alphabet. This definition is very broad. 

Abstract languages like ‚àÖ, the empty set, or {Œµ}, the set containing only the empty string, are languages under this definition. 

> A *subsequence* of s is any string formed by deleting zero or more ***not necessarily consecutive*** positions of s. For example, baan is a subsequence of banana.

If x and y are strings, then the concatenation of x and y, denoted xy, is the string formed by appending y to x. For example, if x = dog and y = house, then xy = doghouse. The empty string is the identity under concatenation; that is , for any string s, Œµs = sŒµ = s.

If we think of concatenation as a product, we can define the "exponentiation" of strings as follows. Define s‚Å∞ to be Œµ, and for all i>0, define s‚Å± to be s‚Å±‚Åª¬πs. 

Since Œµs = s, it follows that s¬π=s. Then s¬≤=ss, s¬≥=sss, and so on.

---

<h2 id="4cbd840ba5cb383f70147e16ae7a05de"></h2>
### 3.3.2 Operations on Languages

In lexical analysis, the most important operations on languages are union, con¬≠catenation, and closure, which are defined formally in Fig. 3.6. 

OPERATION | DEFINITION AND NOTATION
--- | ---
Union of L and M | L ‚à™ M = { s \| s is in L  or s is in M }
Concatenation of L and M | LM = { st \| s is in L  and t is in M }
Kleene closure of L | L<sup>*</sup>  = ‚à™<sup>‚àû</sup>`·µ¢‚Çå‚ÇÄ L‚Å±`
Positive closure of L | L‚Å∫  = ‚à™<sup>‚àû</sup>`·µ¢‚Çå‚ÇÅ L‚Å±`


---

<h2 id="fb4a5b3852381b6aadac0887d6ff4ae2"></h2>
### 3.3.3 Regular Expressions

We describe the language of C identifiers by:

```
letter_ ( letter_ | digit )*
```

 - letter_ is for any letter or the underscore

The regular expressions are built recursively out of smaller regular expres¬≠sions, using the rules described below. 

Each regular expression r denotes a language L(r), which is also defined recursively from the languages denoted by r's subexpressions. Here are the rules that define the regular expressions over some alphabet Œ£ and the languages that those expressions denote.

BASIS: There are two rules that form the basis:

 1. Œµ is a regular expression, and L(Œµ) is {Œµ}, that is, the language whose sole member is the empty string.
 2. If *a* is a symbol in Œ£ ,then **a** is a regular expression, and L(**a**) = {*a*}, that is, the language with one string, of length one, with *a* in its one position. Note that by convention, we use italics for symbols, and boldface for their corresponding regular expression.

INDUCTION: There are four parts to the induction whereby larger regular expressions are built from smaller ones. 

Suppose **r** and **s** are regular expressions denoting languages L(*r*) and L(*s*), respectively.

 1. (*r*)|(*s*) is a regular expression , denoting the language L(*r*) U L(*s*).
 2. (*r*)(*s*) is a regular expression  , denoting the language L(*r*)L(*s*).
 3. (*r*)<sup>\*</sup> is a regular expression denoting (L(*r*))<sup>\*</sup>.
 4. (*r*) is a regular expression denoting L(*r*). 
 	- This last rule says that we can add additional pairs of parentheses around expressions without changing the language they denote.

As defined, regular expressions often contain unnecessary pairs of paren¬≠theses. We may drop certain pairs of parentheses if we adopt the conventions that :

 - a) The unary operator * has highest precedence and is left associative.
 - b) Concatenation has second highest precedence and is left associative.
 - c) | has lowest precedence and is left associative.

Under these conventions, for example, we may replace the regular expression `(a)|((b)*(c))` by `a|b*c`. 

A language that can be defined by a regular expression is called a *regular set*. 

> *regular set* is a language

If two regular expressions r and s denote the same regular set, we say they are *equivalent* and write r = s. For instance, `(a|b) = (b|a)`.

LAW | DESCRIPTION
--- | ---
r\|s = s\|r  |  \| is commutative
r\|(s\|t) = (r\|s)\|t  | \| is associative
r(st) = (rs)t | Concatenation is associative
r(s\|t) = rs\|rt; (s\|t)r = sr\|tr | Concatenation distributes over \|
Œµr = rŒµ = r | Œµ is the identity for concatenation  
r<sup>* </sup> = (r\|Œµ)<sup>* </sup> | Œµ is guaranteed in a closure
r<sup>\*\*</sup> = r<sup>* </sup> | * is idempotent

Figure 3.7: Algebraic laws for regular expressions

---

<h2 id="9a75776dfb73fcfebabbb99bf3422851"></h2>
### 3.3.4 Regular Definitions

For notational convenience, we may wish to give names to certain regular ex¬≠pressions and use those names in subsequent expressions, as if the names were themselves symbols.

If Œ£ is an alphabet of basic symbols, then a *regular definition* is a sequence of definitions of the form:

```
	d‚ÇÅ ‚Üí r‚ÇÅ
	d‚ÇÇ ‚Üí r‚ÇÇ 
	  ...
	dùíè ‚Üí rùòØ
```

where:

 1. Each d·µ¢ is a new symbol, not in Œ£ and not the same as any other of the d's, and
 2. Each r·µ¢ is a regular expression over the alphabet Œ£ U{ d‚ÇÅ,d‚ÇÇ,...,d·µ¢‚Çã‚ÇÅ }¬∑

By restricting r·µ¢ to Œ£ and the previously defined d's, we avoid recursive defini¬≠tions, and we can construct a regular expression over Œ£ alone, for each r·µ¢. 

We do so by first replacing uses of d‚ÇÅ in r‚ÇÇ (which cannot use any of the d's except for d‚ÇÅ ) , then replacing uses of d‚ÇÅ and d‚ÇÇ in r‚ÇÉ by r‚ÇÅ and (the substituted) r‚ÇÇ , and so on. Finally,in rùòØ we replace each d·µ¢, for i=1,2,...,n-1, by the substituted version of r·µ¢ , each of which has only symbols of Œ£ .

Example3.5: Here is a regular definition for the language of C identifiers.

```
	letter_ ‚Üí A | B | ... | Z | a | b | ... | z | _
	  digit ‚Üí 0 | 1 | ... | 9
	  	 id ‚Üí letter_( letter_ | digit )*   
```

Example 3.6 : Unsigned numbers (integer or floating point) are strings such as 5280, 0.01234, 6.336E4, or 1.89E-4

```
	         digit ‚Üí 0 | 1 | ... | 9
        	digits ‚Üí digit digit*
  optionalFraction ‚Üí . digits | Œµ
  optionalExponent ‚Üí ( E ( +|-|Œµ ) digits ) | Œµ
  			number ‚Üí digits optionalFraction optionalExponent
```

 - this specification does not match `1.` 

---

<h2 id="7cedf7c26570a4666ecad1a2eb8730a0"></h2>
### 3.3.5 Extensions of Regular Expressions

 1. *One or more instances*. `+`
 	- The operator + has the same precedence and associativity as the operator \*
 	- Two useful algebraic laws, r\* = r+ | Œµ,   and r+ = rr\* = r\*r
 2. *Ze  or one instance*.   `?`
 	- r? is equivalent to r | Œµ
 	- L(r?) = L(r) U {c}
 	- The ? operator has the same precedence and associativity as \* and +.
 3. *Character classes*.
 	- A regular expression a‚ÇÅ|a‚ÇÇ|...|aùòØ , can be replaced by the shorthand [ a‚ÇÅa‚ÇÇ...aùòØ ]
 	- More importantly, when a‚ÇÅ,a‚ÇÇ,...,aùòØ form a *logical se¬≠quence*, we can replace them by a‚ÇÅ-aùòØ, that is, just the first and last separated by a hyphen '-'. 
 	- Thus, [abc] is shorthand for a|b|c, and [a-z] is shorthand for a|b|...|z.

Example 3.7 : Using these shorthands, we can rewrite the regular definition of Example 3.5 as:

```
	letter_ ‚Üí [A-Za-z_]
	  digit ‚Üí [0-9]
	     id ‚Üí letter_( letter | digit)*  
```

```
	 digit ‚Üí [0-9]
	digits ‚Üí digit+
	number ‚Üí digits (. digits)? ( E[+-]? digits)?
```

---

<h2 id="ebea52fcade7fe28010bd99d30398b5d"></h2>
## 3.4 Recognition of Tokens

 - study how to take the patterns for all the needed tokens 
 - and build a piece of code that examines the input string and finds a prefix that is a lexeme matching one of the patterns

Our discussion will make use of the following running example.

```
stmt ‚Üí if expr then stmt
	 | if expr then stmt else stmt 
	 | Œµ
expr ‚Üí term relop term
	 | term
term ‚Üí id
	 | number
```

Figure 3.10: A grammar for branching statements

Example 3.8 : The grammar fragment of Fig. 3.10 describes a simple form of branching statements and conditional expressions. This syntax is similar to that of the language Pascal, in that **then** appears explicitly after conditions.

For **relop**, we use the comparison operators of languages like Pascal or SQL, where = is "equals" and <> is "not equals", because it presents an interesting structure of lexemes.

The terminals of the grammar, which are **if**, **then**, **else**, **relop**, **id**, and **number** , are the names of tokens as far as the lexical analyzer is concerned. 

The patterns for these tokens are described using regular definitions, as in Fig. 3.11 . The patterns for ***id*** and ***number*** are similar to what we saw in Example 3.7.

```
	digit ‚Üí [0-9]
   digits ‚Üí digit+
   number ‚Üí digits (. digits)? (E [+-]? digits)?
   letter ‚Üí [A-Za-z]
   	   id ‚Üí letter( letter | digits)*
   	   if ‚Üí if
   	 then ‚Üí then
   	 else ‚Üí else
   	relop ‚Üí < | > | <= | >= | = | <>
```

Figure 3.11: Patterns for tokens of Example 3.8

To simplify matters, we make the common assumption that keywords are also *reserved words*: that is, they are not identifiers, even though their lexemes match the pattern for identifiers.

In addition, we assign the lexical analyzer the job of stripping out white¬≠ space, by recognizing the "token" *ws* defined by:

```
	ws ‚Üí ( black | tab | newline)+
```

Token *ws* is different from the other tokens in that,  we do not return it to the parser.

Our goal for the lexical analyzer is summarized in Fig. 3.12. 

LEXEMES | TOKEN NAME | ATTRIBUTE VALUE
--- | --- | ---
Any ws | - | -
if | **if** | -
then | **then** | -
else | **else** | -
Any id | **id** | Pointer to table entry
Any number | **number** | Pointer to table entry
< | **relop** | LT
<= | **relop** | LE
= | **relop** | EQ
<> | **relop** | NE
> | **relop** | GT
>= | **relop** | GE

Figure 3.12: Tokens, their patterns, and attribute values

---

<h2 id="0503745a655577520a59327358161a9f"></h2>
### 3.4.1 Transition Diagrams

As an intermediate step in the construction of a lexical analyzer, we first convert patterns into stylized flowcharts, called "*transition diagrams*". 

In this section, we perform the conversion from regular-expression patterns to transition dia¬≠grams by hand, but in Section 3.6, we shall see that there is a mechanical way to construct these diagrams from collections of regular expressions.

*Transition diagrams* have a collection of nodes or circles, called *states*. Each state represents a condition that could occur during the process of scanning the input looking for a lexeme that matches one of several patterns. We may think of a state as summarizing all we need to know about what characters we have seen between the *lexemeBegin* pointer and the *forward* pointer (as in the situation of Fig. 3.3).

*Edges* are directed from one state of the transition diagram to another. Each edge is *labeled* by a symbol or set of symbols. If we are in some state s, and the next input symbol is a, we look for an edge out of state s labeled by a (and perhaps by other symbols, as well).   

If we find such an edge, we advance the *forward* pointer and enter the state of the transition diagram to which that edge leads. We shall assume that all our transition diagrams are *deterministic*, meaning that there is never more than one edge out of a given state with a given symbol among its labels. 

Starting in Section 3.5, we shall relax the condition of determinism, making life much easier for the designer of a lexical analyzer, although trickier for the implementer. Some important conventions about transition diagrams are:

 1. Certain states are said to be *accepting*, of *final*. 
 	- These states indicate that a lexeme has been found, although the actual lexeme may not consist of all positions between the *lexemeBegin* and *forward* pointers. We always indicate an accepting state by a double circle, and if there is an action to be taken -- typically returning a token and an attribute value to the parser -- we shall attach that action to the accepting state.
 2. In addition, if it is necessary to retract the *forward* pointer one position (i.e., the lexeme does not include the symbol that got us to the accepting state) , then we shall additionally place a \* near that accepting state. 
 	- In our example, it is never necessary to retract forward by more than one position, but if it were, we could attach any number of \* 's to the accepting state.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.13.png)

Example 3.9 : Figure 3.13 is a transition diagram that recognizes the lexemes matching the token **relop**. 

 - We begin in state 0, the start state.   
 	- we see < as the first input symbol, then among the lexemes that match the pattern for **relop** we can only be looking at <, <>, or <=. 
 - We therefore go to state 1, and look at the next character. 
 	- If it is =, then we recognize lexeme <=, enter state 2, and return the token **relop** with attribute LE, the symbolic constant representing this particular comparison operator. 
 	- If in state 1 the next character is > , then instead we have lexeme <>, and enter state 3 to return an indication that the not-equals operator has been found. 
 	- On any other character, the lexeme is <, and we enter state 4 to return that information. 
 		- Note, however, *that state 4 has a * to indicate that we must retract the input one position*. 
 - On the other hand, if in state 0 the first character we see is =, then this one character must be the lexeme. We immediately return that fact from state 5.
 - The remaining possibility is that the first character is >. Then, we must enter state 6 and decide whether the lexeme is >= , or just > . 
 	- Note that if, in state 0, we see any character besides <, =, or >, we can not possibly be seeing a **relop** lexeme, so this transition diagram will not be used. 


---

<h2 id="ac3ab235083c381748fdce42a4c1bb54"></h2>
### 3.4.2 Recognition of Reserved Words and Identifiers

Recognizing keywords and identifiers presents a problem. Usually, keywords like **if** or **then** are reserved, so they are not identifiers even though they look like identifiers. Thus, although we typically use a transition diagram like that of Fig. 3.14 to search for identifier lexemes, this diagram will also recognize the keywords **if** , **then**, and **else** of our running example.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.14.png)

There are two ways that we can handle reserved words that look like iden¬≠tifiers :

 1. Install the reserved words in the symbol table initially. 
 	- A field of the symbol-table entry indicates that these strings are never ordinary identi¬≠fiers, and tells which token they represent. 
 	- We have supposed that this method is in use in Fig. 3.14. When we find an identifier, a call to *installID* places it in the symbol table if it is not already there and returns a pointer to the symbol-table entry for the lexeme found. 
 2. Create separate transition diagrams for each keyword; 
 	- an example for the keyword **then** is shown in Fig. 3.15. 
 	- ![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.15.png)
 	- Note that such a transition diagram consists of states representing the situation after each successive letter of the keyword is seen, followed by a test for a "nonletter-or-digit". 


---

<h2 id="97611c49db1511feaedefc22ac6ada49"></h2>
### 3.4.3 Completion of the Running Example

The transition diagram for **id**'s that we saw in Fig. 3.14 has a simple structure. 

When we first encounter anything but a letter or digit, we go to state 11 and accept the lexeme found. Since the last character is not part of the identifier, we must retract the input one position , we enter what we have found in the symbol table and determine whether we have a keyword or a true identifier.

The transition diagram for token **number** is shown in Fig. 3.16, and is so far the most complex diagram we have seen. 

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.16.png)

 - Beginning in state 12, if we see a digit, we go to state 13. In that state, we can read any number of additional digits. 
 - However, if we see anything but a digit or a dot, we have seen a number in the form of an integer; 
 	- 123 is an example. 
 	- That case is handled by entering state 20, where we return token **number** and a pointer to a table of constants where the found lexeme is entered. These mechanics are not shown on the diagram but are analogous to the way we handled identifiers.

The final transition diagram, shown in Fig. 3.17, is for whitespace. 

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.17.png)

In that diagram, we look for one or more "whitespace" characters, represented by delim in that diagram -- typically these characters would be blank, tab, newline, and perhaps other characters that are not considered by the language design to be part of any token.

Note that in state 24, we have found a block of consecutive whitespace characters, followed by a nonwhitespace character. We retract the input to begin at the nonwhitespace, but we do not return to the parser. Rather, we must restart the process of lexical analysis after the whitespace.

---

<h2 id="d0fb312e375ee37b45a48c9c487d131a"></h2>
### 3.4.4 Architecture of a Transition-Diagram-Based Lexical Analyzer

There are several ways that a collection of transition diagrams can be used to build a lexical analyzer. 

Each state is represented by a piece of code. We may imagine a variable state holding the number of the current state for a transition diagram. A switch based on the value of state takes us to code for each of the possible states, where we find the action of that state. Often, the code for a state is itself a switch statement or multiway branch that determines the next state by reading and examining the next input character.

```
TOKEN getRelop() {
	TOKEN retToken = new(RELOP) ;
	while(1) {  /* repeat character processing until 
				a return or failure occurs */ 
	switch(state) {
		case 0: 
			c = nextChar();
			if(c =='<')state=1;
			elseif(c =='=')state=5;
			else if ( c == '>' ) state = 6;
			/* lexeme is not a relop */
			else fail() ; 
			break;
		case 1: ...
		...
		case 8: 
			retract();
			retToken.attribute = GT;
			return(retToken);

	} // end switch
	} // end while
}
```

Figure 3.18: Sketch of implementation of **relop** transition diagram


Example 3.10 : In Fig. 3.18 we see a sketch of getRelop() , a C++ function whose job is to simulate the transition diagram of Fig. 3.13 and return an object of type TOKEN, that is, a pair consisting of the token name (**relop** in this case) and an attribute value (the code for one of the six comparison operators in this case). 

 - getRelop() first creates a new object retToken and initializes its first component to RELOP, the symbolic code for token **relop**.
 - We see the typical behavior of a state in case 0, the case where the current state is O. 
 	- A function nextChar() obtains the next character from the input . 
 	- If the next input character is not one that can begin a comparison operator, then a function **fail**() is called.
 	- What fail() does depends on the global error¬≠ recovery strategy of the lexical analyzer.
 		- It should reset the *forward* pointer to *lexemeBegin*, in order to allow another transition diagram to be applied to the true beginning of the unprocessed input.
 		- It might then change the value of state to be the start state for another transition diagram, which will search for another token. 
 		- Alternatively, if there is no other transition diagram that remains unused, fail() could initiate an error-correction phase that will try to repair the input and find a lexeme, as discussed in Section 3.1.4.
 - We also show the action for state 8 in Fig. 3.18. Because state 8 bears a \* 
 	- we must retract the input pointer one position (i.e., put c back on the input stream) . 
 	- That task is accomplished by the function retract() . 
 	- Since state 8 represents the recognition of lexeme >=, we set the second component of the returned object, which we suppose is named attribute, to GT, the code for this operator. 

To place the simulation of one transition diagram in perspective, let us consider the ways code like Fig. 3.18 could fit into the entire lexical analyzer.

 1. We could arrange for the transition diagrams for each token to be tried sequentially.  ÊåâÈ°∫Â∫è‰∏Ä‰∏™‰∏™Â∞ùËØï
 	- Then, the function fail() resets the pointer *forward* and starts the next transition diagram, each time it is called. 
 		- This method allows us to use transition diagrams for the individual key¬≠words, like the one suggested in Fig. 3.15.
 		- We have only to use these before we use the diagram for **id**, in order for the keywords to be reserved words.
 2. We could run the various transition diagrams "in parallel," feeding the next input character to all of them and allowing each one to make what¬≠ ever transitions it required. 
 	- If we use this strategy, we must be careful to resolve the case where one diagram finds a lexeme that matches its pattern, while one or more other diagrams are still able to process input.
	- The normal strategy is to ***take the longest prefix of the input*** that matches any pattern. 
		- That rule allows us to prefer identifier the next to keyword then, or the operator -> to - , for example.
 3. The preferred approach, and the one we shall take up in the following sections, is to combine all the transition diagrams into one. 
 	- We allow the transition diagram to read input until there is no possible next state, and then take the longest lexeme that matched any pattern. 
 	- In our running example, this combination is easy, because no two tokens can start with the same character;
 	- However, in general, the problem of combining transition diagrams for several tokens is more complex, as we shall see shortly.


---

<h2 id="921b7eb0390c2aa2b73abf6edb74ab44"></h2>
## 3.5 The Lexical-Analyzer Generator Lex

In this section, we introduce a tool called **Lex**, or in a more recent implemen¬≠tation **Flex**, that allows one to specify a lexical analyzer by specifying regular expressions to describe patterns for tokens. 

 - The input notation for the Lex tool is referred to as the Lex language,  and the tool itself is the Lex compiler. 
 - Behind the scenes, the Lex compiler transforms the input patterns into a transition diagram and generates code, in a file called lex.yy.c, that simulates this tran¬≠sition diagram. 
 - The mechanics of how this translation from regular expressions to transition diagrams occurs is the subject of the next sections; here we only learn the Lex language.

---

<h2 id="d96ef03e4ef48fad67844a05f42082c8"></h2>
### 3.5.1 Use of Lex

Figure 3.22 suggests how Lex is used. 

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.22.png)

 - The normal use of the compiled C program, referred to as a.out , is as a subroutine ofthe parser. 
 - It is a C function that returns an integer ? , which is a code for one of the possible token names. 
 - The attribute value, whether it be another numeric code, a pointer to the symbol table, or nothing, is placed in a global variable **yylval**, which is shared between the lexical analyzer and parser, thereby making it simple to return both the name and an attribute value of a token.

---

<h2 id="657b471d853e0944613b1fc41f7988bd"></h2>
### 3.5.2 Structure of Lex Programs

A Lex program has the following form:

```
	declarations
	%%
	translation rules
	%%
	auxiliary functions
```

 - The *declarations* section includes declarations of 
 	- variables, 
 	- *manifest constants* 
 		- (identifiers declared to stand for a constant, e.g., the name of a token), 
 	- and regular definitions 
 - The *translation rules* each have the form
 	- `Pattern { Action }`
 	- Each pattern is a regular expression 
 		- which may use the regular definitions of the declaration section. 
 	- The actions are fragments of code, typically written in C, although many variants of Lex using other languages have been created.
 - The *auxiliary functions* holds whatever additional functions are used in the actions. 
 	- Alternatively, these functions can be compiled separately and loaded with the lexical analyzer.

The lexical analyzer created by **Lex** behaves in concert with the parser as follows. 

 - When called by the parser, the lexical analyzer begins reading its remaining input, one character at a time, until it finds the longest prefix of the input that matches one of the patterns P·µ¢
 - It then executes the associated action A·µ¢. 
 	- Typically, A·µ¢ will return to the parser, 
 	- but if it does not (e.g., because P·µ¢ describes whitespace or comments), then the lexical analyzer proceeds to find additional lexemes, until one of the corresponding actions causes a return to the parser. 
 - The lexical analyzer returns a single value, the token name, to the parser, but uses the shared, integer variable **yylval** to pass additional information about the lexeme found, if needed.


Example 3.11 : Figure 3.23 is a Lex program that recognizes the tokens of Fig. 3.12 and returns the token found. 

```
%{
	/* definitions of manifest constants 
	LT, LE, EQ, NE, GT, GE,
	IF, THEN, ELSE, ID, NUMBER, RELOP */
%}
/* regular definitions */
delim 	[ \t\n]
ws 		{delim}+
letter 	[A-Za-z]
digit 	[0-9]
id 		{letter} ({letter}|{digit})*
number 	{digit}+ (\. {digit}+)? (E [+-]?{digit}+)?

%%

{ws} 	{/* no action and no return */} 
if 		{return(IF);}
then 	{return(THEN);}
else 	{return(ELSE);}
{id} 	{yylval =(int) installID() ; return(ID);}
{number} {yylval =(int) installNum() ;return(NUMBER) ; }
"<"		{yylval = LT; return(RELOP);} 
"<="	{yylval = LE; return(RELOP);} 
"="		{yylval = EQ; return(RELOP) j} 
"<>"	{yylval = NE; return(RELOP);} 
">"		{yylval = GT; return(RELOP);} 
">="	{yylval = GE; return(RELOP);}

%%

int installID() {
	/* function to install the lexeme, 
	whose first character is pointed to by yytext,
	a d whose length is yyleng, 
	into the symbol table and 
	return a pointer thereto */
}
int installNum() {
	/* similar to installID, but puts numer¬≠ical 
	constants into a separate table */
}
```

Figure 3.23: Lex program for the tokens of Fig. 3.12

A few observations about this code will introduce us to many of the important features of **Lex**.

 - a pair of special brackets, %{ and %}. 
 	- Anything within these brackets is copied directly to the file lex.yy.c , and is not treated as a regular definition. 
 	- It is common to place there the definitions of the manifest constants, using **C #define** statements to associate unique integer codes with each of the manifest constants. 
 	- In our example, we have listed in a comment the names of the manifest constants, LT, IF, and so on, but have not shown them defined to be particular integers
 		- If Lex is used along with Yacc, then it would be normal to define the manifest constants in the Yacc program and use them without definition in the Lex program. 
 - Also in the declarations section is a sequence of regular definitions. 
 	- These use the extended notation for regular expressions. 
 	- Regular definitions that are used in later definitions or in the patterns of the translation rules are surrounded by curly braces '{ }'. 
 	- In the definition of ***id*** and ***number***, parentheses are used as grouping meta symbols.
 - In the auxiliary-function section, we see two such functions, installID() and installNum(). 
 	- Like the portion of the declaration section that appears between %{ ... %} , everything in the auxiliary section is copied directly to file lex.yy.c, but may be used in the actions.
 - patterns and rules
 	- *ws*, , has an associated empty action. If we find whitespace, we do not return to the parser, but look for another lexeme. 
 	- **if**. Should we see the two letters *if* on the input, and they are not followed by another letter or digit (which would cause the lexical analyzer to find a longer prefix of the input matching the pattern for **id**) , then the lexical analyzer consumes these two letters from the input and returns the token name IF, that is, the integer for which the manifest constant IF stands. 
 	- Keywords **then** and **else** are treated similarly.
 - The 5th token has the pattern defined by **id**. 
 	- although keywords like **if** match this pattern as well as an earlier pattern, Lex chooses whichever pattern is listed first in situations where the longest matching prefix matches two or more patterns. 

The action taken when **id** is matched is threefold:

 1. Function installID() is called to place the lexeme found in the symbol table.
 2. This function returns a pointer to the symbol table, which is placed in global variable yylval, where it can be used by the parser or a later component of the compiler. 
 	- Note that installID() has available to it two variables that are set automatically by the lexical analyzer that Lex generates :
 	- (a) yytext is a pointer to the beginning of the lexeme
 	- (b) yyleng is the length of the lexeme found.
 3. The token name ID is returned to the parser.


---

<h2 id="cab9b67a202a7627adfcf2587ce702df"></h2>
### 3.5.3 Conflict Resolution in Lex

We have alluded to the two rules that Lex uses to decide on the proper lexeme to select, when several prefixes of the input match one or more patterns:

 1. Always prefer a longer prefix to a shorter prefix.
 2. If the longest possible prefix matches two or more patterns, prefer the pattern listed first in the Lex program.
 	- this rule makes keywords reserved

---

<h2 id="f57723412a7038cb7392678e61a02b76"></h2>
### 3.5.4 The Lookahead Operator

Lex automatically reads one character ahead of the last character that forms the selected lexeme, and then retracts the input so only the lexeme itself is consumed from the input. 

However, sometimes, we want a certain pattern to be matched to the input only when it is followed by a certain other characters. 

If so, we may use the slash in a pattern to indicate the end of the part of the pattern that matches the lexeme. What follows / is additional pattern that must be matched before we can decide that the token in question was seen, but what matches this second pattern is not part of the lexeme.

Example 3.13 : In Fortran and some other languages, keywords are not re¬≠served. That situation creates problems, such as a statement

```Fortran
	IF(I,J) = 3
```
where IF is the name of an array, not a keyword. This statement contrasts with statements of the form

```Fortran
	IF( condition ) THEN ...
```

where IF is a keyword. 

Fortunately, we can be sure that the keyword IF is always followed by a left parenthesis, some text - the condition - that may contain parentheses, a right parenthesis and a letter. Thus, we could write a Lex rule for the keyword IF like:

```
	IF / \( .* \) {letter}
```

Note that in order for this pattern to be foolproof, we must preprocess the input to delete whitespace. We have in the pattern neither provision for whitespace, nor can we deal with the possibility that the condition extends over lines, since the dot will not match a newline character.
For instance, suppose this pattern is asked to match a prefix of input:

```Fortran
	IF(A<(B+C)*D)THEN...
```

We conclude that the letters IF constitute the lexeme, and they are an instance of token **if**.

---

<h2 id="adab9833ab2dcc6edab8a39432584e49"></h2>
## 3.6 Finite Automata

We shall now discover how **Lex** turns its input program into a lexical analyzer. 

At the heart of the transition is the formalism known as ***finite automata***. These are essentially graphs, like transition diagrams, with a few differences:

 1. Finite automata are ***recognizers***; they simply say "yes" or "no" about each possible input string.
 2. Finite automata come in two flavors:

	- (a) *Nondeterministic finite automata* (NFA) have no restrictions on the labels of their edges. 
		- A symbol can label several edges out of the same state, and Œµ, the empty string, is a possible label.
	- (b) *Deterministic finite automata* (DFA) have, for each state, and for each symbol of its input alphabet, exactly one edge with that symbol leaving that state.

Both deterministic and nondeterministic finite automata are capable of rec¬≠ognizing the same languages -- the regular languages.

---

<h2 id="a46bbbffe97f741d7522f9d7eba5483b"></h2>
### 3.6.1 Nondeterministic Finite Automata

A ***nondeterministic finite automaton*** (NFA) consists of:

 1. A finite set of states S
 2. A set of input symbols Œ£, the *input alphabet*. 
 	- We assume that Œµ is never a member of Œ£ .
 3. A *transition function* that gives, for each state, and for each symbol in Œ£ U {Œµ}, a set of *next states*.
 4. A state s‚ÇÄ from S that is distinguished as the *start state* (or *initial state*) .
 5. A set of states F, a subset of S, that is distinguished as the *accepting states* (or *final states*).


We can represent either an NFA or DFA by a ***transition graph***, where the nodes are states and the labeled edges represent the transition function. 

There is an edge labeled *Œ±* from state *s* to state *t* if and only if *t* is one of the next states for state *s* and input *Œ±* . This graph is very much like a transition diagram, except:

 - a) The same symbol can label edges from one state to several different states, and
 - b) An edge may be labeled by Œµ, instead of, or in addition to, symbols from the input alphabet.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.24.png)

Example 3.14 : The transition graph for an NFA recognizing the language of regular expression **(a|b)\*abb** . 

---

<h2 id="2ebef3028849a4dc0909d534069b2316"></h2>
### 3.6.2 Transition Tables

We can also represent an NFA by a ***transition table***, whose rows correspond to states, and whose columns correspond to the input symbols and Œµ. 

The entry for a given state and input is the value of the transition function applied to those arguments. If the transition function has no information about that state-input pair, we put ‚àÖ in the table for the pair.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.25.png)

The transition table has the advantage that we can easily find the transitions on a given state and input. Its disadvantage is that it takes a lot of space, when the *input alphabet* is large, yet most states do not have any moves on most of the input symbols.

---

<h2 id="7de19e880b9825779ec50e33458727d7"></h2>
### 3.6.3 Acceptance of Input Strings by Automata

An NFA *accepts* input string x if and only if there is some path in the transition graph from the *start state* to one of the ***accepting states***, such that the symbols along the path spell out x. Note that Œµ labels along the path are effectively ignored, since the empty string does not contribute to the string constructed along the path.

The language *defined* (or *accepted*) by an NFA is the set of strings labeling some path from the start to an accepting state. 

As was mentioned, the NFA of Fig. 3.24 defines the same language as does the regular expression **(a|b)\*abb**, that is, all strings from the alphabet {a, b} that end in *abb*. We may use L(A) to stand for the language accepted by automaton A.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/Compiler_F3.26.png)

Example 3.17: Figure 3.26 is an NFA accepting L(aa\*|bb\*). String *aaa* is accepted. 

---

<h2 id="4870f1974b0dd964c769b59b181d7226"></h2>
### 3.6.4 Deterministic Finite Automata

A *deterministic finite automaton* (DFA) is a special case of an NFA where:

 1. There are no moves on input Œµ, and
 2. For each state S and input symbol Œ±, there is exactly one edge out of s labeled Œ±.

If we are using a transition table to represent a DFA, then each entry is a single state. we may therefore represent this state without the curly braces (eg.`{0,1}`)  that we use to form sets.


























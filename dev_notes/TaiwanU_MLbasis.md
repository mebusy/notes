...menustart

 - [Week1  The Learning Problem](#ecde0ae599ab9118d10cbcd51ecbaab0)
 - [Week2 Learning to Answer Yes/No](#5868daf5782375dc2181821611a1a15c)
     - [Perceptron Hypothesis Set](#85afcca55926ce0a29f0f6cb05850a9d)
         - [Perceptron](#d8d47e427c95153bb9f8bec1d2d1df94)
         - [Vector Form of Perceptron Hypothesis](#7f75552635f2c71b44d8ec37aefb9df7)
         - [Perceptrons in â„Â²](#b07c3735cd2797ec2b778d92a0adf769)
     - [Perceptron Learning Algorithm (PLA)](#0e7ff491a0fb09097115fb8590ecddb5)
         - [Practical Implementation of PLA](#03ed0e617baca54a8f30c4314b8395c6)
         - [Why PLA may work](#2c4c57a7143770c331971f4ce207c2e2)
     - [Guarantee of PLA](#e3f4c08eab2f66995b163ecf47f53898)
         - [Linear Separability](#fafcae75a7ed33e57480616db662b7ce)
         - [PLA Fact : W<sub>t</sub> Gets More Aligned with W<sub>f</sub>](#1e0adff1d0ebad51a3e3366ffd90c42a)
         - [PLA Fact : W<sub>t</sub> Does Not Grow Too Fast](#4881687a06277027c5043e17bc1b09df)
     - [Non-Separable Data](#3baec9e63797786ab9197c8328006b4c)
         - [Pocket Algorithm](#b84fec7e58d78ac3562f365b9a0ba808)
 - [Week 3 Types of Learning](#802179691269a6cfc6518c10b66d4776)
     - [Learning with Different Output Space](#e4561cdae64f2ab23476660e7fcd3596)
         - [Multiclass Classification: Coin Recognition Problem](#8aea726ae5a16d695949a7fdb31468f9)
         - [Structured Learning : Sequence Tagging Problem](#a5756f5557e96b4c381b0eff885d54ce)
     - [Learning with Different Data Label](#684c38a5e13528daec2ab3efa2097be3)
     - [Learning with Different Protocol](#6def4688e7d3b9cfc2f058ed2729c813)
     - [Learning with Different Input Space](#6ebd75d7ad445321e62d64acb494084a)
 - [Week 4 Feasibility of Learning](#cfd8a0e711b39b612b7fd7279a78d83e)
     - [Probability to the Rescue](#e2757331874147511e796bb57cd0130f)
         - [Hoeffding's Inequality](#2738a5bd77944b4f0a65a05dcf269384)
     - [Connection to Learning](#4d3ee8b9af56c703975b205e29f34cfa)
         - [The Formal Guarantee](#bc490f18155779c237c38b6532ca7b7f)
         - [Verification of One h](#911f375f353e71713d92e0e8d439e40e)
     - [Connection to Real Learning](#d9ee0f5ea86476845fea40190ab1462e)
         - [Coin Game](#9ba5efcb15754a3919e1c165c9746157)
         - [BAD Sample and BAD Data](#0f6ca2986821d310f080364af49eed46)
         - [BAD Data for Many h](#16553486c255ce8a5d8e557b67c3c9a6)
         - [Bound of BAD Data](#c82862b9ceccd7efd1ca9d2bab635903)
         - [The 'Statistical Learning Flow'](#d8634320f9db8392b907e680f3fbdeef)
 - [Week5 Traning versus Testing](#e25bb439455099bb6fd2a7b680e73f15)
     - [Trade-off on M](#8d5764cc1a7dce832f420e7096b07f1b)
     - [Effective Number of Lines](#83bee210aad456619c22ae2c5e2dc963)
         - [How many Kinds of Lines for 1~4 Inputs in 2D  ?](#624848f04dceb9194a26b7aa039879f2)
         - [Effective Number Of Lines](#8c31579792d76e74531e953be4313740)
     - [Effective Number of Hypotheses](#8fc2f515778b49e4ad6450b9cf3d6c49)
         - [Dichotomies: Mini-hypothesis](#8d22448394722fd85b23c2eb8246dcb2)
         - [Growth Function](#dfa7e0a4bb23bbcf4e814b850274fd66)
         - [TODO](#b7b1e314614cf326c6e2b6eba1540682)
     - [Break Point](#5a0ecfb80c0bc36fc6c7b593079594cb)
         - [The 4 Growth Functions](#b5ce0fb28ce372d75a041bef143918ef)
         - [Break Point of H](#d5be8e86759a610959b92abd0aa75199)
 - [Week 6 Theory of Generalization  ä¸¾ä¸€åä¸‰](#a191a7c2b6e0987654f6d92503bf1078)
     - [Restriction of Break Point](#7cd79213ae5758377b9aed8accc60591)
     - [Bounding Function: Basic Cases](#3bf7d47fb91c77c8c30afce81e8a67e1)
         - [Table of Bounding Function](#e51ebc67414533b0af139f8439d3408e)
     - [Bounding Function: Inductive Cases](#2c65d0169d44316f221fe6051944e78e)
         - [Bounding Function : The Theorem](#20885a853d6349f8a2c1b4a13f9fd078)
     - [A Pictorial Proof](#ac41223f5ec900a59b7a7c730d2db790)
         - [That's All !](#942c21d4344c1e2c04cd9cb2b0635e7f)
 - [Week7 The VC Dimension](#15173fa6f984d4d655f1b15b08355016)
     - [Definition of VC Dimension](#03b215c6ba1906c71c6856766d5a91dc)
         - [The 4 VC Dimensions](#ef6ecf6a562931f940c6f8fa8f315ec9)
         - [VC Dimemsion and Learning](#2ab6a2e714fe88daa38d0ebadbd820b7)
     - [VC Dimension of Perceptrons](#010c451c23e144261c78eed3918be60c)
         - [d<sub>VC</sub> â‰¥ d+1](#0bcebb5418877c668b7f8078fd53e2a6)
         - [d<sub>VC</sub>  â‰¤ d+1](#b23a9ffbf5d00fecd76ee661c5bae97c)
     - [Physical Intuition of VC Dimension](#a7ab380b76ef972f810d737e226f4fe6)
         - [Degrees of Freedom](#364130468e37b396a03b78270e87774b)
         - [Two Old Friends](#c30d7a9f1110a7115386a1a1449444ff)
         - [M and d<sub>vc</sub>](#668bae56a418c6c053a975bbc41bef59)
     - [Interpreting VC Dimension](#4fa217a4f25efe58ca38efc2921a9ece)
         - [VC Bound Rephrase:  Penalty for Model Complexity](#cf1207c2cd37bca9c19baefe3be5f449)
         - [THE VC Message](#fd5261ae2542df593b9ab1967d1212b7)
         - [VC Bound Rephrase : Sample Complexity](#2103d3a1527ad166beedae934932a08e)
         - [Summary](#290612199861c31d1036b185b4e69b75)
 - [Week 8  Noise and Error](#6d87d0270ecffd6ed877bf64cf997573)
     - [Noise and Probabilistic Target](#a3f810ce646f0936df658a4d30a15b6d)
     - [Error Measure](#d2d99ba3e6ff66dfef2164352b2c98a1)
         - [Pointwise Error Measure](#9a0ea46a375f6caa83eb7371460f252e)
         - [Two Important Pointwise Error Measures](#6c2d0a3129771ec5553a0a7b40c04800)
         - [Ideal Mini-Target](#4800505041049669fa7b042ae638b020)
     - [Algorithmic Error Measure](#2187b661cffc7f0490d1f364850ab22a)
         - [Choice of Error Measure](#46b76063f10f6397dfd67504c091971d)
         - [Take-home Message for Now](#df794cf0b049355a25a3dc69dacf670b)
     - [Weighted Classification](#77f01330c84d591c46ceb54c844f4fbe)
         - [Minimizing E<sub>in</sub> for Weighted Classification](#aae4803feb7519588dfdfd0080792d30)
         - [Systematic Route](#62efbe469e781542f448dfebe1e436d8)
         - [Weighted Pocket Algorithm](#55d55c54777e3438c85343abfae313ac)

...menuend


<h2 id="ecde0ae599ab9118d10cbcd51ecbaab0"></h2>


# Week1  The Learning Problem

 - *Algorithm* takes *Data* and *Hypothesis set*  to get final hypothesis *g*.
    - H: set of candidate formula , with different weights W

<h2 id="5868daf5782375dc2181821611a1a15c"></h2>


# Week2 Learning to Answer Yes/No

<h2 id="85afcca55926ce0a29f0f6cb05850a9d"></h2>


## Perceptron Hypothesis Set

<h2 id="d8d47e427c95153bb9f8bec1d2d1df94"></h2>


### Perceptron

 - x = { xâ‚,xâ‚‚,...,x<sub>d</sub> }
 - y = { +1 | -1  }
 - h(x) = sign( âˆ‘áµˆáµ¢â‚Œâ‚ wáµ¢xáµ¢ - threshold  )

<h2 id="7f75552635f2c71b44d8ec37aefb9df7"></h2>


### Vector Form of Perceptron Hypothesis
    
 - æŠŠ *threshold* ä¹Ÿå½“æˆæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ wâ‚€

```
h(x) = sign( âˆ‘áµˆáµ¢â‚Œâ‚ wáµ¢xáµ¢ - threshold  )
h(x) = sign( âˆ‘áµˆáµ¢â‚Œâ‚ wáµ¢xáµ¢ + (-threshold)*(+1)  )
h(x) = sign( âˆ‘áµˆáµ¢â‚Œâ‚€ wáµ¢xáµ¢ )
h(x) = sigh ( Wáµ€X )
```

<h2 id="b07c3735cd2797ec2b778d92a0adf769"></h2>


### Perceptrons in â„Â²

 - h(x) = sign( wâ‚€ +wâ‚xâ‚ +wâ‚‚xâ‚‚ )
    - h(x) = 0 is a **lines** ( ir hyperplanes in â„áµˆ )

 - perceptrons <=> **linear (binary) clasifiers**
    - PS. (2Dç©ºé—´ä¸‹) äºŒå…ƒåˆ†ç±»ä¹Ÿå¹¶ä¸ä»…é™ä¸ ç›´çº¿åˆ’åˆ†çš„æƒ…å†µï¼Œh(x)=0 ä¹Ÿå¯ä»¥æ˜¯ä¸€æ¡æ›²çº¿ã€‚åªæ˜¯è¿™ä¸ªperceptron æ˜¯ç›´çº¿æƒ…å†µ



<h2 id="0e7ff491a0fb09097115fb8590ecddb5"></h2>


## Perceptron Learning Algorithm (PLA)

 - will represent *gâ‚€* by its weight verctor Wâ‚€ 
 - å¦‚æœ ç›´çº¿ *g*=0 è¿˜ä¸å®Œç¾ï¼Œæˆ‘ä»¬ä¸€å®šå¯ä»¥æ‰¾å¾—å‡º èµ„æ–™ä¸­çš„æŸä¸€ä¸ªç‚¹ ( x<sub>n(t)</sub> , y<sub>n(t)</sub>  ) , åœ¨è¿™ä¸ªç‚¹ä¸Š  *g* çŠ¯äº†é”™ 
 - çŠ¯äº†é”™ï¼Œæˆ‘ä»¬å°±è¦æƒ³åŠæ³•æ¥ä¿®æ­£å®ƒ ï¼Œ å¦‚ä½•ä¿®æ­£ï¼Ÿ
    - å¦‚æœ y åº”è¯¥æ˜¯æ­£çš„ï¼Œg å¾—åˆ°çš„æ˜¯è´Ÿçš„ï¼Œè¯´æ˜ W,X çš„è§’åº¦å¤ªå¤§ï¼Œ é‚£æˆ‘ä»¬å°±æŠŠ å‘é‡W è½¬å›æ¥;åä¹‹ï¼Œæˆ‘ä»¬æŠŠå‘é‡W  è½¬å¼€
    - è¿™ä¸¤ç§æƒ…å†µï¼Œå¯ä»¥ç»Ÿä¸€çš„ç”± `W=W+yx` å¤„ç†
    - ![](../imgs/TU_ML_PLA_correct.png)
 - ç®—æ³•ä¼ªä»£ç å¦‚ä¸‹:

![](../imgs/TU_ML_PLA_pseudo.png)

 - A fault confessed is half redressed.
 - Weight Space
    - A point in the space represents a particular setting of  W
    - each training case X can be represented as a hyperplane through the origin.
        - The plane goes through the origin and is perpendicular to the input vector X .
    - The weights must lie on one side of this hyper-plane to get the answer correct.
    - æ›´å…·ä½“çš„ weight space è¯´æ˜ï¼Œè§ [Neural Networks](https://github.com/mebusy/notes/blob/master/dev_notes/NeuralNetworks.md)
        - å®è·µä¸­ï¼Œä¹Ÿèƒ½æŠŠ Wï¼ŒXè§’è‰²äº’æ¢ä¹Ÿå¯ä»¥ï¼Ÿ

<h2 id="03ed0e617baca54a8f30c4314b8395c6"></h2>


### Practical Implementation of PLA 

![](../imgs/TU_ML_CyclicPLA_pseudo.png)

 - æŒ‰æŸä¸ªé¡ºåºä¾¿åˆ© æ ·æœ¬ï¼Œå¦‚æœå‘ç°é”™è¯¯å°±çº é”™ï¼Œ çŸ¥é“æŸä¸ªå®Œæ•´çš„å¾ªç¯æ²¡æœ‰é”™è¯¯ã€‚
 - æ³¨æ„ PLA æ ·æœ¬è¾“å…¥ X éœ€è¦æ’å…¥ bias


<h2 id="2c4c57a7143770c331971f4ce207c2e2"></h2>


### Why PLA may work 

 - sign(Wáµ€X) != y<sub>n</sub> ,  W<sub>t+1</sub> <-  W<sub>t</sub> + y<sub>n</sub>X<sub>n</sub>
 - y<sub>n</sub>W<sub>t+1</sub>X<sub>n</sub> >=  y<sub>n</sub>W<sub>t</sub>X<sub>n</sub>
    - å¦‚æœy<sub>n</sub> æ˜¯è´Ÿçš„ï¼Œç®—æ³•çš„ä½œç”¨æ˜¯ åŠªåŠ›ä½¿Wáµ€Xå¾€æ­£çš„æ–¹å‘åŠªåŠ›ï¼Œä½¿å¾—æ›´æ¥è¿‘æ­£ç¡®çš„ y

<h2 id="e3f4c08eab2f66995b163ecf47f53898"></h2>


## Guarantee of PLA

<h2 id="fafcae75a7ed33e57480616db662b7ce"></h2>


### Linear Separability

 - if PLA halts (i.e. no more mistakes) , ( **necessary condition** ) D allows some *W* to make no mistake 
 - call such D **linear separable**

<h2 id="1e0adff1d0ebad51a3e3366ffd90c42a"></h2>


### PLA Fact : W<sub>t</sub> Gets More Aligned with W<sub>f</sub>

 - linear separable D <=> exists perfect  W<sub>f</sub> such that y<sub>n</sub> = sign(  W<sub>f</sub>áµ€x<sub>n</sub> )

![](../imgs/TU_ML_PLA_fact1.png)

 - ç”±ä¸Šå¯ä»¥ï¼Œå¦‚æœ W<sub>t</sub> çš„é•¿åº¦æ²¡æœ‰å˜é•¿çš„è¯ï¼Œé‚£ä¹ˆå¯ä»¥çœ‹åˆ° PLA ä½¿å¾—  W<sub>t</sub> æ›´åŠ æ¥è¿‘W<sub>f</sub> 

<h2 id="4881687a06277027c5043e17bc1b09df"></h2>


### PLA Fact : W<sub>t</sub> Does Not Grow Too Fast 

 - W<sub>t</sub> changed only when mistake 
    - <=> sign( W<sub>t</sub>áµ€x<sub>n(t)</sub> ) â‰  y<sub>n(t)</sub> 
    - <=> y<sub>n(t)</sub> W<sub>t</sub>áµ€x<sub>n(t)</sub> â‰¤ 0  (WXå’Œy å¼‚å·)

![](../imgs/TU_ML_PLA_fact2.png)

 - ç»¼åˆè¿™ä¸¤æ¡ï¼Œæˆ‘ä»¬å¯ä»¥å¾—åˆ°

![](../imgs/TU_ML_PLA_fact_conclude.png)

 - Guarantee
    - as long as *linear separable* and *correct by mistake*
    - inner product of W<sub>f</sub> and W<sub>t</sub> grows fast; length of W<sub>t</sub> grows slowly
    - PLA 'lines' are more and more aligned with W<sub>f</sub> => halts

<h2 id="3baec9e63797786ab9197c8328006b4c"></h2>


## Non-Separable Data

<h2 id="b84fec7e58d78ac3562f365b9a0ba808"></h2>


### Pocket Algorithm

 - modify PLA (black lines) by keeping best weights in pocket

![](../imgs/TU_ML_PLA_modify_pseudo.png)

 - éå†æ ·æœ¬ï¼Œå¦‚æœ‰æœ‰é”™ï¼Œå°±çº é”™ -- update w
 - æŠŠæ›´æ–°åçš„w  æ”¾åˆ°æ•´ä¸ªæ ·æœ¬ä¸­æµ‹è¯•é”™è¯¯ç‡ï¼Œ è®°å½•é”™è¯¯ç‡æœ€å°çš„é‚£ä¸ª wg
 - ä¸€èˆ¬è®¾ç½® w æ›´æ–°æ»¡ næ¬¡åï¼Œç»“æŸæ•´ä¸ªç®—æ³•



<h2 id="802179691269a6cfc6518c10b66d4776"></h2>


# Week 3 Types of Learning

<h2 id="e4561cdae64f2ab23476660e7fcd3596"></h2>


## Learning with Different Output Space

<h2 id="8aea726ae5a16d695949a7fdb31468f9"></h2>


### Multiclass Classification: Coin Recognition Problem 

 - classify US coins (1c,5c,10c,25c)
    - y = {1,2,...,K}
 - written digits => 0,1,...,9
 - picture => apple, orange , strawberry

<h2 id="a5756f5557e96b4c381b0eff885d54ce"></h2>


### Structured Learning : Sequence Tagging Problem

 - multiclass classification : word => word class
 - structured learning :
    - sentence => structure (class of each word)
 - y = { PVN,PVP,NVN,PV,... } , not including VVVVV
 - huge multiclass classification problem 
    - (structure â‰¡ hyperclass) without 'explicit' class definition

<h2 id="684c38a5e13528daec2ab3efa2097be3"></h2>


## Learning with Different Data Label

 - Supervised : all y<sub>n</sub>
 - Unsupervised : no y<sub>n</sub>
    - clustering :  {X<sub>n</sub>} => cluser(X)
    - density estimation : {X<sub>n</sub>} => density(X)
        - â‰ˆ 'unsupervised bounded regression' 
        - i.e. traffic reports with location => dangerous areas
    - outlier detection {X<sub>n</sub>} => unusual(X) 
        - â‰ˆ 'unsupervised binary regression' 
        - i.e. Internet logs => intrusion alert
    - ... and a lot more !
 - Semi-supervised : some y<sub>n</sub>
    - leverage unlabeled data to avoid 'expensive' labeling
    - eg.
        - face images with a few labeled => face identifier (Facebook)
        - medicine data with a few labeled => medicine effect predictor
 - Reinforcement Learning : implicit y<sub>n</sub>


<h2 id="6def4688e7d3b9cfc2f058ed2729c813"></h2>


## Learning with Different Protocol

 - Batch Learning
 - Online: hypothesis improves through receiving data instances sequeentially
    - PLA can be easily adapted to online protocol 
    - reinforment learning is often done online
 - Active Learning:  Learning by 'Asking'   
    - active: improve hypothesiis with fewer lables (hopefully) by asking questions **strategically**
    - è·Ÿä¸Šé¢ä¸¤ä¸ªè¢«åŠ¨å­¦ä¹ ä¸åŒ, labelæˆæœ¬å¾ˆé«˜çš„æƒ…å†µ é€‚åˆå°è¯•

<h2 id="6ebd75d7ad445321e62d64acb494084a"></h2>


## Learning with Different Input Space

 - concrete features
    - æ¯”å¦‚ç¡¬å¸åˆ†ç±»ï¼Œé€‰æ‹© å¤§å°ï¼Œé‡é‡ç­‰ç­‰ ä½œä¸ºfeature
    - é€šå¸¸å¸¦æœ‰äººç±»çš„æ™ºæ…§å¯¹è¿™ä¸ªé—®é¢˜çš„æè¿°
 - Raw Features: digit recognition problem
    - often need human or machines to convert to concrete ones
 - Abstract Features : Rating Prediction problem
    - given previous (userid, itemid, rating) tuples , predict the rating that some userid would give to itemid ?
    - 'no physical meaning' ; thus even more difficult for ML

<h2 id="cfd8a0e711b39b612b7fd7279a78d83e"></h2>


# Week 4 Feasibility of Learning

<h2 id="e2757331874147511e796bb57cd0130f"></h2>


## Probability to the Rescue

 - consider a bin of many many *orange* and *green* marbles
 - we don't know the *orange* portion (probability) 
 - can you **infer** the *orange* probability ?

 - Solution 1:  random sampling  ?

---

 - bin: assume
    - orange probability = Î¼
    - green probability = 1-Î¼
    - with Î¼ **unknonw** 
 - sample 
    - N marbles sampled independently , with 
        - *orange* fraction = Î½ 
        - *green* fraction = 1-Î½
    - now Î½ **known**
 - does **in-sample Î½**  say anything about out-of-sample Î¼ ?
    - **NO!**   possibly not: sample can be mostly *green* while bin is mostly *orange*
    - **YES!**  probably yes: in-sample Î½ likely close to unknown Î¼.
    - æ²¡æœ‰åŠæ³•æœ‰ä¸ªç¡®å®šçš„ç­”æ¡ˆã€‚
        - æ²¡æœ‰åŠæ³•è¯´ï¼Œæˆ‘æŠ½10é¢—èµ·æ¥ï¼Œè¿™10é¢—çš„æ¯”ä¾‹å°±ä¸€å®šæ˜¯ç½å­é‡Œé¢çš„æ¯”ä¾‹ã€‚ä¸è¿‡å‘¢ï¼Œå¤§è‡´ä¸Šæœ‰å¾ˆå¤§çš„å‡ ç‡æ˜¯è¿™ä¸ªæ ·å­ã€‚

<h2 id="2738a5bd77944b4f0a65a05dcf269384"></h2>


### Hoeffding's Inequality

 - sample of size N 
 - Î¼ = *orange* probability in bin
 - Î½ = *orange* fraction in sample
 - in big sample (N large) , Î½ is probably close to Î¼ ( within Îµ ) 
    - ![](../imgs/TU_MLbasis_hoeffding_ineq.png)
    - å½“Nå¾ˆå¤§æ˜¯ï¼ŒÎ¼å’ŒÎ½ ç›¸å·®å¾ˆå¤§çš„å‡ ç‡ å¾ˆå°
 - called **Hoeffding Inequality** , for marbles, coin, polling
 - The statement 'Î¼-Î½' is **probably approximately correct** (PAC)

---

 - valid for all N and Îµ
    - ä¸ç®¡ä½ é€‰æ‹©çš„N å’Œ Îµ æ˜¯å¤šå°‘ï¼Œè¿™ä¸ªå¼å­éƒ½æ˜¯å¯¹çš„
 - does not depend on Î¼ , **no need to 'know' Î¼**
    - ç­‰å¼å³è¾¹ä¸éœ€è¦Î¼
 - large sample size N for looser gap Îµ => higher probability for `Î¼â‰ˆÎ½`
 - if large N , can probably infer unknown Î¼  by known Î½.

---

 - Q: Let Î¼ = 0.4. Use Hoeffding's Inequality to bound the probability that a sample of 10 marbles will have Î½ â‰¤ 0.1. What bound do you get?
    - (1) 0.67 , (2) 0.40 , (3) 0.33 , (4) 0.05
 - A: set N=10, and Îµ = 0.3, you will get the anwser (3)
    - `2* (math.e **(  -2*e*e*N  ) ) = 0.3305977764431732`
    - BTW, (4) is the actual probability and Hoeffding gives only an upper bound to that.

<h2 id="4d3ee8b9af56c703975b205e29f34cfa"></h2>


## Connection to Learning

bin | learning
--- | ---
unknown *orange* prob. Î¼ | fixed hypothesis h(x) ?  target f(x)
marble âˆˆ bin | X âˆˆ ğ•
orange marble | h is wrong <=> h(x) â‰  f(x)
green marble | h is right <=> h(x) = f(x)
size-N sample from bin | check h on â…… = { (X<sub>n</sub>, y<sub>n</sub>) } , if no noise , y<sub>n</sub>=f(X<sub>n</sub>)

 - if **large N** & **i.i.d X<sub>n</sub>** infer unknown [ h(x) â‰  f(x) ]  probability by known [ h(x<sub>n</sub>) â‰  y<sub>n</sub>  ]
 - for any fixed *h* ,  can probably infer
    - **unknown E<sub>out</sub>(h)** = Îµ<sub>X~P</sub> [ h(x) â‰  f(x) ]  , (Î¼)
    - by **known E<sub>in</sub>(h)** = 1/NÂ·âˆ‘á´º<sub>n=1</sub> [ h(x<sub>n</sub>) â‰  y<sub>n</sub>  ] , (Î½)

<h2 id="bc490f18155779c237c38b6532ca7b7f"></h2>


### The Formal Guarantee

![](../imgs/TU_MLb_fg.png)

same as the 'bin' analogy...

 - valid for all N and Îµ
 - does not depend on E<sub>out</sub>(h) , **no need to know E<sub>out</sub>(h)**
    - f and P can stay unknown
 - 'E<sub>in</sub>(h) = E<sub>out</sub>(h)' is **probably approximately correct (PAC)**.
 - if **E<sub>in</sub>(h) â‰ˆ E<sub>out</sub>(h)** and **E<sub>in</sub>(h) small** 
    - => E<sub>out</sub>(h) small.
    - => h â‰ˆ f with respect to P 

<h2 id="911f375f353e71713d92e0e8d439e40e"></h2>


### Verification of One h

 - for any fixed *h*, when data large enough,  E<sub>in</sub>(h)  â‰ˆ E<sub>out</sub>(h) 
    - can we claim 'good learning' ( g â‰ˆ f  ) ?
 - Yes: if E<sub>in</sub>(h)  small for the fixed h , and **A pick the h as g** => 'g = f' PAC
 - No: if **A forced to pick THE h as g** which has not small E<sub>in</sub>(h)

<h2 id="d9ee0f5ea86476845fea40190ab1462e"></h2>


## Connection to Real Learning

<h2 id="9ba5efcb15754a3919e1c165c9746157"></h2>


### Coin Game

 - if everyone in size-150 , flip a  coin 5 times , one will get 5 heads with large probability
    - 1 - (31/32)Â¹âµâ° > 99%
 - BAD sample:  E<sub>in</sub> and E<sub>out</sub> **far away**

<h2 id="0f6ca2986821d310f080364af49eed46"></h2>


### BAD Sample and BAD Data

 - BAD Sample
    - e.g. E<sub>out</sub> = 1/2, but getting all heads ( E<sub>in</sub> = 0 )
 - BAD Data for One h
    - E<sub>in</sub> and E<sub>out</sub> **far away** 
    - e.g. E<sub>out</sub> big (far from f) , but E<sub>in</sub>  small ( correct on most examples )
 - Hoeffding ä¿è¯çš„æ˜¯ï¼ŒBAD Data for h çš„æ€»ä½“æ¦‚ç‡æ¯”è¾ƒå°

<h2 id="16553486c255ce8a5d8e557b67c3c9a6"></h2>


### BAD Data for Many h

 - BAD data for many h
    - <=> **no 'freedom of choice'** by A
    - <=> there exists some h such that E<sub>in</sub> and E<sub>out</sub> **far away** 

![](../imgs/TU_MLb_bad_data.png)

 - for M hypotheses , bound of â„™<sub>D</sub>[BAD D] ?

<h2 id="c82862b9ceccd7efd1ca9d2bab635903"></h2>


### Bound of BAD Data

![](../imgs/TU_MLb_bound_of_baddata.png)

 - æ¯”åŸæ¥çš„ Hoeffding å¤§äº†100å€

 - finite-bin version of Hoeffding , valid for all **M**, N , and Îµ
 - does not depend on E<sub>out</sub>(h<sub>m</sub>) , **no need to know E<sub>out</sub>(h<sub>m</sub>)**
    - f and P can stay unknown
 - 'E<sub>in</sub>(g) = E<sub>out</sub>(g)' is PAC , regardless of A 
    - å¦‚æœèµ„æ–™é‡D å¤Ÿå¤šï¼Œå¯ä»¥è¯´ æ¯ä¸€ä¸ªhéƒ½æ˜¯å®‰å…¨çš„ï¼š ä¹Ÿå°±æ˜¯è¯´
        - å¦‚æœ hypothesis setåªæœ‰æœ‰é™å¤šç¨®é¸æ“‡ æˆ‘çš„è³‡æ–™é‡å¤ å¤šï¼Œé‚£éº¼ä¸ç®¡æˆ‘çš„æ¼”ç®—æ³•Aæ€éº¼é€‰ Einè·ŸEoutéƒ½æœƒæ¥è¿‘
 - most reasonable A (like PLA/pocket) : pick the h<sub>m</sub> with **lowest E<sub>in</sub>(h<sub>m</sub>)** as g

<h2 id="d8634320f9db8392b907e680f3fbdeef"></h2>


### The 'Statistical Learning Flow'

 - if |H| = M finite , N large enough
    - for whatever *g* picked by A , E<sub>out</sub>(g) â‰ˆ E<sub>in</sub>(g)
 - if A finds one *g* with E<sub>in</sub>(g) â‰ˆ 0,
    - PAC guarantee for  E<sub>out</sub>(g) â‰ˆ 0
 - M = âˆ ? (like perceptrons) 
    - see next lecture

<h2 id="e25bb439455099bb6fd2a7b680e73f15"></h2>


# Week5 Traning versus Testing

<h2 id="8d5764cc1a7dce832f420e7096b07f1b"></h2>


## Trade-off on M

 1. can we make sure that E<sub>out</sub>(g) is close enoughto E<sub>in</sub>(g)?
 2. can we make E<sub>in</sub>(g) small enough 

 - M is the size of hyperthesis set

Question | small M | large M
--- | --- | ---
Q1  | Yes!  â„™[BAD] = 2Â·MÂ·exp(...) | No!, â„™[BAD] = 2Â·MÂ·exp(...)   
Q2  | No! too few choices  | Yes!, many choices


 - using the right M (or H) is important
 - M = âˆ doomed ?

---

 - One way to use the inequality , is to pick a tolerable difference Îµ as well as a tolerable BAD probability Î´ , and the gather data with size(N) large enough to achieve those tolerance criteria.
    - Let Î´ = 0.1 , Î´ = 0.05 , and M=100. What is the data size need ?
    - `N =   math.log( delta / ( 2*M) ) / (  -2*eps**2  ) = 414.7024820051013  `   

<h2 id="83bee210aad456619c22ae2c5e2dc963"></h2>


## Effective Number of Lines

 - Where did uniform Bound Fail ?
    - union bound :  â„™[B1 or B2 or ... or B<sub>M</sub>] â‰¤ â„™[B1]+â„™[B2]+ ... + P[B<sub>M</sub>]
    - consider for M = âˆ , the sum of right side  may > 1 , é‚£è¿™ä¸ªbound å°±æ²¡æœ‰æ„ä¹‰äº†
 - è¿™ä¸ªbound åˆ°åº•å‡ºäº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ
    - ä¸ºä»€ä¹ˆæˆ‘ä»¬å¯ä»¥ç”¨union bound ï¼Ÿ å› ä¸ºåäº‹æƒ…ä¸å¤ªä¼šé‡å 
    - äº‹å®ä¸Šï¼Œ ä¸¤ä¸ªç›¸è¿‘çš„h1,h2ï¼Œ å‘ç”Ÿåäº‹æƒ…çš„Data set é€šå¸¸éå¸¸æ¥è¿‘
    - é‡å éƒ¨åˆ†å°±é€ æˆäº†ä¸Šé™çš„ over-estimating , ä»è€Œå¯¼è‡´æ— æ³•å¤„ç†Mæ— é™å¤§çš„æƒ…å†µ
 - æ‰€ä»¥éœ€è¦æƒ³åŠæ³•æ‰¾å‡ºè¿™äº›åäº‹æƒ…é‡å çš„éƒ¨åˆ†ã€‚ 
    - ç¬¬ä¸€æ­¥ï¼Œä¹Ÿè®¸æ˜¯æˆ‘ä»¬èƒ½ä¸èƒ½æŠŠæˆ‘ä»¬çš„æ— é™å¤šä¸ªhypothesis , åˆ†æˆæœ‰é™å¤šç±»

<h2 id="624848f04dceb9194a26b7aa039879f2"></h2>


### How many Kinds of Lines for 1~4 Inputs in 2D  ?

 - 1 input
    - 2 kinds line
 - 2 inputs
    - 4 kinds line
 - 3 inputs 
    - 8 kinds line
    - å¦‚æœ ä¸‰ç‚¹å…±çº¿ï¼Œåªæœ‰ 6ç§çº¿
 - 4 inputs
    - 14 kinds line ( æ³¨æ„ä¸æ˜¯16 )
    - å…±çº¿çš„æƒ…å†µï¼Œæ›´å°‘

<h2 id="8c31579792d76e74531e953be4313740"></h2>


### Effective Number Of Lines

 - maximum kinds of lines with respect to N inputs Xâ‚,Xâ‚‚,...X<sub>N</sub>
 - must be â‰¤ 2á´º
 - wish
    - â„™[ |E<sub>in</sub>-E<sub>out</sub>| > Îµ ] â‰¤ 2Â·effective(N)Â·exp( -2ÎµÂ²N )
 - if 
    - 1. effective(N) can replace M , and 
    - 2. effective(N) â‹˜ 2á´º 
    - é‚£ä¹ˆå½“ä½ çš„Nè¶³å¤Ÿå¤§çš„æ—¶å€™ï¼Œ å³è¾¹çš„è¿™é¡¹ä¼šè¶‹è¿‘äº0
 - æ‰€ä»¥å°±ç®—æœ‰ç„¡é™å¤šæ¢ç·š å¦‚æœæˆ‘å€‘èƒ½å¤ æŠŠé€™å€‹ç„¡é™å¤šæ¢ç·šï¼Œæ›æˆé€™å€‹effectiveé€™å€‹æœ‰é™çš„æ•¸å­— è€Œä¸”é€™å€‹effectiveé€™å€‹æ•¸å­—ï¼Œå¯¦éš›ä¸Šæ¯”2çš„Næ¬¡æ–¹ ä¾†çš„å°å¾ˆå¤šçš„è©±ï¼Œæˆ‘å€‘å°±å¯ä»¥ä¿è­‰ï¼Œå¯èƒ½å¯ä»¥å­¸å¾—åˆ°æ±è¥¿ã€‚ å¥½ï¼Œæš«æ™‚ä¾†èªªé€™æ˜¯æˆ‘å€‘çš„çŒœæƒ³.

<h2 id="8fc2f515778b49e4ad6450b9cf3d6c49"></h2>


## Effective Number of Hypotheses

 - what about çº¿ä»¥å¤–çš„ hypothesis set ?  e.g. é«˜ç»´åº¦çš„hyperplane.

<h2 id="8d22448394722fd85b23c2eb8246dcb2"></h2>


### Dichotomies: Mini-hypothesis

 - H = { hypothesis  h: X â†’ { x, o} } 
 - call h(xâ‚,xâ‚‚, ... , X<sub>N</sub> ) = ( h(xâ‚),h(xâ‚‚), ... , h(X<sub>N</sub>)  ) âˆˆ {x,o}á´º 
    - a **dichotomy**: hypothesis 'limited' to the eyes of xâ‚,xâ‚‚, ... , X<sub>N</sub> 
    - dichotomy : äºŒåˆ†çš„
 - H( xâ‚,xâ‚‚, ... , X<sub>N</sub>  ) :
    - **all dichotomies 'implemented' by H on xâ‚,xâ‚‚, ... , X<sub>N</sub>**

 Â· | hypothesis H | dichotomies H( xâ‚,xâ‚‚, ... , X<sub>N</sub>  ) 
--- | --- | ---
e.g. | all lines in â„Â² | {oooo, ooox , ooxx, ...}
size | possibly infinite | upper bounded by 2á´º

 - å¦‚æœä¸€ä¸ªhypothesis setï¼Œ æ¯ä¸€ç¨®dichotomyéƒ½å¯ä»¥åš å‡ºä¾†ï¼Œæ¯ä¸€ç¨®dichotomyéƒ½å¯ä»¥åšå‡ºä¾†ä»£è¡¨ å®ƒçš„é€™å€‹æˆé•·å‡½æ•¸æ˜¯2á´º ã€‚
    - OKæˆ‘çµ¦ä½ Nå€‹é»ï¼Œä½ å¯ä»¥å§2á´º é‚£éº¼å¤šç¨®dichotomy çµ±çµ±åšå‡ºä¾†ï¼Œé‚£æˆ‘å€‘èªªé€™Nå€‹é»æˆ‘å€‘æŠŠå®ƒå–ä¸€å€‹ç‰¹åˆ¥çš„åå­—å«
    - shattered


<h2 id="dfa7e0a4bb23bbcf4e814b850274fd66"></h2>


### Growth Function 

 - æˆ‘ä»¬èƒ½ä¸èƒ½ç”¨è¿™ä¸ªdichotomy set çš„å¤§å°ï¼Œæ¥å–ä»£åŸæ¥ Hoeffding é‡Œé¢é‚£ä¸ªè®©æˆ‘ä»¬è§‰å¾—å¾ˆå›°æ‰°çš„ï¼Œå¯èƒ½æ˜¯æ— é™å¤§çš„Må‘¢ï¼Ÿ
 - å°éº»çƒ¦ï¼š
    - |  H( xâ‚,xâ‚‚, ... , X<sub>N</sub>  ) |: depend on inputs ( xâ‚,xâ‚‚, ... , X<sub>N</sub>   )
    - dichotomy set å–å†³äºæˆ‘ä»¬é€‰å¥½çš„ xâ‚,xâ‚‚, ... , X<sub>N</sub>  ï¼Œ æˆ‘ä»¬å¸Œæœ›ç§»é™¤è¿™ä¸ªä¾èµ–
 - growth function:  remove dependence by **taking max of  all possible ( xâ‚,xâ‚‚, ... , X<sub>N</sub>  )**
    - ![](../imgs/TaiU_MLb_growth_fun.png)
 - finite , upper-bounded by 2á´º

<h2 id="b7b1e314614cf326c6e2b6eba1540682"></h2>


### TODO

<h2 id="5a0ecfb80c0bc36fc6c7b593079594cb"></h2>


## Break Point 

<h2 id="b5ce0fb28ce372d75a041bef143918ef"></h2>


### The 4 Growth Functions 

case | growth Function
--- | --- 
positive rays | N+1
positive intervals | 1/2Â·NÂ²+1/2Â·N+1
convex sets |  2á´º
2D perceptrons | < 2á´º in some cases

 - What if m<sub>H</sub>(N) replaces M ?
    - polynomial: good 
    - exponential : bad 
        - convex set çš„æƒ…å†µï¼Œå°±ç®—Nå¤Ÿå¤§ï¼Œä¹Ÿä¸è§å¾— èƒ½å¤Ÿç¡®ä¿ E<sub>in</sub> å’Œ E<sub>out</sub> éå¸¸æ¥è¿‘ 
 - é‚£ä¹ˆï¼Œ2D perceptron åˆ°åº•æ˜¯å¥½çš„è¿˜æ˜¯ä¸å¥½çš„å‘¢ï¼Ÿ


<h2 id="d5be8e86759a610959b92abd0aa75199"></h2>


### Break Point of H 

 - what do we know about 2D perceptrons now ?
    - three inputs: exists shatter 
    - four inputs: 'for all' no shatter
 - if no *k* inputs can be shattered by H , call *k* a **break point** of H 
    - m<sub>H</sub>(k) < 2áµ
    - k+1, K+2, k+3, ... also break points !!
    - will study minimum break point *k*
    - ä¹Ÿå°±æ˜¯è¯´ï¼Œä»»æ„å– kä¸ªè¾“å…¥ï¼Œéƒ½æ— æ³•è¢« H shattered



case | break point
--- | --- 
positive rays | 2
positive intervals | 3
convex sets |  no break point 
2D perceptrons | 4 
  

 - conjecture æ¨æµ‹ 
    - no break point: m<sub>H</sub>(N) = 2á´º (sure!)
    - break point k: m<sub>H</sub>(N) = O( N<sup>k-1</sup>  )
        - growth functionçš„å¢é•¿é€Ÿåº¦ æä¸å¥½ä¼šå’Œ å’Œbreak point æœ‰å…³


case | break point | growth Function | O
--- | --- | --- | --- 
positive rays | 2 | N+1 | O(N)
positive intervals | 3 | 1/2Â·NÂ²+1/2Â·N+1 | O(NÂ²)
convex sets | no bp | 2á´º | O(2á´º)
2D perceptrons | 4 | < 2á´º in some cases |  O(NÂ³) ?

---

<h2 id="a191a7c2b6e0987654f6d92503bf1078"></h2>


# Week 6 Theory of Generalization  ä¸¾ä¸€åä¸‰

<h2 id="7cd79213ae5758377b9aed8accc60591"></h2>


## Restriction of Break Point

 - what 'must be true' when **minimum break point k==2** ?
    - N=1, every m<sub>H</sub>(N) == 2 by definition
    - N=2, every m<sub>H</sub>(N) < 4 by definition
        - maximum possible = 3
    - N=3, every m<sub>H</sub>(N) == 4 < 2Â³ 
        - eg. æœ€å¤šäº§ç”Ÿ4ä¸­ï¼Œå†å°è¯•åŠ å…¥ä»»æ„ä¸€ç§ dichotomyéƒ½ä¼šç ´å break point k=2
        - o o o 
        - o o x
        - o x o
        - x o o
 - break point *k*  restricts maximum possible m<sub>H</sub>(N) a lot for N > k
 - IDEA:   m<sub>H</sub>(N) 
    - â‰¤ maximum possible m<sub>H</sub>(N) given *k*
    - â‰¤ poly(N)

<h2 id="3bf7d47fb91c77c8c30afce81e8a67e1"></h2>


## Bounding Function: Basic Cases

 - bounding function B(N,k):
    - maximum **possible** m<sub>H</sub>(N) when break point = k
 - combinatorial quantity:
    - maximum number of length-N vectors with (o,x) while 'no shatter' any length-k subvectors
    - å¯ä»¥å¾ˆæŠ½è±¡åœ°æƒ³è±¡ æˆ‘æœ‰ä¸€å †çš„å‘é‡ï¼Œé€™äº›å‘é‡å…¶å¯¦å°±æ˜¯æˆ‘çš„dichotomyï¼Œ æˆ‘çš„dichotomyé€™äº›å‘é‡æ˜¯åœˆåœˆå‰å‰çµ„æˆçš„ï¼Œé€™äº›åœˆåœˆå‰å‰çš„é•·åº¦æ˜¯nï¼Œ ç„¶å¾Œå‘¢ï¼Œä½†æ˜¯æˆ‘åšä¸€å€‹é™åˆ¶ï¼Œæˆ‘èªªé€™äº›é•·åº¦æ˜¯nçš„å‘é‡ å¦‚æœæˆ‘æŠŠå®ƒçš„æŸäº›ç¶­åº¦é®èµ·ä¾†ï¼Œæˆ‘åªçœ‹å®ƒçš„å…¶ä¸­ kå€‹ç¶­åº¦çš„è©±ï¼Œ æˆ‘éƒ½ä¸å¸Œæœ›çœ‹åˆ°åœˆåœˆå‰å‰æ‰€æœ‰çš„ æƒ…å½¢ï¼Œä¹Ÿå°±æ˜¯èªªæˆ‘ä¸å¸Œæœ›çœ‹åˆ°2áµ ç§çµ„åˆã€‚ç”¨æˆ‘å€‘çš„è¡“èªä¾†èªªï¼Œå°±æ˜¯ ä¸èƒ½å‡ºç¾shatterï¼Œä¸èƒ½æŠŠé€™å€‹kå€‹é»ï¼Œé€šé€šéƒ½koæ‰ã€‚ 
 - irrelevant of the details of H , e.g. B(N,3) bounds both 
    - positive intervals (k=3)
    - 1D perceptrons (k=3)

 - new goal:  B(N,k) â‰¤ poly(N) ?



<h2 id="e51ebc67414533b0af139f8439d3408e"></h2>


### Table of Bounding Function 

 - B(N,1) = 1
 - B(N,k) = 2á´º for N < k
    - including all dichotomies not violating 'breaking condition'
 - N(N,k) = 2á´º-1 for N = k


![](../imgs/TU_MLb_tab_bounding_func_0.png)

 - quiz: for the 2D perceptrons , which of the following claim is true ?
    - minimum break point k = 2.   ( False, k = 4  )
    - m<sub>H</sub>(4) = 15.  (False,   m<sub>H</sub>(4) = 14 )
    - m<sub>H</sub>(N) < B(N,k) when N=k=minimum break point  ( True )
    - m<sub>H</sub>(N) > B(N,k) when N=k=minimum break point  ( False, B(4,4)=15 )
 - so bounding function B(N,k) can be 'loose' in bounding m<sub>H</sub>(N) 


<h2 id="2c65d0169d44316f221fe6051944e78e"></h2>


## Bounding Function: Inductive Cases

 - B(N,k) â‰¤ B(N-1,k) + B(N-1, k-1)

![](../imgs/TU_MLb_tab_bounding_func_1.png)

 - **acutally , 'â‰¤' can be '='**. 

<h2 id="20885a853d6349f8a2c1b4a13f9fd078"></h2>


### Bounding Function : The Theorem

![](../imgs/TU_MLb_bounding_func_theorem.png)

 - for fixed *k*, B(N,k) upper bounded by poly(N) 
    - â‡’ m<sub>H</sub>(N) is poly(N) **if break point exists**

<h2 id="ac41223f5ec900a59b7a7c730d2db790"></h2>


## A Pictorial Proof

 - ç¾åœ¨æœ‰äº†é€™å€‹æˆé•·å‡½æ•¸ã€ ä¸Šé™å‡½æ•¸çš„é€™äº›è§€å¿µ æˆ‘å€‘å›éé ­ä¾†åšä»€éº¼ï¼Ÿ
 - æˆ‘å€‘çš„æˆé•·å‡½æ•¸å¯èƒ½æ˜¯è·Ÿå¤šé …å¼ä¸€æ¨£è¦é•·å¤§ï¼Œé‚£æˆ‘å€‘èƒ½ä¸èƒ½ å°±æŠŠé€™å€‹æˆé•·å‡½æ•¸ä¸Ÿé€²å»åŸä¾†çš„é€™å€‹finite-binçš„Hoeffdingè£¡é¢, ç„¶å¾Œ- äº‹æƒ…å°±è§£æ±ºäº†?
 - ç•¶ç„¶äº‹æƒ…å¯¦éš›ä¸Šæ²’æœ‰é‚£éº¼ç°¡å–® æˆ‘å€‘æœ€å¾Œæœ€å¾Œèƒ½å¤ åšåˆ°çš„äº‹æƒ…ä¸¦ä¸æ˜¯é•·é€™æ¨£ï¼Œä¸¦ä¸æ˜¯èªªï¼ŒæŠŠ åŸä¾†é‚£å€‹å¤§Mç”¨å°mH of Nï¼Œç„¶å¾Œå–ä»£æ‰å°±ç®—äº† æˆ‘å€‘èƒ½å¤ åšåˆ°äº‹æƒ…æ˜¯ä»€éº¼ï¼Ÿæˆ‘å€‘èƒ½å¤ åšåˆ°çš„äº‹æƒ…æ˜¯ ä¸€å€‹é•·å¾—å·®ä¸å¤šçš„ç‰ˆæœ¬ï¼Œé€™å·®ä¸å¤šçš„ç‰ˆæœ¬è£¡é¢æœ‰å¹¾ä»¶äº‹æƒ…ï¼š
    - ç¬¬ä¸€ä»¶äº‹æƒ…æ˜¯ï¼Œæˆ‘è¦Nå¤ å¤§ 
    - ç„¶ååˆå¤šå‡ºäº†ä¸€äº›å¸¸æ•°

![](../imgs/TU_MLb_pictorial_proof_0.png)

 - è¿™ä¸ªå…¬å¼çš„è¯æ˜éœ€è¦ä¸€äº›æ•°å­¦ä¸Šçš„æ¨å¯¼ç­‰ç­‰.

---

<h2 id="942c21d4344c1e2c04cd9cb2b0635e7f"></h2>


### That's All !

 - Vapnik-Chervonenkis (VC) bound:

![](../imgs/TU_MLb_vc_bound.png)


 - Q: For positive rays , m<sub>H</sub>(N) = N+1. Plug it into the VC bound for Îµ=0.1, and N=10000. What is VC bound of BAD events?
 - A: 0.298...
 - é€™ä¸¦ä¸æ˜¯ ä¸€å€‹å¾ˆå°çš„æ•¸å­—
    - æˆ‘å€‘åœ¨ç®—é€™å€‹boundçš„éç¨‹ä¸­ï¼ŒèŠ±äº†å¾ˆå¤šçš„è¿‘ä¼¼åœ¨è£¡é¢ é‚£æ‰€ä»¥é€ æˆäº†é€™å€‹boundå…¶å¯¦ä¸è¦‹å¾—çœŸçš„æ˜¯é‚£éº¼æº– 
    - é€™å€‹boundæ—¢ç„¶ä¸é‚£éº¼æº– æˆ‘å€‘ç‚ºä»€éº¼è¦èŠ±é€™éº¼å¤§çš„åŠ›æ°£å»æ¨å°ã€ å»è¬›å®ƒ? 


---

<h2 id="15173fa6f984d4d655f1b15b08355016"></h2>


# Week7 The VC Dimension

 - ä¸Šå‘¨è®²åˆ°äº†èˆ‰ä¸€åä¸‰
 - èˆ‰ä»€ä¹ˆï¼Œèˆ‰ä¾‹å­? åƒæˆ‘å€‘è¨“ç·´çš„æ™‚å€™åœ¨èˆ‰ä¾‹å­ 
 - ç„¶å¾Œåä¸‰ä¹Ÿå°±æ˜¯èªªæˆ‘å€‘æ¸¬è©¦çš„æ™‚å€™æœƒè·Ÿæˆ‘å€‘è¨“ç·´çš„æ™‚å€™çš„è¡¨ç¾æ˜¯é¡ä¼¼ 
 - æˆ‘å€‘ä¸Šå€‹ç¦®æ‹œè·¨å‡ºäº†å¾ˆé‡è¦çš„ä¸€æ­¥ï¼Œæˆ‘å€‘èªªæˆ‘å€‘å¯ä»¥ ç¢ºä¿æˆ‘å€‘çš„Einï¼Œä¹Ÿå°±æ˜¯æˆ‘å€‘è¨“ç·´æ™‚å€™çš„è¡¨ç¾ï¼Œè·Ÿæˆ‘å€‘çš„Eout æˆ‘å€‘æ¸¬è©¦æ™‚å€™çš„è¡¨ç¾æ˜¯é¡ä¼¼çš„ã€‚
 - ä»€éº¼æ™‚å€™å¯ä»¥é€™æ¨£ç¢ºä¿å‘¢ï¼Ÿ
    - æˆ‘å€‘åªè¦ æˆ‘å€‘çš„æˆé•·å‡½æ•¸åœ¨æŸå€‹åœ°æ–¹éœ²å‡ºäº†ä¸€ç·šæ›™å…‰ï¼Œå¹¶ä¸” å¦‚æœæˆ‘å€‘çš„è³‡æ–™é‡å¤ å¤šçš„æ™‚å€™ï¼Œæˆ‘å€‘å°±å¯ä»¥ç¢ºä¿Einè·ŸEoutæ˜¯æ¥è¿‘çš„ 
 - é‚£å¾é€™è£¡å‡ºç™¼å‘¢ï¼Œæˆ‘å€‘å°±è¦é–‹å§‹çœ‹çœ‹é€™å€‹ä¸€ç·šæ›™å…‰çš„æ„ç¾© æ˜¯ä»€éº¼ï¼Ÿå¯¦éš›ä¸Šé€™å°±æœƒå¼•åˆ°æˆ‘å€‘å¾…æœƒå…’å°é€™å€‹VC dimension.

<h2 id="03b215c6ba1906c71c6856766d5a91dc"></h2>


## Definition of VC Dimension

 - the formal name of **maximum non-**break point
 - Definition:
    - VC dimemsion of H , denoted d<sub>VC</sub>(H) is 
        - **largest** N for which m<sub>H</sub>(N) = 2á´º
    - the **most** inputs H that can shatter
    - d<sub>VC</sub> = 'minimum k' -1
    - æ³¨æ„è¦è€ƒè™‘åˆ°æ‰€æœ‰å¯èƒ½çš„è¾“å…¥
 - N â‰¤ d<sub>VC</sub>  â‡’  H can shatter some N inputs 
 - N > d<sub>VC</sub>  â‡’  N is a break point for H
    - k > d<sub>VC</sub>  â‡’  k is a break point for H
    
<h2 id="ef6ecf6a562931f940c6f8fa8f315ec9"></h2>


### The 4 VC Dimensions

 Â· | VC Dimension
--- | --- 
positive rays |  1
positive intervals | 2 
convex sets | âˆ 
2D perceptrons | 3 

 - VC dimension æ˜¯ finite çš„ hypothesis set å°±æ˜¯å¥½çš„ hypothesis set

<h2 id="2ab6a2e714fe88daa38d0ebadbd820b7"></h2>


### VC Dimemsion and Learning

 - finite d<sub>VC</sub> â‡’ g 'will' generialize ( E<sub>out</sub>(g) â‰ˆ E<sub>in</sub>(g)   )
    - regardless of learning algorithm A
        - å³ä¾¿æ˜¯å¾ˆç³Ÿç³•çš„ç®—æ³•ï¼Œ E<sub>in</sub> å’Œ E<sub>out</sub> ä¹Ÿæ˜¯å¾ˆæ¥è¿‘çš„ï¼Œåªæ˜¯è¿™æ ·çš„æƒ…å†µå¯¹æˆ‘ä»¬æ²¡ä»€ä¹ˆå¥½å¤„
    - regardless of input distribution P 
    - regardless of target function f 

 - Q: If there is a set of N inputs that cannot be shattered by H. Based only on this information , what can we conclude about d<sub>VC</sub>(H) ?
    1. d<sub>VC</sub>(H) > N
    2. d<sub>VC</sub>(H) = N
    3. d<sub>VC</sub>(H) < N 
    4. no conclusion can be made.
 - A:4  
    - It is possible that there is another set of N inputs that can be shatterred, which means d<sub>VC</sub>(H) â‰¥ N.
    - It is also possible that no set of N input can be shattered , which means d<sub>VC</sub>(H) < N.
    - Neither cases can be ruled out by one non-shattering set.

<h2 id="010c451c23e144261c78eed3918be60c"></h2>


## VC Dimension of Perceptrons

 - 1D perceptron (pos/neg rays): d<sub>VC</sub> = 2
 - 2D perceptrons : d<sub>VC</sub> = 3
 - d-D preceptrons : d<sub>VC</sub> = ? 
    - guess d + 1 ?  

 - how to proof ? 2 steps 
    1. prove  d<sub>VC</sub>  â‰¥ d+1
    2. prove  d<sub>VC</sub>  â‰¤ d+1

<h2 id="0bcebb5418877c668b7f8078fd53e2a6"></h2>


### d<sub>VC</sub> â‰¥ d+1

 - how ?
    - There are **some** of d+1 **inputs** we can shatter.
 - some 'trivial' inputs
    - ![](../imgs/TU_MLb_vd_dim_trivial_input.png)
    - æœ€å·¦ç°è‰²çš„1 æ˜¯æ’å…¥çš„ bias
 - åœ¨äºŒç»´ä¸Šï¼Œåªæœ‰3ä¸ªç‚¹ (0,0) , (1,0) , (0,1) ,   è¿™ä¸‰ä¸ªç‚¹æ˜¯å¯ä»¥è¢«shatterçš„
 - æˆ‘ä»¬éœ€è¦è¯æ˜çš„æ˜¯ï¼Œ åœ¨d ç»´åº¦çš„æ—¶å€™ï¼Œè¿™äº›d+1ä¸ªç‚¹ä¹Ÿæ˜¯å¯ä»¥è¢«shatterçš„
 - note: X is **invertible!**
    - é€†çŸ©é˜µå­˜åœ¨å¯¹æˆ‘ä»¬æœ‰ä»€ä¹ˆæ„ä¹‰å‘¢ï¼Ÿ
 - å›æƒ³ä¸€ä¸‹ä»€ä¹ˆæ˜¯ shatterï¼Ÿ ç»™å®šxoæ’åˆ—ç»„åˆä¸­ä»»æ„ä¸€ä¾‹( xxooxoxo ... ) -- ç§°ä¸ºy = ( yâ‚,...y<sub>d+1</sub> ) , æˆ‘éƒ½è¦èƒ½å¤Ÿåšåˆ° X ä¹˜ä¸ŠæŸä¸ªwï¼Œå–ç¬¦å·åï¼Œè¦ç­‰äºY 
    - sign(Xw) = y 
 - æ€ä¹ˆåšåˆ°è¿™ç‚¹ï¼Ÿ  å¦‚æœ æˆ‘çš„ XÂ·w çš„ç»“æœç›´æ¥ç­‰äºyï¼Œ é‚£ä¹ˆå–ç¬¦å·åï¼Œè¿˜æ˜¯ç­‰äºy
    - sign(Xw) = y â‡  Xw = y
 - è¿™æ ·wä¸€å®šèƒ½æ‰¾åˆ°å—ï¼Ÿ èƒ½ï¼  w = Xâ»Â¹y
    - sign(Xw) = y â‡  Xw = y    â‡”  w = Xâ»Â¹y 
 - proven !!

 
<h2 id="b23a9ffbf5d00fecd76ee661c5bae97c"></h2>


### d<sub>VC</sub>  â‰¤ d+1

 - how ?
    - We can not shatter **any** set of d+2 inputs
 - A 2D Special Case
    - æˆ‘ä»¬åœ¨2Dä¸ŠåŠ å…¥ä¸€ä¸ªç‚¹ (1,1)
    - ![](../imgs/TU_MLb_special_2d_case.png)
    - x4 = x2 + x3 - x1 , linear dependency , ä¼šé™åˆ¶æˆ‘ä»¬äº§ç”Ÿdichotomy çš„ç»„åˆ
 - d-D General Case 
    - å¯¹ D+1 æ–¹é˜µåŠ å…¥ä»»æ„ä¸€ç¬” èµ„æ–™ x<sub>d+2</sub> , äº§ç”Ÿçš„ D+1 x  D+2 çŸ©é˜µï¼Œ è¡Œæ¯”åˆ—å¤šï¼Œè¡Œ å¿…ç„¶ linear dependency. 

---

<h2 id="a7ab380b76ef972f810d737e226f4fe6"></h2>


## Physical Intuition of VC Dimension

<h2 id="364130468e37b396a03b78270e87774b"></h2>


### Degrees of Freedom
 
![](../imgs/TU_MLb_vcd_phy_intuition.png)


 - hypothesis parameters w = ( wâ‚€,wâ‚,...,w<sub>d</sub> ) :
    - **creates degrees of freedom**
 - hypothesis quantity M = |H| :
    - 'analog' degrees of freedom
    - æ—‹é’®å¯ä»¥æŒ‡å‘ä»»æ„å¯èƒ½çš„æ–¹å‘
 - hypothesis 'power' d<sub>vc</sub> = d+1:
    - **effective 'binary' degrees of freedom**
 - VC Dimensionçš„ç‰©ç†æ„ç¾©ï¼Œå¤§è‡´ä¸Šå°±æ˜¯æˆ‘çš„hypothesis set ,åœ¨æˆ‘è¦åšäºŒå…ƒåˆ†é¡çš„ç‹€æ³ä¸‹ï¼Œåˆ°åº•æœ‰å¤šå°‘çš„è‡ªç”±åº¦ï¼Ÿ
    - è¡¡é‡ é€™å€‹è‡ªç”±åº¦ï¼Œä¹Ÿå°±å‘Šè¨´æˆ‘å€‘é€™å€‹hypothesis setåˆ°åº•èƒ½å¤ åšåˆ°å¤šå°‘äº‹æƒ…ï¼Ÿåšåˆ°ä»€éº¼äº‹æƒ…ï¼Ÿ ç”¢ç”Ÿå¤šå°‘çš„çš„dichotomy. é€™å€‹hypothesis set å¼ºä¸å¼º,æœ‰å¤šå¼º?
    - w çš„é•¿åº¦ï¼Œå°±æ˜¯æˆ‘ä»¬æ‹¥æœ‰çš„æ—‹é’®ä¸ªæ•°

<h2 id="c30d7a9f1110a7115386a1a1449444ff"></h2>


### Two Old Friends

 - Positive Rays ( d<sub>vc</sub>=1 ) 
    - ![](../imgs/TU_MLb_pos_ray_free.png)
    - åªæœ‰ä¸€ä¸ªå¯è°ƒèŠ‚çš„ æŒ‰é’®
 - Positive Intervals ( d<sub>vc</sub>=2 )
    - ![](../imgs/TU_MLb_pos_intval_free.png)
    - æœ‰ 2ä¸ªå¯è°ƒèŠ‚ æŒ‰é’®

<h2 id="668bae56a418c6c053a975bbc41bef59"></h2>


### M and d<sub>vc</sub>

 - copied from Lecture 5
    1. can we make sure that E<sub>out</sub>(g) is close enoughto E<sub>in</sub>(g)?
    2. can we make E<sub>in</sub>(g) small enough 

![](../imgs/TU_MLb_M_vs_VC.png)

 - using the right d<sub>vc</sub> ( or H ) is important


<h2 id="4fa217a4f25efe58ca38efc2921a9ece"></h2>


## Interpreting VC Dimension 

<h2 id="cf1207c2cd37bca9c19baefe3be5f449"></h2>


### VC Bound Rephrase:  Penalty for Model Complexity

![](../imgs/TU_MLb_vcbound_rephrase.png)

 - è¿™ä¸ªå¼å­çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ
    - æœ‰å¾ˆé«˜çš„æ©Ÿæœƒ Einè·ŸEoutçš„å·®åˆ¥æœƒè¢«é™åˆ¶åœ¨ é€™å€‹æ ¹è™Ÿè¡¨è¾¾å¼é‡Œé¢, ä¹Ÿå°±æ˜¯èªªæˆ‘å€‘ç¾åœ¨æè¿°çš„æ˜¯å¥½äº‹æƒ…ç™¼
 - Eout ä¼šè¢«é™åˆ¶åœ¨ ä¸¤ä¸ªéƒ¨åˆ†ä¸­é—´ï¼Œåœ¨ç»Ÿè®¡ä¸Šï¼Œå¾ˆåƒ  confidence interval -- ä¿¡èµ–åŒºé—´
    - é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šæ¯”è¾ƒåœ¨æ„ï¼Œ å³è¾¹è¿™éƒ¨åˆ†
    - ![](../imgs/TU_MLb_vcbound_rephrase.png)
    - âˆš... :  penalty for **model complexity**
        - æˆ‘ä»¬ hypothesis set ä»Šå¤©åˆ°åº•æœ‰å¤š powerful , ä½†æ˜¯ æˆ‘åœ¨ generalizationçš„æ—¶å€™è¦ä»˜ä»˜å‡ºçš„ä»£ä»·å°±æ˜¯è¿™ä¹ˆå¤š
        - è¡¨ç¤º: Î©( N, H, Î´ ) 
            - N: æœ‰å¤šå°‘ä¸ªç‚¹
            - H: ä½ çš„VC dimension
            - Î´: ä½ è§‰å¾—ä½ æœ‰å¤šå¹¸è¿

<h2 id="fd5261ae2542df593b9ab1967d1212b7"></h2>


### THE VC Message

 - with **a high probability** ,
    - ![](../imgs/TU_MLb_vc_message.png)

![](../imgs/TU_MLb_vc_message_graph.png)

 - æŠŠ Ein åšå¾—æ›´ä½ï¼Œå°±éœ€è¦ä¸€ä¸ªå¼ºå¤§çš„ hypothesis set, ä½†æ˜¯è¿™è¦ä»˜å‡º å¾ˆå¤§çš„ model complexity çš„ä»£ä»·ï¼Œä¸è§å¾—æ˜¯æœ€å¥½çš„é€‰æ‹©ã€‚

<h2 id="2103d3a1527ad166beedae934932a08e"></h2>


### VC Bound Rephrase : Sample Complexity

 - æ¨£æœ¬çš„è¤‡é›œåº¦æˆ–å¯¦éš›ä¸Šå°±æ˜¯è³‡æ–™é‡çš„è¤‡é›œåº¦ã€‚ 
    - åƒä½ è€æ¿é–‹å‡ºäº†ä¸€äº›è¦æ ¼ï¼Œæˆ‘å¸Œæœ›ä½ çš„Einè·ŸEoutå·®æœ€å¤š0.1ï¼Œ ç„¶å¾Œå‘¢æˆ‘å¸Œæœ›å£äº‹æƒ…ç™¼ç”Ÿçš„æ©Ÿæœƒæœ€å¤šæœ€å¤šå°±10%ï¼Œä¹Ÿå°±æ˜¯èªªæˆ‘å€‘ 90%çš„æ™‚å€™ç›¸ä¿¡å¥½äº‹æƒ…æœƒç™¼ç”Ÿã€‚ ç„¶å¾Œå†ä¾†æˆ‘çµ¦ä½ ä¸€å€‹learning modelï¼Œ å‡è¨­å®ƒVC Dimensionæ˜¯3ï¼Œä¾‹å¦‚èªª2Dçš„perceptronï¼Œ
    - éº¼ä½ è€æ¿å°±å•äº†ï¼Œä½ åˆ°åº•è¦å¤šå°‘çš„è³‡æ–™æ‰å¤ ï¼Ÿ 
 - Îµ = 0.1 , Î´ = 0.1 (90% good thing)  , d<sub>vc</sub> = 3 , want 4(2N)<sup>d<sub>vc</sub></sup>Â· exp( -1/8Â·ÎµÂ²Â·N ) â‰¤ Î´ 
 - Sample complexity:
    - **need N â‰ˆ 10000 Â· d<sub>vc</sub> in theory**.
    - practical rule of thumb:   **N â‰ˆ 10 Â· d<sub>vc</sub>  often enough !**
 - æ‰€ä»¥ VC bound å…¶å® éå¸¸å®½æ¾çš„. why ?
    - Hoeffding for unknown E<sub>out</sub> :   **any distribution, any target**
    - m<sub>H</sub>(N)  instead of |H(Xâ‚,...,X<sub>N</sub>)|   :  **'any' data** 
        - æˆé•¿å‡½æ•°
    - N<sup>d<sub>vc</sub></sup> instead of m<sub>H</sub>(N)  :   **'any' H of same d<sub>vc</sub>** 
        - ä¸Šé™çš„ä¸Šé™
    - union bound on worst cases:   **any choice made by A**

<h2 id="290612199861c31d1036b185b4e69b75"></h2>


### Summary

 - VC Bound æ˜¯æ©Ÿå™¨å­¸ç¿’è£¡é¢æœ€é‡è¦çš„ä¸€å€‹ç†è«–å·¥å…·ã€‚ 
 - VC Dimension æ˜¯æœ€å¤§çš„ non-break pointã€‚  åœ¨Perceptrons ä¸Šï¼Œå®ƒæ°å¥½å°±æ˜¯ d + 1ï¼Œå°±æ˜¯æˆ‘çš„ Perceptrons è£¡é¢åˆ°åº•æœ‰å¤šå°‘å€‹ç¶­åº¦ã€‚ 
 - ç‰©ç†æ„ç¾©ä¸Šï¼ŒVC Dimension å‘Šè¨´æˆ‘å€‘èªªï¼Œå“æˆ‘å€‘çš„ é€™å€‹ Hypothesis Set åˆ°åº•æœ‰å¤šå°‘çš„è‡ªç”±åº¦ï¼Œå¤§æ¦‚å°±æ˜¯å•Š æœ‰å¤šå°‘çš„è‡ªç”±çš„é€™å€‹æ—‹éˆ•
 - VC Dimension ä»¥ç”¨ä¾†çœ‹çœ‹æˆ‘å€‘çš„ Modelï¼Œ æˆ‘å€‘çš„ Hypothesis Set åˆ°åº•æœ‰å¤šè¤‡é›œï¼Œç„¶å¾Œæˆ‘å€‘èƒ½å¤ ç”¨å®ƒä¾†æ±ºå®šï¼Œæˆ‘å€‘å¤§æ¦‚éœ€è¦å¤šå°‘çš„ è³‡æ–™æ‰å¤ ã€‚ 

---

<h2 id="6d87d0270ecffd6ed877bf64cf997573"></h2>


# Week 8  Noise and Error

<h2 id="a3f810ce646f0936df658a4d30a15b6d"></h2>


## Noise and Probabilistic Target

 - å¦‚æœæˆ‘å€‘çš„hypothesis setçš„VC Dimensionæ˜¯æœ‰é™çš„ï¼Œç„¶å¾Œæˆ‘å€‘æœ‰è¶³å¤ å¤šçš„è³‡æ–™ï¼Œ æˆ‘å€‘çš„æ¼”ç®—æ³•åˆèƒ½å¤ æ‰¾åˆ°ä¸€å€‹hypothesiså®ƒçš„Einå¾ˆä½çš„è©±ï¼Œé‚£éº½æˆ‘å€‘å°±å¤§æ¦‚ å­¸åˆ°äº†æ±è¥¿
 - ä¸è¿‡ï¼Œæˆ‘å€‘åŸä¾†çš„VC Boundï¼Œåœ¨æ¨å°çš„éç¨‹ä¸­ï¼Œ æˆ‘å€‘æœ‰ä¸€äº›å‡è¨­ï¼Œæˆ‘å€‘æ€éº½æ¨£æ”¾å¯¬é€™äº›å‡è¨­ï¼Œè®“æˆ‘å€‘å°é€™å€‹VC Boundçš„VC Dimensionçš„äº†è§£ï¼Œ å¯ä»¥æ”¾å¯¬åˆ°æ›´å¤šä¸åŒçš„å•é¡Œä¸Šé¢ã€‚
 - å¦‚æœæˆ‘ä»¬çš„èµ„æ–™ä¸­ åŠ ä¸ŠNoiseï¼Œ å°æˆ‘å€‘ä¹‹å‰æ•´å€‹ç†è«–çš„æ¨å°æœƒä¸æœƒæœ‰å½±éŸ¿å‘¢ï¼Ÿ

---

 - Target Distribution P(y|x)
    - characterizes behavior of "**mini-target**" on one **x** 

 - can be viewed as 'ideal mini-target' + noise , e.g.
    - P('o'|x)=0.7 , P('Ã—'|x) = 0.3
    - ideal mini-target f(x) = 'o'
    - 'flipping noise level' = 0.3
 - deterministic target *f* : **special case of target distribution**
    - P(y|x) = 1 ,  if y = f(x)
    - P(y|x) = 0 ,  if y â‰  f(x)
 - goal of learning
    - predict **ideal mini-target (w.r.t P(y|x))** on *often-seen inputes(w.r.t P(x))*.
    - åœ¨å¸¸å¸¸sampleåˆ°çš„èµ„æ–™é‡Œé¢ï¼Œè¦é¢„æµ‹çš„å¥½ï¼Œè¿™å°±æ˜¯ MLè¦åšåˆ°çš„äº‹æƒ…ã€‚


<h2 id="d2d99ba3e6ff66dfef2164352b2c98a1"></h2>


## Error Measure

- naturally considered
    - out-of-sample: averaged over unknown x
    - pointwise: evaluated on one x
    - classification: [ prediction â‰  target ]
        - often also called â€˜0/1 errorâ€™

<h2 id="9a0ea46a375f6caa83eb7371460f252e"></h2>


### Pointwise Error Measure

![](../imgs/TU_MLb_pointwiseerr.png)

<h2 id="6c2d0a3129771ec5553a0a7b40c04800"></h2>


### Two Important Pointwise Error Measures

![](../imgs/TU_MLb_pointwiseerr2.png)

<h2 id="4800505041049669fa7b042ae638b020"></h2>


### Ideal Mini-Target

- interplay between noise and error:
    - P(y|x) and err define ideal mini-target f(x)

- `P(y = 1|x) = 0.2, P(y = 2|x) = 0.7, P(y = 3|x) = 0.1`

![](../imgs/TU_MLb_idealminitarget.png)


<h2 id="2187b661cffc7f0490d1f364850ab22a"></h2>


## Algorithmic Error Measure


<h2 id="46b76063f10f6397dfd67504c091971d"></h2>


### Choice of Error Measure

- Fingerprint Verification : f = 
    - +1, you
    - -1, intruder
- two types of error: *false accept* and **false reject**

 Â· | g +1  | g -1 
--- | --- | ---
f +1 | no error | **false reject**
f -1 | *false accept* | no error

- 0/1 error penalizes both types equally


- supermarket: fingerprint for discount
    - **false reject: very unhappy customer, lose future business**
    - *false accept: give away a minor discount, intruder left fingerprint :-)*


 cost matrix | g +1  | g -1 
--- | --- | ---
f +1 | 0 |  **10**
f -1 | *1*| 0

- CIA: fingerprint for entrance
    - *false accept: very serious consequences!*
    - **false reject: unhappy employee, but so what? :-)**

 cost matrix  | g +1  | g -1 
--- | --- | ---
f +1 | 0 |  **1**
f -1 | *1000*| 0

<h2 id="df794cf0b049355a25a3dc69dacf670b"></h2>


### Take-home Message for Now

- **err is application/user-dependent**
- Algorithmic Error Measures err'
    - å¦‚æœç”¨æˆ·èƒ½è¯´å‡ºä»–ä»¬æƒ³è¦ä»€ä¹ˆï¼Œæœ€å¥½ï¼Œ å¦‚æœä¸èƒ½çš„è¯:
- 1. plausible:  æˆ‘ä»¬è‡ªå·±è®¾è®¡ä¸€ä¸ªå¯ä»¥è¯´æœæˆ‘ä»¬è‡ªå·±çš„ç®—æ³•
    - 0/1: minimum â€˜flipping noiseâ€™ --NP-hard to optimize, remember? :-)
    - squared: minimum Gaussian noise
-2. æˆ– è®¾è®¡ä¸€ä¸ªå®¹æ˜“ä¸€ç‚¹çš„ç®—æ³•     easy to optimize for A
    - closed-form solution
    - convex objective function


<h2 id="77f01330c84d591c46ceb54c844f4fbe"></h2>


## Weighted Classification

- CIA Cost (Error, Loss, . . .) Matrix

 Â· | g +1  | g -1 
--- | --- | ---
f +1 | 0 |  **1**
f -1 | *1000*| 0

- weighted classification:
    - different â€˜weightâ€™ for different (x, y)

<h2 id="aae4803feb7519588dfdfd0080792d30"></h2>


### Minimizing E<sub>in</sub> for Weighted Classification

- NaÃ¯ve Thoughts
    - PLA: doesnâ€™t matter if linear separable. :-)
    - pocket: modify pocket-replacement rule
        - pocket: some guarantee on E<sub>in</sub><sup>0/1</sup>
        - modified pocket: similar guarantee on E<sub>in</sub><sup>W</sup> ?

<h2 id="62efbe469e781542f448dfebe1e436d8"></h2>


### Systematic Route

- é€šè¿‡å¤åˆ¶ç›¸å…³èµ„æ–™ï¼Œè½¬åŒ–W æˆç­‰ä»·çš„ 0/1é—®é¢˜

![](../imgs/TU_MLb_weighted_pocket.png)

<h2 id="55d55c54777e3438c85343abfae313ac"></h2>


### Weighted Pocket Algorithm

- using â€˜virtual copyingâ€™, weighted pocket algorithm include:
    - weighted PLA:
        - randomly check âˆ’1 example mistakes with **1000** times more probability (for CIA cost)
    - weighted pocket replacement:

- systematic route (called â€˜reductionâ€™): **can be applied to many other algorithms!**




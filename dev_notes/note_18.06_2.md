...menustart

 - [18.Determinants](#ae8178b3d3c78ee97adfcc298fe0b11e)
     - [Determinants , det A = |A|](#fc7bcc02eaa53ccf05c2b3f48461de47)
     - [Signed](#71fed0c3428bf1a2e19af257c4bac379)
     - [Properties](#9fc2d28c05ed9eb1d75ba4465abf15a9)
 - [19. Formular for Determinant](#077014dde3a16154d48a5807d1c351bb)
     - [Formula for detA ( n! terms )](#a4ea43d88bcd4db4f7d7fe3abe7052b4)
     - [Cofactor formula](#d02910574d79a0726db429910342d33e)
 - [20. APPLICATIONS OF DETERMINANTS](#d32bea3e6901eaca807e7ff0a03afc52)
     - [Formula for Aâ»Â¹](#23d31b234b8839bc2675b16fff1de2da)
     - [Cramers Rule for x=Aâ»Â¹b](#8d1f376d9179ba4326b17606a2f64136)
     - [| detA | = volume of box](#b99ff4a9543c3ccbfac05190cc07dd68)
 - [21. EigenValues - EigenVectors](#2a051e916e56ff2ad1a6b47745f85a6b)
     - [det\[A-Î»I\] = 0](#9e08a229f7feafd17be0f57fed7d544d)
     - [Trace = Î»â‚+Î»â‚‚+ ... + Î»<sub>n</sub>](#95946abeb88b298e53a6c970166fb7aa)
 - [22. Diagonalizing](#7082803699d165541950b7de2ae708e3)
     - [Diagonalizing a matrix](#7b86df847bba7602c3c3d17d03b2841f)
     - [Powers of A / equation u<sub>k+1</sub> = Au<sub>k</sub>](#d975eb14bcadcace9392fa08591c6162)
     - [Recap](#8912c5512db9003e5c8ce07b7ff36a88)
 - [23.](#d4fbca7834560377a100c91364ec53ec)
     - [Differential Equations  du/dt=Au](#9c0f59a904be7c01984a683e511ca015)
     - [Exponential e<sup>At</sup> of a matrix](#627bb884755ff95a306e6587c9bed913)
 - [24.](#21250cdbabe0f7964564376f3d523f38)
     - [Markov matrices](#0134f701c2230c4f2fc4488d2214160a)
     - [Fourier Series and projections](#1c401566682351198dc0b2cf460094c1)
 - [25.](#13f3022d707d2db79b2bbc152c21c483)
     - [Symmetric Matrices](#b899f85c23e42aea33f7684a076389ca)
     - [26.](#dd3391b01e6d23199e9d35f0a4f5d427)
         - [Complexo](#023666df9ae9590deecc67a35fb5839e)
         - [Complex Vectors , length](#e5c0693de0b5a3e017ca1bcabf2a8444)
         - [Complex Matrices](#9ff43aa3e54f31207991c2e82ccf292d)
     - [27](#02e74f10e0327ad868d138f2b4fdd6f0)
         - [Positive Definite Matrix (Tests)Â²](#f374bd8e0ba50a2cca911b7d328d59fb)
         - [Ellipsoids in Râ¿](#e77fe7259561aabc1f580cd55ad18daa)
     - [28](#33e75ff09dd601bbe69f351039152189)
         - [Aáµ€A is positive definite](#714c646ab1eb70195e6ae70fbf2681b6)
         - [Similar Matrices A,B](#ae75137209c482f86ac4dd1100e7e6af)
     - [29](#6ea9ab1baa0efb9e19094440c317e21b)
         - [Singular Value Decomposition = SVD](#cdbd45bcf4e7ece0654c0d7b925abb97)
     - [30](#34173cb38f07f89ddbebc2ac9128303f)
         - [Linear Transformation](#1b08f35b032e4fa969a0b2e5ebcb2c03)

...menuend


<h2 id="ae8178b3d3c78ee97adfcc298fe0b11e"></h2>


# 18.Determinants 

Up to now we paid a lot of attention to rectangular matrices. 

Now, concentrating on square matrices. Determinants and Eigen values are big, big chunk of 18.06.

<h2 id="fc7bcc02eaa53ccf05c2b3f48461de47"></h2>


## Determinants , det A = |A| 

Every square matrix has a number associated with , called its determinant. 

- det A = 0 , means A is singular.  
- det A !=0 , means A is invertible.

<h2 id="71fed0c3428bf1a2e19af257c4bac379"></h2>


## Signed

Determinant is signed.

<h2 id="9fc2d28c05ed9eb1d75ba4465abf15a9"></h2>


## Properties 

3 base properties:

1. det I = 1.
2. Exchanging rows reverse sign of det.  
3. The determinant depends linearly on **one** row. (single row linearity)
    - 3a: Add vectors in a row
        - 
        ```
        |a+a' b+b'| =|a b| + |a' b'|
        |c    d   |  |c d|   |c  d |
        ```
    
    - 3b: Multiply by t in row
        - 
        ```
        |ka kb| = k |a b|
        | c  d|     |c d|
        ```
        - det2A = 2â¿ detA
        - det(A+B) â‰  detA + detB

---

<details>
<summary>
p4 ~ p10
</summary>

 - p4: property 2 also said , dup rows => det=0 
 - p5: elimination ( - k rowi from rowj ) doesn't change det.
    - æ•°å­¦ä¸Šï¼Œå‡å»çš„è¿™éƒ¨åˆ† ä½¿ç”¨p3 å¯ä»¥åˆ†ç¦»å‡ºå»ï¼Œè¿™éƒ¨åˆ†çš„det ä¸º0.
    - é›†åˆä¸Šï¼Œå¹³è¡Œå››è¾¹å½¢,åº•ä¸å˜å‘ç”Ÿåˆ‡å˜ï¼Œé¢ç§¯ä¸å˜
 - p6: zero row => det=0.
    - p3b, in case a=0,b=0 , KÂ·det=det => det=0.
 - p7: triangular matrix, det = product of pivots
 - p8: If A is singular, then det A = 0. If A is invertible, then det A â‰  0.
 - p9: det AB = detA * detB  (very valuable property)
    - detAâ»Â¹
 - p10: detAáµ€ = detA 
    - è¡Œåˆ—å¼ï¼Œæ‰€æœ‰è¡Œçš„æ€§è´¨ï¼Œå¯¹åˆ—åŒæ ·æœ‰æ•ˆ

</details>



<h2 id="077014dde3a16154d48a5807d1c351bb"></h2>


# 19. Formular for Determinant

<h2 id="a4ea43d88bcd4db4f7d7fe3abe7052b4"></h2>


## Formula for detA ( n! terms )

 - use property 3 to break down each row 
    - [a b c] = [a 0 0] + [0 b 0] + [0 0 c]
 - the determinant of A can be expanded into nâ¿ terms 
 - a lot terms has 0 det,  they can be removed.
 - The nonzero terms have to come in different columns and rows. 
    - ![](../imgs/LA_det_3x3_6terms.png)
 - ![](../imgs/LA_det_bigFormula.png)
 
<h2 id="d02910574d79a0726db429910342d33e"></h2>


## Cofactor formula

Cofactor is a way of breaking up above big formula that connects the nxn determinant to a determinant one smaller. 

![](../imgs/LA_det_cofactor_filter.png)

- **The determinant of A is a combination of any row i times its cofactors**.
    - **det A by cofactors:** det A = aáµ¢â‚Cáµ¢â‚ + aáµ¢â‚‚Cáµ¢â‚‚ + ... + aáµ¢<sub>ğ‘›</sub>Cáµ¢<sub>ğ‘›</sub>. (10)
    - The cofactor is the determinant of Máµ¢â±¼ , with the correct sign:
        - **delete row i and column j:** Cáµ¢â±¼ = (-1)â±âºÊ² detMáµ¢â±¼.   (11)


<h2 id="d32bea3e6901eaca807e7ff0a03afc52"></h2>


# 20. APPLICATIONS OF DETERMINANTS

<h2 id="23d31b234b8839bc2675b16fff1de2da"></h2>


## Formula for Aâ»Â¹

![](../imgs/la_det_ap_inverse.gif)

<details>
<summary>
ACáµ€ = (detA) I
</summary>

![](../imgs/LA_det_apply_inverse3.png)

The critical question is: Why do we get zeros off the diagonal? 

The answer is: we are actually computing the determinant of a new matrix which has equal rows. 

</details>

<h2 id="8d1f376d9179ba4326b17606a2f64136"></h2>


## Cramers Rule for x=Aâ»Â¹b

![](../imgs/la_1806_cramer_rule.gif)

What do I get in Cáµ€b ? What's the first entry of Cáµ€b ?  

Somehow I multiply cofactors by the entries of b, anytime I'm multiplying cofactors by numbers, I think, I'm getting the determinant of something. Let's call the matrix B , whose determinant comes out of Cáµ€b .

So xâ‚ = detBâ‚/detA  , xâ‚‚ = detBâ‚‚/detA , ... 

**4C:** Cramer's rule: The jth component of x = Aâ»Â¹b is the ratio

![](../imgs/LA_det_cramer_rule.png)

Actually, Cramer's Rule is a disastrous way to go, because compute these determinants takes like approximately forever.  

But having a formula allows you to do algebra instead of algorithms. They're nice formulas, but I just don't want you to use them. 


<h2 id="b99ff4a9543c3ccbfac05190cc07dd68"></h2>


## | detA | = volume of box


```
    å·²çŸ¥ä¸‰è§’å½¢çš„ä¸‰ä¸ªé¡¶ç‚¹ï¼š
    (x1,y1),(x2,y2),(x3,y3),æ±‚é¢ç§¯:
    è§£ï¼š
                |x1 y1 1|
    S= 1/2 det  |x2 y2 1|
                |x3 y3 1|
    
    
    å¦‚æœæœ‰ä¸ªé¡¶ç‚¹æ˜¯åŸç‚¹,æ¯”å¦‚(x1,y1)=(0,0)
    
    S =  1/2 det|x2,y2|
                |x3,y3|
```

---

<h2 id="2a051e916e56ff2ad1a6b47745f85a6b"></h2>


# 21. EigenValues - EigenVectors 

There are certain vectors where Ax comes out parallel to x. And those are the eigenvectors.  **Ax=Î»x**.

If A is singular, Î»=0 is an eigenvalue.

Now the question is how do we find these x-s and Î».

<details>
<summary>
Projection Matrix
</summary>

Any x in projection plane  Ax=x , Î»=1

Any x âŸ‚ plane,  Ax=0 , Î»=0.

</details>

<details>
<summary>
Permutation Matrix
</summary>

```
A =

   0   1
   1   0
```

x = [1;1] , Î»=1

x = [-1;1] , Î»=-1. In fact, the trace tells you right away what the other eigenvalue is.

</details>

How to solve Ax=Î»x ?

(A-IÎ»)x = 0.    A-IÎ» has to be singular.


<h2 id="9e08a229f7feafd17be0f57fed7d544d"></h2>


## det[A-Î»I] = 0 

The idea will be to find Î» first. I'll find n Î»'s.  A Î» could be repeated, A repeated Î» is source of all trouble in 18.06.

<h2 id="95946abeb88b298e53a6c970166fb7aa"></h2>


## Trace = Î»â‚+Î»â‚‚+ ... + Î»<sub>n</sub>

Fact: sum of Î»'s = aâ‚â‚+aâ‚‚â‚‚+ ... + a<sub>nn</sub>.


<details>
<summary>
Symmetric Matrix Example 
</summary>

```
A =
   3   1
   1   3


det(A-Î»I) = |3-Î» 1| = (3-Î»)Â²-1
            |1 3-Î»|

          = Î»Â²-6Î»+8 
```

This example is great. 6 is the trace ,and 8 is the determinant. 

Î»â‚=4, Î»â‚‚=2 . 

```
A-4I = 
  -1   1
   1  -1
```

xâ‚= [1;1]

```
A-2I = 
   1   1
   1   1
```

xâ‚‚= [-1;1]

</details>

What's the relateion between the Permutation example and Symmetric example ?

Simple but useful observation: if I add *n*Â·I to a matrix, its eigenvectors don't change, and its eigen values are *n* bigger.


<details>
<summary>
Why does it happen ? 
</summary>

Suppose I have a matrix A, and Ax=Î»x. I add 3I to that matrix. 

(A+3I)x = Î»x + 3x = (Î»+3)x

</details>


<details>
<summary>
Rotation matrix Q
</summary>

```
Q = 
   0  -1
   1   0
```

90Â° rotation.

trace = 0 = Î»â‚+Î»â‚‚ , det = 1 = Î»â‚Î»â‚‚.  Î»Â²=-1.

What I'm leading up to with this example is that something is gonna go wrong, because what vector can come out parallel to itself after a 90Â° rotation? 

Î»â‚=i, Î»â‚‚=-i. 

</details>

If a matrix was symmetric, it wouldn't happen ( complex number Î» ). So if we stick to matrices that are symmetric , or close to symmetri, then the eigenvalues will stay real. But if we move far away from symmetric , for this example it's anti-symmetric , the bad thing will happen. 


<details>
<summary>
Here's one more bad thing that could happen.
</summary>

```
A =
   3   1
   0   3
```

This is a triangular matrix.  It's really useful to know you can read the eigenvalues off, they're  right on the diagonal. 

Î»â‚ = Î»â‚‚ = 3. 

```
(A-Î»I)x = |0 1|x = 0
          |0 0|
```

xâ‚=[1;0]  , xâ‚‚= NO 2nd independent x 

This is a degenerate matrix. It's only got one line of eigenvectors instead of two.  It's this possibility of a repeated eigenvalue opens this further possiblity of a shortage of eigenvectors. 

</details>


<h2 id="7082803699d165541950b7de2ae708e3"></h2>


# 22. Diagonalizing 

<h2 id="7b86df847bba7602c3c3d17d03b2841f"></h2>


## Diagonalizing a matrix 


<details>
<summary>
Sâ»Â¹AS = Î›
</summary>

Suppose we have n independent eigenvectors of A , put them in columns of S.  
S is the eigenvector matrix. 

```
     â¡ |  |      |  â¤   â¡ |    |        |   â¤
AS = â¢ xâ‚ xâ‚‚ ... xn â¥ = â¢Î»â‚xâ‚ Î»â‚‚xâ‚‚ ... Î»nxn â¥.
     â¢ |  |      |  â¥   â¢ |    |        |   â¥
     â£ |  |      |  â¦   â£ |    |        |   â¦
```

Each time A multiply eigenvectors , the eigen value comes out.

```
     â¡  |   |        |  â¤   â¡  | |      |  â¤ â¡Î»â‚       â¤
AS = â¢Î»â‚xâ‚ Î»â‚‚xâ‚‚ ... Î»nxnâ¥ = â¢ xâ‚ xâ‚‚ ... xn â¥ â¢  Î»â‚‚     â¥.
     â¢  |   |        |  â¥   â¢  | |      |  â¥ â¢    ...  â¥
     â£  |   |        |  â¦   â£  | |      |  â¦ â£       Î»nâ¦
```

If I want a number to multiply x,  then I can do it by puting xáµ¢ in column i (I get S back again), and multiply by a diagonal matrix. 

**AS = SÎ›**. 

Provided my assumption of n independent eigenvectors,  I multiply Sâ»Â¹ on the left , I got 

**Sâ»Â¹AS=Î›**.

if I multiply  Sâ»Â¹ on the right , I got 

**A=SÎ›Sâ»Â¹**.

</details>


<h2 id="d975eb14bcadcace9392fa08591c6162"></h2>


## Powers of A / equation u<sub>k+1</sub> = Au<sub>k</sub>

<details>
<summary>
Can I just begin to use that ? For example how about AÂ² ? What's eigenvectors and eigenvalues of AÂ² ? 
</summary>

If Ax = Î»x 

AÂ²x = Î»Ax = Î»Â²x. 

So the eigenvalues of AÂ² are Î»Â², and the eigenvector is the same x as A.

This is one way to do it . 

Now let me see that also from formula A=SÎ›Sâ»Â¹.

AÂ²x = SÎ›Sâ»Â¹Â·SÎ›Sâ»Â¹x = SÎ›Â²Sâ»Â¹. It's telling me the same thing that I just learned here, but in a matrix form. It's telling me that the S is the same, the eigenvectors are the same; but the eigenvalues are squared.

</details>

Aáµ = SÎ›áµSâ»Â¹ . 

Eigenvalues and eigenvectors give a great way to understand the powers of a matrix. 

Theorem: Aáµâ†’0 as kâ†’âˆ  if all |Î»áµ¢|<1 .  

So far I'm operationg on one assumption: I had a full set of n independent eigenvectors. If we don't have n independent eigenvectors can not diagonalize the matrix,  Sâ»Â¹ can not make sense.

**A is sure to have n independent eigenvectors (and be diagonalizable) if all the Î»'s are different (no repeated Î»)**. 

If I have repeated Î»'s , I may or may not have n independent eigenvectors (think about the Identity matrix). 


<details>
<summary>
Start with given vector uâ‚€ , u<sub>k+1</sub> = Au<sub>k</sub>, calculate uâ‚â‚€â‚€
</summary>

uâ‚=Auâ‚€, u<sub>k</sub>=Aáµuâ‚€

The next section is gonna to solve systems of differential equations. I'm going to have derivatives. 

write uâ‚€ as the combination of eigenvectors: 

uâ‚€ = câ‚xâ‚ +câ‚‚xâ‚‚ + ... +c<sub>n</sub>x<sub>n</sub>

Now multiply by A :

Auâ‚€ = câ‚Î»â‚xâ‚ +câ‚‚Î»â‚‚xâ‚‚ + ... +c<sub>n</sub>Î»<sub>n</sub>x<sub>n</sub>

uâ‚â‚€â‚€ = AÂ¹â°â°uâ‚€ = câ‚Î»â‚Â¹â°â°xâ‚ +câ‚‚Î»â‚‚Â¹â°â°xâ‚‚ + ... +c<sub>n</sub>Î»<sub>n</sub>Â¹â°â°x<sub>n</sub> = Î›Â¹â°â°Sc


</details>


<details>
<summary>
Fibonacci Example:  F<sub>k+2</sub> = F<sub>k+1</sub> + F<sub>k</sub>
</summary>

0,1,1,2,3,5,8,13, ... 

F<sub>k+2</sub> = F<sub>k+1</sub> + F<sub>k</sub>

Right now what I've got is a single equation, not a system, and it's second-order, with second derivatives. I want to get first derivatives. The way to do it is to introduce u<sub>k</sub> will be a vector:

![](../imgs/la_1806_uk.png)

This is my unknown. 

So I'm going to get a 2x2 system, first order, instead of a scalar system, second order:

![](../imgs/la_1806_fib_system.png)

This is my system. 

And what's my one step equation? 

![](../imgs/la_1806_fib_equation.png)

What's the eigenvalues and eigenvectors of this matrix ?  

Î»â‚ = -0.61803, Î»â‚‚ = 1.61803 . 

How fast are those Fibonacci numbers increasing ? What's controlling the growth of these Fibonacci numbers? It's the eigenvalues. 

And which eigenvalue is controlling that growth? The big one. 

Since uâ‚€ = [1;0] , câ‚xâ‚+câ‚‚xâ‚‚ = [1;0].

Fâ‚â‚€â‚€ â‰ˆ câ‚Â·(1.61803)Â¹â°â°.   ( the other terms involves câ‚‚ is ignored since it is very slow. )



</details>

<h2 id="8912c5512db9003e5c8ce07b7ff36a88"></h2>


## Recap 

 - eigenvalue Î»
    1. nxn matrix has n eigenvalues
    2. eigenvalue is allowed to be 0 ( i.e. projection matrix )
    3. sum of Î»'s  equals the trace of matrix,  
    4. product of Î»'s equals determinant of matrix. 
    5. singular matrix must have 0 Î».  
        - number of 0 Î»'s = dimension of null space.
    6. repeated Î» **MAY or may NOT** result in missing eigenvectors
    7. eigenvalue could be complex number.
        - Symmetric matrix's eigenvalues are all real. 
 - eigenvector 
    1. Eigenvectors of sysmmetric matrix are orthogonal.  (doesn't all eigenvectors orthogonal )
    2. A matrix may have no eigenvectors.  ( ? any example ? )


<h2 id="d4fbca7834560377a100c91364ec53ec"></h2>


# 23. 

This section is about how to solve a system of first order, first derivative, constant coefficient linear equations. And if we do it right, it turns directly into linear algebra. The key idea is the sulutions to contant coefficient linear equations are **exponentials**.  

And the result, one thing we will find it's completely parallel to powers of a matrix. 


<h2 id="9c0f59a904be7c01984a683e511ca015"></h2>


## Differential Equations  du/dt=Au

<details>
<summary>
Example 
</summary>

u(0) = [1;0]

duâ‚/dt = -uâ‚+2uâ‚‚

duâ‚‚/dt =  uâ‚-2uâ‚‚

-> A = â¡ -1  2 â¤
       â£  1 -2 â¦

So it starts u at time 0, everything starts at uâ‚. And as time goes on, duâ‚‚/dt weill be positive because of that uâ‚ term, so flow will move into the uâ‚‚ component and it will go out of the uâ‚ component. 

So we'll just follow that movement as time goes forward by looking at the eigenvalues and eigenvectors of that matrix.

Î»â‚ = 0, Î»â‚‚ = -3.

The eigen values are telling me something. Î»â‚‚ = -3 , this eigen value is going to disappear, eâ»Â³áµ—.  Î»â‚ = 0, eâ°áµ—=1. 

So I'm expecting that this solution'll have 2 parts, eâ°áµ— and eâ»Â³áµ— parts. and as time goes on, the second part'll disappear and the first part will be a steady status. ( å› ä¸ºè¿™æ˜¯å¾®åˆ†æ–¹ç¨‹ï¼Œæ‰€ä»¥0æ˜¯steady status??? )

xâ‚=[2;1] , xâ‚‚=[1;-1]

Solution: u(t) = câ‚e<sup>Î»â‚</sup>xâ‚ + câ‚‚e<sup>Î»â‚‚</sup>xâ‚‚

TODO çœ‹ä¹¦ã€‚




</details>


<h2 id="627bb884755ff95a306e6587c9bed913"></h2>


## Exponential e<sup>At</sup> of a matrix

TODO

---

<h2 id="21250cdbabe0f7964564376f3d523f38"></h2>


# 24. 

<h2 id="0134f701c2230c4f2fc4488d2214160a"></h2>


## Markov matrices


```bash
A =

   0.10000   0.01000   0.30000
   0.20000   0.99000   0.30000
   0.70000   0.00000   0.40000
```

<details>
<summary>
property: 
</summary>

1. all entreis >= 0
2. all columns add to 1.

The powers of Markov matrix are all Markov matrices.

**Steady State**:  Î» = 1. 

Key points:

1.  Î» = 1 is an eigenvalue
2. all other |Î»áµ¢| < 1

Remember: uk = Aáµuâ‚€ = câ‚Î»â‚áµxâ‚ +câ‚‚Î»â‚‚áµxâ‚‚ + ... +cnÎ»náµxn 

Those terms which has |Î»áµ¢| < 1 goes to 0.

since all columns of A markov matrix add to 1, then A-I must be singular ( all columns of A-I add to 0 ) . That is , one of the eigenvalues must be 1. 

**Eigenvalus of Aáµ€ are the same as eigenvalues of A.**  because det(A-Î»I) = det(Aáµ€-Î»I).  (Determinant's property 10.)

</details>

<details>
<summary>
application :
</summary>

u<sub>k+1</sub> = Au<sub>k</sub> , A is Markov Matrix

The populations in California and Massachusetts.

Every year, 90% people of California stays, and 10% moves to Massachusetts, while 80% people of Massachusetts stays , and 20% moves to California.

```
u = |u_cal |
    |u_mass|

A =

   0.90000   0.20000
   0.10000   0.80000
```

after many many  yeas later, where are those peoples ? 

We can easily get the 2 eigenvalues : Î»â‚=1, Î»â‚‚=0.7.

and the eigenvector for the steady status is xâ‚=[2;1].  Now we can jump to infinity right now, finally , the California will have 2/3 total peoples, while Mass has 1/3 of all. 

To verify: 

let uâ‚€ = [0;1000],

since uâ‚€ = câ‚xâ‚ +câ‚‚xâ‚‚  =  câ‚[2;1] +câ‚‚[-1;1] , we get câ‚ = 1000/3.

A<sup>k+1</sup>uâ‚€ = câ‚Â·1<sup>k+1</sup>[2;1] = [666.67 ; 333.33] 



</details>



<h2 id="1c401566682351198dc0b2cf460094c1"></h2>


## Fourier Series and projections 


<details>
<summary>
Projections with orthonormal basis  
</summary>

for any v ,  

v = xâ‚qâ‚+ xâ‚‚qâ‚‚+ ... + x<sub>n</sub>q<sub>n</sub>

how to calculate xâ‚ quickly ?  multiplied by qâ‚áµ€ on both side 

qâ‚áµ€v = xâ‚.

This result can be achieved from the maxtix view .

Qx = v.  x=Qâ»Â¹v = Qáµ€v  => xâ‚=qâ‚áµ€v


</details>

<details>
<summary>
Fourier Series 
</summary>

f(x) =aâ‚€ + aâ‚cosx + bâ‚sinx + a2cos2x + bâ‚‚cos2x + ... 

This one's infinite, but the key property of things being orthogonal is still true for sin and cos. So it is the propery that makes Fourier series work. 

Joseph Fourier realized that, hey, I could could work in function space.

Instead of a vector v, I could have a function f(x) . Instead of orthornal vectors qâ‚,qâ‚‚,..., I could have orthogonal functions, but infinitely many of them. 

So the basis are functions , 1, cosx, sinx, cos2x, sin2x, ... , and they are orthogonal. 

What does it means "funtions are orthogonal" ?

fáµ€g = âˆ«â‚€<sup>2Ï€</sup> f(x)g(x)dx = 0.

How do I get aâ‚?  I take the inner product of everything with cos(x).


</details>


---

<h2 id="13f3022d707d2db79b2bbc152c21c483"></h2>


# 25.

<h2 id="b899f85c23e42aea33f7684a076389ca"></h2>


## Symmetric Matrices

A = Aáµ€
 
<details>
<summary>
Eignvalues / Eigenvectors 
</summary>

What's special about the eigenvectors ?

1. The eigenvalues are REAL.
2. The eigenvectors are (can be chosen) PERPENDICULAR

Usual case :  A = SÎ›Sâ»Â¹
Symmetric case: A = QÎ›Qâ»Â¹ = QÎ›Qáµ€ , one of the most famous theorem.

Why real eigenvalues ?

TODO

If A is a real matrix , A has real eigenvalues and perpendicular eigenvectors means  A = Aáµ€.

If A is a complex matrix ,  it meas A = AÌ…áµ€.

A = QÎ›Qáµ€  = Î»â‚qâ‚qâ‚áµ€ + Î»â‚‚qâ‚‚qâ‚‚áµ€ + ...  

here all q are orthonoraml , and qqáµ€ is a projection matrix.

Every symmetric matrix is a combination of perpendicular projection matrices. 

Symmetric  matrix has another property:  #positive pivots == #positive eigenvalues.

</details>


<details>
<summary>
Start: Positive Definite Matrices
</summary>

It's symmtrix.   

1. All eigenvalues are positive.  
2. All the pivots are positive.  
3. **all** the **sub**determinant is positive. (including the determinant) 

</details>

<h2 id="dd3391b01e6d23199e9d35f0a4f5d427"></h2>


## 26.

<h2 id="023666df9ae9590deecc67a35fb5839e"></h2>


### Complexo

The Fourier Matrix is the most important complex matrix.  It's the matrix that we need in Fourier transform.  

And the really special thing is the fast Fourier transform (FFT). It's being used in a thousand places. How do I mulitply fast by that matrix? 

Normally, multiplications by nxn matrix would be O(nÂ²).  The FFT's idea reduces this nÂ² down to nlogâ‚‚n. 

<h2 id="e5c0693de0b5a3e017ca1bcabf2a8444"></h2>


### Complex Vectors , length

záµ€z is no good.  Doing that inner productio won't give me the right thing.

Because the length square should be positive, but complex number may make it 0 or negative.

What I really want is zÌ…áµ€z = |z|Â². 

Here is a symbol to do both: zá´´z.   H stands for Hermite. 

**Inner product** of complex vector:  yÌ…áµ€x.


<h2 id="9ff43aa3e54f31207991c2e82ccf292d"></h2>


### Complex Matrices

Symmetric Aáµ€=A  is no good if A is complex. 

right version:  AÌ…áµ€=A.   The diagonal should be real, and the other entries should be conjugate.  Hermitian matrix :  Aá´´=A

```octave
A =
   2        3 + 1i
   3 - 1i   5 
```

Unitary matrix: like an orthogonal matrix, nxn matrix with orthonormal columns, perpendicular unit columns, perpendicularity is computed by **conjugate**. Fourier matrix happens to be one of these guys. 

1. the 1st column fills with 1
2. the 2nd column is 1, w, wÂ², ... wâ¿â»Â¹
3. the 3rd column is the power of 2nd column: 1,wÂ²,wâ´,...,wÂ²â½â¿â»Â¹â¾
4. more columns ...
5. the last colun is 1,wâ¿â»Â¹, wÂ²â½â¿â»Â¹â¾, ..., wâ½â¿â»Â¹â¾â½â¿â»Â¹â¾


This is a symmetric matrix , not in perfect way, and 

(F<sub>n</sub>)<sub>ij</sub> = wâ±Ê² ,  i,j=0,...,n-1

Here, w is a special number.

wâ¿=1.

w = e<sup>i2Ï€/n</sup> = ~~cos 2Ï€/n + iÂ·sin 2Ï€/n~~

w is no the unit circle of complex plane. for example ,if n = 6, then w lands on unit circle ( 1/6 circle, 60 degree ). 

Where is wÂ² ? the angle is doubled ( 2/6 circle, 120 degree )

if n =4 , w<sub>n</sub> = e<sup>iÏ€/2</sup> = i .

```octave
Fâ‚„ = 

  1  1  1  1
  1  i  iÂ² iÂ³
  1  iÂ² iâ´ iâ¶
  1  1Â³ iâ¶ iâ¹

   = 

  1  1  1  1
  1  i -1 -i
  1 -1  1 -1
  1 -i -1  i
```

This matrix is so remarkable.   It's the 4x4 matrix that comes into the 4 point Fourier transform. The inverse matrix will be a nice matrix also. 

Those columns are orthogonal ( PS. you should take 1 column's conjugate ). They're not quite orthonormal.  But I can fix it easily, they all have length 2. 

```octave
Fâ‚„ = 1/2 *

  1  1  1  1
  1  i -1 -i
  1 -1  1 -1
  1 -i -1  i
```

Now I can inverse it right away:  Fâ‚„á´´.

So what's good? What property is it that leads to the FFT ? 

Here is the idea. Fâ‚† has a neat connection to Fâ‚ƒ, half as big. There's a connection of Fâ‚ˆ to Fâ‚„. There's a connection of Fâ‚†â‚„ to Fâ‚ƒâ‚‚. 

(wâ‚†â‚„)Â² = wâ‚ƒâ‚‚.

![](../imgs/la_fourier_6to3.png)

Fâ‚†â‚„ ä¹Ÿæœ‰ç±»ä¼¼ä¸Šé¢çš„ Fâ‚†=DFâ‚ƒP çš„åˆ†è§£ï¼Œè¿™æ ·å°±å¯ä»¥ 64Â² é™ä½ä¸º 2(32)Â²+32.

D is powers of w 

```octave
D = 

  1  
     w
       wÂ²
         ...
            wâ¿â»Â¹
```

Q: what is the P ?  How does recursive idea use here ? 

64Â² -> 2(32)Â²+32 -> 2( 2(16)Â²+16 ) + 32

When I do the recursion more and more times, I get simpler and simpler factors in the middle, eventually I'll be down to 2 point or 1 point Fourier transforms.  Finally I get to 6x32, that's my final count. 

Conclusion: the FFT multiplies by nxn matrix, but it does it not in nÂ² steps, but in 1/2Â·nlogn steps. 


<h2 id="02e74f10e0327ad868d138f2b4fdd6f0"></h2>


## 27 

<h2 id="f374bd8e0ba50a2cca911b7d328d59fb"></h2>


### Positive Definite Matrix (Tests)Â²

Now, all matrices are symmetric. my question is , is it postive definite.

1. all eigen values are positive
2. all determinate of submatrices are postive.
3. all pivots are positive. 
4. `*`  xáµ€Ax > 0

Graphics of f(x,y) = xâƒ—áµ€Axâƒ— = axÂ²+2bxy+cyÂ² 

saddle point, imagine the graph something like human legs.


<h2 id="e77fe7259561aabc1f580cd55ad18daa"></h2>


### Ellipsoids in Râ¿

A = QÎ›Qáµ€  (how to connect them ? )




<h2 id="33e75ff09dd601bbe69f351039152189"></h2>


## 28 


<h2 id="714c646ab1eb70195e6ae70fbf2681b6"></h2>


### Aáµ€A is positive definite

If A,B are pos def , so is A+B.  xáµ€(A+B)x > 0.

xáµ€Aáµ€Ax = (Ax)áµ€Ax = |Ax|Â² >= 0.

One comment about numerial things: with a positive definte matrix, you never have to do row exchanges, you never run into unsuitably small numbers or zeroes in the pivot postion. 


<h2 id="ae75137209c482f86ac4dd1100e7e6af"></h2>


### Similar Matrices A,B

nxn matrices A and B are simular means : for some invertible M, B = Mâ»Â¹AM.

Example:

Sâ»Â¹AS = Î›  , A is similar to Î›.

like, I'm putting these matrices into families, all the matrices in the family are similar to each other. The outstanding member of the family is the diagonal guy (i.e. Î›) . 

suppose 

```octave
A =

   2   1
   1   2
```

```octave
Î› =

   3   0 
   0   1 
```

```octave
M =

   1   4
   0   1


B = Mâ»Â¹AM = 

   -2  -15
    1    6
```

A,Î›,B are all similar matrices. They all have something in common. And that is they have the same eigenvalues.  ( and same number of eigenvectors. )  

proof:

```octave
Ax=Î»x  (B=Mâ»Â¹AM)
AMMâ»Â¹x = Î»x 
Mâ»Â¹AMMâ»Â¹x = Î»Mâ»Â¹x 
BMâ»Â¹x = Î»Mâ»Â¹x  , let b=Mâ»Â¹x
Bb = Î»b
```

Eigenvector of B is Mâ»Â¹(eigenvector of A).

---- 

I now have to get into the less happy possibility that the 2 eigenvalues could be the same. 

BAD CASE 

for example, Î»â‚=Î»â‚‚=4.

There are 2 families :

1. the small one family includes only 1 matrix : 4I 
    - 
    ```octave
    A =

       4   0
       0   4
    ```
    - since Mâ»Â¹AM = A 
2. the big one family includes 
    - 
    ```octave
    A =

       4   1
       0   4
    ```
    - those matrices has only 1 eigenvector.
    - more members of family
    ```octave
    B =

       5   1
      -1   3
    ```


<h2 id="6ea9ab1baa0efb9e19094440c317e21b"></h2>


## 29

<h2 id="cdbd45bcf4e7ece0654c0d7b925abb97"></h2>


### Singular Value Decomposition = SVD

A = UÎ£Váµ€

- It is the final and best factorization of a matrix.
    - âˆ‘ diagonal matrix
    - U,V  orthogonal matrix

A can be any matrix.  Any matrix has this SVD. 

One thing we'll bring together is the very good family: 


- symmtric , postive , definte matrix.
    - A = QÎ›Qáµ€.  Because they were symmetic , their eigenvectors were orthogonal.
    - This is the SVD in case our matrix is symmetric postive definte.

- A = SÎ›Sáµ€. 
    - This is my usual one. My usual one is the eigenvectors and eigenvalues. This is no good in general because usually S isn't orthogonal.
    - In the symmetric case , the eigenvectors are orthogonal, so my ordinary S has become Q. And 
    - In the positive definite case , the ordinary Î› become positive Î›.

---

What am I looking for now? 

Avâ‚=uâ‚.  vâ‚ in row space gets taken over to uâ‚ in column space.

In SVD, what I'm looking for is an orthogonal basis in row space that gets knocked over into an orthogonal space in columm space.

I'm not only going to make these orthogonal, but also make them orthonormal. 

Ïƒâ‚uâ‚=Avâ‚

Ïƒâ‚‚uâ‚‚=Avâ‚‚

AV = UÎ£

So, this is my goal, to find an orthonormal basis in the row space(V), and an orthonormal basis in the column space(U).

A = Uâˆ‘Vâ»Â¹ = Uâˆ‘Váµ€

Now I have 2 orthogonal matrices here, And I don't want to find them both at once. So I want to cook up some expression that will make the Us disappear and leave me only the V. 

It's the same combination that keeps showing up whenever we have a general rectangular matrix:

Aáµ€A = Vâˆ‘áµ€Uáµ€Uâˆ‘Váµ€ = Vâˆ‘Â²Váµ€

Now Us are out of the picture now. I'm only having to choose the Vs, and what are these Vs?  And what are these âˆ‘s ?

They're the eigenvectors and eigenvalues for the matrix Aáµ€A.

How I going to find the Us ? One way would be to look at AAáµ€. 

So the overall picture is , the Vs are the eigenvectors of Aáµ€A , and Us are the eigenvectors of AAáµ€. The Ïƒs are the positive  square roots of eigenvalue of Aáµ€A ?

Example

```octave
A =

   4   4
  -3   3

Aáµ€A =

   25    7
    7   25
```

vâ‚ = [1/âˆš2;1/âˆš2] ,  vâ‚‚ = [1/âˆš2, -1/âˆš2]

Ïƒâ‚ = 32 , Ïƒâ‚‚ = 18

So 

```octave
V = 

    1/âˆš2    1/âˆš2 
    1/âˆš2   -1/âˆš2

âˆ‘ =

    âˆš32     0
     0     âˆš18
```

```octave
AAáµ€ = 
   
   32    0
    0   18
```

uâ‚ = [1;0] , uâ‚‚ = [0;-1]

The eigenvalues are the same as Aáµ€A.


---

<h2 id="34173cb38f07f89ddbebc2ac9128303f"></h2>


## 30

<h2 id="1b08f35b032e4fa969a0b2e5ebcb2c03"></h2>


### Linear Transformation 

T(v+w) = T(v)+T(w)

T(cv) = cT(v)

In linear transformation, zero vector can not move.  T(0) = 0 .

10.05



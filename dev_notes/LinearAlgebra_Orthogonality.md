...menustart

 - [Orthogonality](#878bcbccb5d63db01d2193f2e15cae28)
	 - [3.1 ORTHOGONAL VECTORS AND SUBSPACES](#70835fd4404a5ddec835801ba42ecfb0)
	 - [3.2 COSINES PROJECTIONS ONTO LINES](#870df6062f5bb62d6f82187a8efebbe1)
	 - [3.3 PROJECTIONS LEAST SQUARES](#4a2df06c2d276ad56402eecaa894e5d5)
	 - [3.4 ORTHOGONAL BASES AND GRAM-SCHMIDT](#09bbcb83a58fddde91f52c847cd92d46)

...menuend



<h2 id="878bcbccb5d63db01d2193f2e15cae28"></h2>
# Orthogonality

<h2 id="70835fd4404a5ddec835801ba42ecfb0"></h2>
## 3.1 ORTHOGONAL VECTORS AND SUBSPACES

 - In choosing a basis, we tend to choose an orthogonal basis , to make those calculations simple. 
 - A further specialization makes the basis just about optimal: The vectors should have length 1. 
 - For an orthonormal basis (orthogonal unit vectors), we will find:
 	- 1. the length ‚Äñx‚Äñ of a vector
 	- 2. the test x·µÄy = 0 for perpendicular vectors;
 	- 3. how to create perpendicular vectors from linearly independent vectors.

More than just vectors, subspaces can also be perpendicular. The 4 fundamental subspaces are perpendicular in pairs, 2 in R·µê and 2 in R‚Åø. That will complete the fundamental theorem of linear algebra.


The first step is to find the ***length of a vector***. 

 - ***The length ‚Äñx‚Äñ in R‚Åø is the positive square root of x·µÄx***


**Orthogonal Vectors**

```bash
Orthogonal vectors    x·µÄy = 0 

proof:

	‚Äñx‚Äñ¬≤ + ‚Äñy‚Äñ¬≤ = ‚Äñx+y‚Äñ¬≤   # ÂãæËÇ°ÂÆöÁêÜ
=>  x·µÄx + y·µÄy = (x+y)·µÄ(x+y)
=>  x·µÄx + y·µÄy = x·µÄx + x·µÄy + y·µÄx + y·µÄy
# vector inner product leads to scalar , so x·µÄy == y·µÄx
=>  x·µÄx + y·µÄy = x·µÄx + 2x·µÄy + y·µÄy 
=>  x·µÄy = 0 
```

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_inner_product_xTy.png)

> ***zero vector is orthogonal to any vector***.

Useful fact: **If nonzero vectors v‚ÇÅ, ... , vk are mutually orthogonal** (every vector is perpendicular to every other), **then those vectors are linearly independent**.

 - The coordinate vectors e‚ÇÅ, ... , en in R‚Åø are the most important orthogonal vectors. 
 - Those are the columns of the identity matrix. 
 - They form the simplest basis for R‚Åø, and
 - they are *unit vectors* - each has length ‚Äñe·µ¢‚Äñ = 1. 
 - They point along the coordinate axes. 
 	- If these axes are rotated, the result is a new **orthonormal basis**: 
 	- a new system of *mutually orthogonal unit vectors*. 
 - In R¬≤ we have cos¬≤Œ∏ + sin¬≤Œ∏ = 1:
 	- **Orthonormal vectors in R2** v‚ÇÅ = ( cosŒ∏, sinŒ∏ ) and v‚ÇÇ = ( -sinŒ∏, cosŒ∏ ).



**Orthogonal Subspaces**

**Every vector** *in one subspace must be orthogonal to* **every vector** *in the other subspace*.

 - Subspaces of ***R¬≥*** can have dimension 0, 1, 2, or 3. 
 - The subspaces are represented by lines or planes through the origin
 	- and in the extreme cases, by the origin alone or the whole space. 
 - The subspace {0} is orthogonal to all subspaces. 
 - A line can be orthogonal to another line, or it can be orthogonal to a plane
 - **but a plane cannot be orthogonal to a plane in R¬≥**.
 	- because 2 orthogonal plane has dimension 4 in total , they must intersect in a line in R¬≥ 
 	- 2 vectors in a same line are not orthogonal
 	- **2 orthogonal subspace never intersect in any none zero vector**
 	- I have to admit that the front wall and side wall of a room look like perpendicular planes in R¬≥. But by our definition, that is not so!
 	- 2 planes can be orthogonal in R‚Å¥

**3B**: 

 - Two subspaces V and W of the same space R‚Åø are *orthogonal* if every vector v in V is orthogonal to every vector w in W: 
 	- v·µÄw = 0 for all v and w.

**Example 2**: Suppose V is the plane spanned by v‚ÇÅ = (1, 0, 0, 0) and v‚ÇÇ = (1, 1, 0, 0). If W is the line spanned by w = (0, 0, 4, 5), then w is orthogonal to both v's. The line W will be orthogonal to the whole plane V.

In this case, with subspaces of dimension 2 and *l* in R4, there is room for a third subspace. The line L through z = (0, 0, 5, -4) is perpendicular to V and W. Then the dimensions add to 2 + 1 + 1 = 4. What space is perpendicular to all of V, W, and L ?


The important orthogonal subspaces don't come by accident, and they come two at a time. In fact orthogonal subspaces are unavoidable: ***They are the fundamental subspaces***! The first pair is the *nullspace and row space*. Those are subspaces of **R‚Åø** - the rows have n components and so does the vector x in Ax = 0. We have to show, using Ax = 0, that ***the rows of A are orthogonal to the nullspace vector x***.


**3C Fundamental theorem of orthogonality**:  

 - The row space is orthogonal to the nullspace (in R‚Åø) , Ax = 0 
 - The column space is orthogonal to the left nullspace (in R·µê)

***First proof***  Suppose x is a vector in the nullspace. Then Ax = 0 , and this system of *m* equations can be written out as rows of A multiplying x:

```
	 | ... row 1 ... | |x‚ÇÅ|   |0|
	 | ... row 2 ... |¬∑|x‚ÇÇ| = |0|
Ax = | 				 | |  |	  | |
	 | 				 | |  |	  | |
	 | ... row m ... | |  |	  |0|
	 				   |xn|
```

 - The main point is already in the first equation: row 1 is orthogonal to x. 
 	- Their inner product is zero; 
 - Every right-hand side is zero, so x is orthogonal to every row. 
 	- Therefore x is orthogonal to every combination of the rows. 
 - Each x in the nullspace is orthogonal to each vector in the row space, so ***N(A) ‚ä• C(A·µÄ)*** .


The other pair of orthogonal subspaces comes from A·µÄy = 0, or y·µÄA = 0:

```
	  |c 	   c|
	  |o 	   o|
	  |l 	   l|
y·µÄA = |u  ...  u| = [0 ... 0]
	  |m 	   m|
	  |n 	   n|
	  |		    |
	  |1 	   n|
```

 - The equation says , from the zeros on the right-hand side , the vector y is orthogonal to every column.
 	- Therefore y is orthogonal to every combination of the columns.
 - y is orthogonal to the column space, and it is a typical vector in the left nullspace:
 	- ***N(A·µÄ) ‚ä• C(A)***



***Second proof***  If x is in the nullspace then Ax = 0. If v is in the row space, it is a combination of the rows: v = A·µÄz for some vector z. Now, in one line:

```
Nullspace ‚ä• Row space :   
	v·µÄx = (A·µÄz)·µÄx = z·µÄAx = z·µÄ0 = 0.    (8)
```


**DEFINITION** Given a subspace V of R‚Åø, the space of all vectors orthogonal to V is called the **orthogonal complement** of V. It is denoted by **V‚ä• = "V perp."**  (‚ä• Âú®Âè≥‰∏äËßí)

 - the nullspace is the orthogonal complement of the row space
 	- A vector z can't be orthogonal to the nullspace but outside the row space
 - **Dimension formula**:  dim(row space) + dim(nullspace) = number of columns.
 	- r + ( n -r ) = n 
 
**3D Fundamental Theorem of Linear Algebra, Part II**

 - The nullspace is the *orthogonal complement* of the row space in R‚Åø.
 - The left nullspace is the *orthogonal complement* of the column space in R·µê.

**3E** Ax = b is solvable if and only if  y·µÄb = 0  whenever y·µÄA  = 0.

 - Ax = b requires b to be in the column space. 
 - Ax = b **requires b to be perpendicular to the left nullspace**.


---

**The Matrix and the Subspaces**

 - Splitting R‚Åø into orthogonal parts V and W , will split every vector into x = v + w. 
 - The vector v is the projection onto the subspace V. 
 - The orthogonal component w is the projection of x onto W. 

Figure 3.4 summarizes the fundamental theorem of linear algebra.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_Figure_3.4.png)

 - The nullspace is carried to the zero vector. 
 - Every Ax is in the column space. 
 - Nothing is carried to the left nullspace. 
 - The real action is between the row space and column space, and you see it by looking at a typical vector ***x*** :
 	- It has a "row space component" and a "nullspace component," with `x = x·µ£+ xn`,  When multiplied by A, this is `Ax = Ax·µ£ + Axn`:
 	- The nullspace component goes to zero: Axn = 0.
 	- The row space component goes to the column space: `Ax·µ£ = Ax`.

Of course everything goes to the column space - the matrix cannot do anything else. I tried to make the row and column spaces the same size, with equal dimension r.

**3F** From the row space to the column space. A is actually invertible. Every vector b in the column space comes from exactly one vector x·µ£ , in the row space.

***Proof*** Every b in the column space is a combination Ax of the columns. In fact, b is Ax·µ£ with x·µ£ in the row space, since the nullspace component gives Ax = 0. If another vector x, in the row space gives Ax'·µ£ = b, then A(x·µ£ - x'·µ£) = b - b = 0. This puts x·µ£ - x'·µ£ in the nullspace and the row space, which makes it orthogonal to itself. Therefore it is zero, and x·µ£ = x'·µ£ . Exactly one vector in the row space is carried to b.

> ***Every matrix transforms its row space onto its column space.***

On those r-dimensional spaces A is invertible (if x in row space ). On its nullspace A is zero. When A is diagonal, you see the invertible submatrix holding the r nonzeros.


A·µÄ goes in the opposite direction, from R·µê to R‚Åø and from C(A) back to C(A·µÄ). Of course the transpose is not the inverse! A·µÄ moves the spaces correctly, but not the individual vectors. That honor belongs to A‚Åª¬π if it exists - and it only exists if r = m = n. We cannot ask A‚Åª¬π to bring back a whole nullspace out of the zero vector.

When A‚Åª¬π fails to exist, the best substitute is the ***pseudoinverse*** A‚Å∫. This inverts A where that is possible: A‚Å∫Ax = x for x in the row space. On the left nullspace, nothing can be done: A‚Å∫y = 0. Thus A‚Å∫ inverts A where it is invertible, and has the same rank r. One formula for A‚Å∫ depends on the ***singular value decomposition*** - for which we first need to know about eigenvalues.



<h2 id="870df6062f5bb62d6f82187a8efebbe1"></h2>
## 3.2 COSINES PROJECTIONS ONTO LINES


![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_projection_onto_line.png)

**Figure 3.5** The projection p is the point ( on the line through a ) closest to b.

The situation is the same when we are given a plane (or any subspace S) instead of a line. Again the problem is to find the point p on that subspace that is closest to b. ***This point p is the projection of b onto the subspace***. Geometrically, that gives the distance between points b and subspaces S. But there are two questions that need to be asked:

 1. Does this projection actually arise in practical applications?
 2. If we have a basis for the subspace S, is there a formula for the projection p ?

The answers are certainly yes. This is exactly the problem of the ***least-squares*** solution to an overdetermined system. The vector b represents the data from experiments or questionnaires, and it contains too many errors to be found in the subspace S. The equations are inconsistent, and Ax = b has no solution.

The least-squares method selects p as the best choice to replace b. 

A formula for p is easy when the subspace is a line.   Projection onto a higher dimensional subspace is by far the most important case; it corresponds to a least-squares problem with several parameters. The formulas are even simpler when we produce an orthogonal basis for S.


**Inner Products and Cosines**

Relationship of inner products and angles.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_innerProduct_and_angle.png)

Figure 3.6 The cosine of the angle Œ∏ = Œ≤ - Œ± using inner products.

 - The length ‚Äña‚Äñ is the hypotenuse ÊñúËæπ in the triangle OŒ±Q. 
 	- So the sine and cosine of a are : `sinŒ± = a‚ÇÇ/‚Äña‚Äñ , cosŒ± = a‚ÇÅ/‚Äña‚Äñ` 
 	- For the angle Œ≤ , `sinŒ± = b‚ÇÇ/‚Äñb‚Äñ , cosŒ± = b‚ÇÅ/‚Äñb‚Äñ` 
 	- **cosŒ∏ = cos(Œ≤ - Œ±) = cosŒ≤cosŒ± + sinŒ≤sinŒ± = ( a‚ÇÅb‚ÇÅ + a‚ÇÇb‚ÇÇ ) / ‚Äña‚Äñ¬∑‚Äñb‚Äñ**  ,  (1)

The numerator in formula (1) is exactly the inner product of a and b. It gives the relationship between a·µÄb and cos Œ∏:

**3G** The cosine of the angle between any *nonzero vectors* a and b is :  

```
	cosŒ∏ = a·µÄb / ‚Äña‚Äñ¬∑‚Äñb‚Äñ  (2)
```



**Projection onto a Line**

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_projection_onto_line2.png)

 - Now we want to find the projection point p. 
 - This point must be some multiple p = xÃÇa of the given vector a 
 	- every point on the line is a multiple of a. 
 - The problem is to compute the coefficient `xÃÇ`. 
 - All we need is the geometrical fact that ***the line from b to the closest point p = xÃÇa is perpendicular to the vector a***:

```
	  (b - xÃÇa) ‚ä• a ,  
  or  a·µÄ(b - xÃÇa) = 0 ,  
  or  xÃÇ = (a·µÄb / a·µÄa)   (4)
```

That gives the formula for the number xÃÇ and the projection p:

**3H** The projection of the vector b onto the line in the direction of a is p = xÃÇa:

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_projection_onto_a_line.png)

This leads to the **Schwarz inequality** in equation (6), which is the most important inequality in mathematics.  A special case is the fact that arithmetic means ¬Ω(x + y) ‚â• geometric mean  ‚àöxy . 

```
Schwarz inequality:  |a·µÄb| ‚â§ ‚Äña‚Äñ¬∑‚Äñb‚Äñ    (6)
```

All vectors a and b satisfy the ***Schwarz inequality***, which is |cosŒ∏| ‚â§ 1 in R‚Åø.

One final observation about `|a·µÄb| ‚â§ ‚Äña‚Äñ¬∑‚Äñb‚Äñ` . Equality holds *if and only if* b is a multiple of a. The angle is Œ∏ = 0¬∞ or Œ∏ = 180¬∞ and the cosine is 1 or -1. In this case b is identical with its projection p, and the distance between b and the line is zero.


**Projection Matrix of Rank 1**

The projection of *b* onto the line through *a* lies at p = a(a·µÄb/a·µÄa). That is our formula p = xÃÇa, but the vector *a* is put before the number xÃÇ = a·µÄb/a·µÄa. Projection onto a line is carried out by a ***projection matrix P***, and written in this new order we can see what it is. 

P is the matrix that multiplies b and produces p:

```
	p = Pb ,  P = aa·µÄ/a·µÄa    (7)
```

That is a column times a row -a square matrix- divided by the number a·µÄa .  (aa·µÄ ÊòØ‰∏Ä‰∏™Áß©1Áü©Èòµ)

This matrix has two properties that we will see as typical of projections:

 1. **P is a symmetric matrix**.    ( aa·µÄ ÂØπÁß∞)
 2. **Its square is itself: P¬≤= P**.


Every column is a multiple of a, and so is Pb = xÃÇa. The vectors that project to p = 0 are especially important. They satisfy a·µÄb = 0 -- they are perpendicular to a and their component along the line is zero. They lie in the nullspace , the perpendicular plane.

Actually that example is too perfect. It has the nullspace orthogonal to the column space, which is haywire. The nullspace should be orthogonal to the row space. But because P is symmetric, its row and column spaces are the same.

**Remark on scaling**: The projection matrix aa·µÄ/a·µÄa  is the same if a is doubled:

 - if a = [1 1 1]·µÄ , P ÁöÑÊâÄÊúâÂÖÉÁ¥† = 1/3 
 - if a = [2 2 2]·µÄ , P ÁöÑÊâÄÊúâÂÖÉÁ¥†‰ªçÁÑ∂ = 1/3 


The line through a is the same, and that's all the projection matrix cares about. If a has unit length, the denominator is a·µÄa = 1 and the matrix is just P = aa·µÄ .

Example 3: Project onto the "Œ∏-direction" in the x-y plane. The line goes through *a = (cosŒ∏, sinŒ∏)* and the matrix is symmetric with P¬≤ = P:

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_3.2_example3.png)

Here c is cosŒ∏, s is sinŒ∏, and c2 + s2 = 1 in the denominator. This matrix P was discovered in Section 2.6 on linear transformations. Now we know P in any number of dimensions. We emphasize that it produces the projection p:


***To project b onto a, multiply by the projection matrix P:  p = Pb***

---

**Transposes from Inner Products**

Finally we connect inner products to A·µÄ. Up to now, A·µÄ is simply the reflection of A across its main diagonal; the rows of A become the columns of A·µÄ, and vice versa. The entry in row i, column j of A·µÄ is the (j, i) entry of A:

 **Transpose by reflection**: (A·µÄ)·µ¢‚±º = (A)‚±º·µ¢

There is a deeper significance to A·µÄ. Its close connection to inner products gives a new and much more "abstract" definition of the transpose:

**3J** The transpose A·µÄ can be defined by the following property:  

 - The inner product of Ax with y equals the inner product Of x with A·µÄy. Formally, this simply means:

```
	(Ax)·µÄy = x·µÄA·µÄy = x·µÄ(A·µÄy)     (8)
```

This definition gives us another (better) way to verify the formula (AB)·µÄ = B·µÄA·µÄ. Use equation (8) twice:

```
Move A then move B: 
	(ABx)·µÄy = (Bx)·µÄ(A·µÄy) = x·µÄ(B·µÄA·µÄy)
```

The transposes turn up in reverse order on the right side, just as the inverses do in the formula (AB)‚Åª¬π = B‚Åª¬π A‚Åª¬π. We mention again that these two formulas meet to give the remarkable combination (A‚Åª¬π)·µÄ = (A·µÄ)‚Åª¬π .


<h2 id="4a2df06c2d276ad56402eecaa894e5d5"></h2>
## 3.3 PROJECTIONS LEAST SQUARES

Up to this point, Ax = b either has a solution or not. If b is not in the column space C(A), the system is inconsistent and Gaussian elimination fails. This failure is almost certain when there are several equations and only one unknown:

```
	More equations 	2x = b‚ÇÅ
	than unknowns-	3x = b‚ÇÇ
	no solution?	4x = b‚ÇÉ
```

This is solvable when b1, b2, b3 are in the ratio 2:3:4. The solution x will exist only if b is on the same line as the column a = (2, 3, 4).

In spite of their unsolvability, inconsistent equations arise all the time in practice. They have to be solved!  Rather than expecting no error in some equations and large errors in the others, it is much better to *choose the x that minimizes an average error E in the m equations*.

The most convenient "average" comes from the *sum of squares*:

```
Squared error  
	E¬≤ = (2x-b‚ÇÅ)¬≤ + (3x-b‚ÇÇ)¬≤ + (4x-b‚ÇÉ)¬≤ .
```

 - If there is an exact solution, the minimum error is E = 0. 
 - In the more likely case that b is not proportional to a, the graph of E¬≤ will be a parabola ( para 'beside' + bolƒì 'a throw' -> ÊäõÁâ©Á∫ø). The minimum error is at the lowest point, where the derivative is zero:

```
dE¬≤/dx = 2[(2x-b‚ÇÅ)2 +(3x-b‚ÇÇ)3 +(4x-b‚ÇÉ)4] = 0. 
```

Solving for x, the least-squares solution of this model system ax = b is denoted by xÃÇ :  (you need some calculus)

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_least_square_solution_example.png)  

**3K**:  The least-squares solution to a problem ax = b  in one unknon is xÃÇ = aa·µÄ/a·µÄa .

**Least-Squares Problems with Several Variables**

Now we are ready for the serious step, to ***project b onto a subspace*** . This problem arises from Ax = b when A is an m by n matrix.   The number m of observations is still larger than the number n of unknowns, so it must be expected that Ax = b will be inconsistent. *Probably, there will not exist a choice of x that perfectly fits the data b*. In other words, the vector b probably will not be a combination of the columns of A; it will be outside the column space.

Again the problem is to choose xÃÇ so as to minimize the error, and again this minimization will be done in the least-squares sense. The error is E = ‚ÄñAx - b‚Äñ, and ***this is exactly the distance from b to the point Ax in the column space***. Searching for the least-squares solution xÃÇ , which minimizes E, is the same as locating the point p = AxÃÇ that is closer to b than any other point in the column space.

We may use geometry or calculus to determine xÃÇ . In n dimensions, we prefer the appeal of geometry; p must be the "projection of b onto the column space." The error vector e = b - AxÃÇ must be perpendicular to that space (Figure 3.8). Finding xÃÇ and the projection p = AxÃÇ is so fundamental that we do it in two ways:

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_F3.8.png)

 1. All vectors perpendicular to the column space lie in the left nullspace. Thus the error vector e = b - AxÃÇ must be in the nullspace of A·µÄ:
 	- `A·µÄ(b-AxÃÇ) = 0  , or A·µÄAxÃÇ = A·µÄb`
 	- Âú®ÂùèÊñπÁ®ãÂ∑¶Âè≥‰πò‰∏ä A·µÄ ,Â∞±ÊòØÂ•ΩÊñπÁ®ã„ÄÇ A·µÄA ÊòØÊú¨Á´†ÁöÑÊ†∏ÂøÉ
 2. The error vector must be perpendicular to each column a‚ÇÅ , ... , an of A:
 	- ![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_errorVector_perpend_t_column.png)
 	- This is agin `A·µÄ(b-AxÃÇ) = 0  , or A·µÄAxÃÇ = A·µÄb`.

The fastest way is just to multiply the unsolvable equation Ax = b by A·µÄ. All these equivalent methods produce a square coefficient matrix A·µÄA. It is symmetric (its transpose is not AA·µÄ!) and it is the fundamental matrix of this chapter.

The equations A·µÄAxÃÇ = A·µÄb are known in statistics as the **normal equations**.


**3L**:  

 - When Ax b is inconsistent, its least-squares solution minimizes ‚ÄñAx - b‚Äñ¬≤  
 	- **normal equations** :  `A·µÄAxÃÇ = A·µÄb`     (1)
 - A·µÄA is invertible exactly when the columns of A are linearly independent! Then,
 	- **Best estimate xÃÇ**:  `xÃÇ = (A·µÄA)‚Åª¬πA·µÄb`		(2)
 - The projection of b onto the column space is the nearest point AxÃÇ:
 	- **Projection**:  `p = AxÃÇ = A¬∑(A·µÄA)‚Åª¬πA·µÄb` 	(3)
 	- `also:  p = Pb = A(A·µÄA)‚Åª¬πA·µÄ¬∑b`


Remark:

 1. Suppose b is actually in the column space of A. Then the projection of b is still b:
 	- **b in column space**:  *p = A(A·µÄA)‚Åª¬πA·µÄ¬∑Ax = Ax = b* .
 	- The closest point p is just b itself.
 2. At the other extreme, suppose b is perpendicular to every column, so A·µÄb = 0. In this case b projects to the zero vector
 	- **b in left nullspace**:  *p = A(A·µÄA)‚Åª¬πA·µÄ¬∑b = A(A·µÄA)‚Åª¬π 0 = 0*.
 3. When A is square and invertible, the column space is the whole space. Every vector projects to itself, p equals b, and x = xÃÇ:
 	- **If A is invertible**:  *p = A(A·µÄA)‚Åª¬πA·µÄ¬∑b = AA‚Åª¬π(A·µÄ)‚Åª¬πA·µÄ b = b*.
 4. Suppose A has only one column, containing a. Then the matrix A·µÄA is the number a·µÄa and xÃÇ is a·µÄb/a·µÄa. We return to the earlier formula.


**The Cross-Product Matrix of A·µÄA**

The matrix A·µÄA is certainly symmetric. Its transpose is (A·µÄA)·µÄ = A·µÄA·µÄ·µÄ, which is A·µÄA again. Its i, j entry (and j, i entry) is the inner product of column i of A with column j of A. The key question is the invertibility of A·µÄA, and fortunatelyÔºö

>**A·µÄA has the same nullspace as A.**

Certainly if Ax = 0 then A·µÄAx = 0. Vectors x in the nullspace of A are also in the nullspace of A·µÄA. 

To go in the other direction, start by supposing that A·µÄAx = 0, and take the inner product with x to show that Ax = 0:

```
  x·µÄA·µÄAx = 0,  or ‚ÄñAx‚Äñ¬≤ = 0 ,  or Ax = 0.
```

The two nullspaces are identical. In particular, if A has independent columns (and only x = 0 is in its nullspace), then the same is true for A·µÄA:

**3M** If A has independent columns, then A·µÄA is square, symmetric, and invertible.

We show later that A·µÄA is also positive definite (all pivots and eigenvalues are positive). This case is by far the most common and most important. Independence is not so hard in m-dimensional space if m > n. We assume it in what follows.

---

**Projection Matrices**

We have shown that the closest point to b is p = A(A·µÄA)‚Åª¬πA·µÄb. This formula expresses in matrix terms the construction of a perpendicular line from b to the column space of A. The matrix that gives p is a projection matrix, denoted by P:

```
  Projection Matrix:  P = A(A·µÄA)‚Åª¬πA·µÄ    (4)
```

 - This matrix projects any vector b onto the column space of A.  
	- In other words, p = Pb is the component of b in the column space, 
	- and the error e = b - Pb is the component in the orthogonal complement. 
 - I - P is also a projection matrix! 
 	- It projects b onto the orthogonal complement, 
 	- and the projection is b - Pb (e).

In short, we have a matrix formula for splitting any b into two perpendicular components. Pb is in the column space C(A), and the other component (I - P)b is in the left nullspace N(AT) - which is orthogonal to the column space.

These projection matrices can be understood geometrically and algebraically.

**3N** The projection matrix P = A(A·µÄA)‚Åª¬πA·µÄ has two basic properties:

 1. It equals its square: P¬≤ = P.
 2. It equals its transpose: P·µÄ = P

Conversely, any symmetric matrix with P¬≤ = P represents projection.

***Proof*** It is easy to see why P¬≤ = P. 

To prove that *P* is also symmetric, take its transpose. Multiply the transposes in reverse order, and use symmetry of (A·µÄA)‚Åª¬π, to come back to P:

```
	P·µÄ = (A·µÄ)·µÄ((A·µÄA)‚Åª¬π)·µÄA·µÄ 
	   = A((A·µÄA)·µÄ)‚Åª¬πA·µÄ 
	   = A(A·µÄA)‚Åª¬πA·µÄ = P.
```

For the converse, we have to deduce from P¬≤ = P and P·µÄ = P that Pb ***is the projection of b onto the column space of P***. The error vector b - Pb is orthogonal to the space. For any vector Pc in the space, the inner product is zero:

```
	// PS: (I-P)·µÄ = (I-P)
	(b-Pb)·µÄPc = b·µÄ(I-P)·µÄPc=b·µÄ(P-P¬≤)c = 0.  

```

Thus b - Pb is orthogonal to the space, and Pb is the projection onto the column space.

Suppose A is actually invertible. If it is 4 x 4, then its four columns are independent and its column space is all of R‚Å¥. What is the projection onto the whole space? It is the identity matrix.

```
  P = A(A·µÄA)‚Åª¬πA·µÄ = AA‚Åª¬π(A·µÄ)‚Åª¬πA·µÄ = I.  (5)
```

The identity matrix is symmetric, I¬≤ = I, and the error b - Ib is zero.

To repeat: We cannot invert the separate parts A·µÄ and A when those matrices are rectangular. It is the square matrix A·µÄA that is invertible , not A·µÄ and A.

---

**Least-Squares Fitting of Data**

Suppose we do a series of experiments, and expect the output b to be a linear function of the input t. We look for a ***straight line*** b = C + Dt. 

```
	C + Dt‚ÇÅ = b‚ÇÅ
	C + Dt‚ÇÇ = b‚ÇÇ
		...
	C + Dtm = bm
```

This is an *overdetermined* system, with m equations and only two unknowns. If errors are present, it will have no solution. A has two columns, and x = (C, D) : 

```
|1 t‚ÇÅ|        |b‚ÇÅ|
|1 t‚ÇÇ| |C|  = |b‚ÇÇ|   , or Ax = b.   (7)
| .  | |D|	  |. |
|1 tm|		  |bm|
```

The best solution (CÃÇ, DÃÇ) is the xÃÇ that minimizes the squared error E¬≤.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_F3.9.png)

The vector p = AxÃÇ is as close as possible to b. (Figure 3.9). On the graph, the errors are the ***vertical distances*** `b - C - Dt` to the straight line (not perpendicular distances!). It is the vertical distances that are squared, summed, and minimized.

```
	|1 -1| |C|	|1|
	|1  1|¬∑|D| =|1|.
	|1  2|		|3|
```

If those equations Ax = b could be solved, there would be no errors. They can't be solved because the points are not on a line. Therefore they are solved by least squares:

```bash
A·µÄAxÃÇ = A·µÄb  is :  |3 2|¬∑|CÃÇ|= |5|
				  |2 6| |DÃÇ|  |6|	
```

The best solution is CÃÇ = 9/7, DÃÇ = 4/7 and the best line is `9/7 + 4/7¬∑t`.

Note the beautiful connections between the two figures. The problem is the same but the art shows it differently. In Figure 3.9b, b is not a combination of the columns (1, 1, 1) and (-1, 1, 2). In Figure 3.9, the three points are not on a line. Least squares replaces points b that are not on a line by points p that are! ***Unable to solve Ax = b, we solve AxÃÇ = p***.

 - Figure 3.9b is in three dimensions (or m dimensions if there are m points) 
 - and Figure 3.9a is in two dimensions (or n dimensions if there are n parameters).

**3O√ü**  The measurements b‚ÇÅ, ..., bm are given at distinct points t‚ÇÅ , ..., tm. Then the straight line C + Dt which minimizes E¬≤ comes from least squares:

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_3O.png)
 
--- 

TODO **Weighted Least Squares**

---

<h2 id="09bbcb83a58fddde91f52c847cd92d46"></h2>
## 3.4 ORTHOGONAL BASES AND GRAM-SCHMIDT

In an orthogonal basis, every vector is perpendicular to every other vector. The coordinate axes are mutually orthogonal. That is just about optimal, and the one possible improvement is easy: Divide each vector by its length, to make it a unit vector. That changes an ***orthogonal*** basis into an ***orthonormal*** basis of q's:

**3P** The vectors q‚ÇÅ, ..., qn are ***orthonormal*** if 

```
q·µ¢·µÄq‚±º = ‚é∞ 0 whenever i ‚â† j, giving the orthogonality; 
		‚é± 1 whenever i = j, giving the normalization.
```

***A matrix with orthonormal columns will be called Q***. ( square or rectangular )
 
The most important example is the *standard basis*. For the x-y plane, the bestknown axes e‚ÇÅ = (1, 0) and e‚ÇÇ = (0, 1) are not only perpendicular but horizontal and vertical. Q is the 2 x 2 identity matrix. In n dimensions the standard basis e‚ÇÅ, ... , en again consists of *the columns of Q = I*:

```
			   |1|		 |0|		  	 |0|
Standard	   |0|		 |1|		  	 |0|
 basis	  e‚ÇÅ = |0|, e‚ÇÇ = |0|, ... , en = |0|
			   |.|		 |.|		  	 |.|
			   |0|		 |0|		  	 |1|
```

That is not the only orthonormal basis! We can rotate the axes without changing the right angles at which they meet. These rotation matrices will be examples of Q.

If we have a subspace of R‚Åø, the standard vectors e·µ¢ might not lie in that subspace. But the subspace always has an orthonormal basis, and it can be constructed in a simple way out of any basis whatsoever. This construction, which converts a skewed set of axes into a perpendicular set, is known as ***Gram-Schmidt orthogonalization***.

To summarize, the three topics basic to this section are:

 1. The definition and properties of orthogonal matrices Q.
 2. The solution of Qx = b, either n x n or rectangular (least squares).
 3. The Gram-Schmidt process and its interpretation as a new factorization A = QR.

**3Q**  If Q (square or rectangular) has orthonormal columns, then Q·µÄQ = I:

```
Orthonormal columns:
	‚é°-- q‚ÇÅ·µÄ --‚é§ ‚é°|  |  | ‚é§   ‚é°1 0 ¬∑ 0‚é§  
	‚é¢-- q‚ÇÇ·µÄ --‚é•¬∑‚é¢q‚ÇÅ q‚ÇÇ qn‚é• = ‚é¢0 1 ¬∑ 0‚é• = I.		(1)
	‚é¢         ‚é• ‚é¢|  |  | ‚é•   ‚é¢¬∑ ¬∑ ¬∑ ¬∑‚é•
	‚é£-- qn·µÄ --‚é¶ ‚é£|  |  | ‚é¶   ‚é£0 0 ¬∑ 1‚é¶
```

 - ***An orthogonal matrix is a square matrix with orthonormal columns.***  Then **Q·µÄ is Q‚Åª¬π** , ***the transpose is the inverse***.
 	-  *Orthonormal matrix* would have been a better name, but it is too late to change. 
 	- Also, there is no accepted word for a rectangular matrix with orthonormal columns. We still write Q, but we won't call it an "orthogonal matrix" unless it is square.
 	- Âè™ÊúâQÊòØÊñπÈòµÁöÑÊÉÖÂÜµ‰∏ãÔºåÊàë‰ª¨ÊâçÁß∞‰πã‰∏∫: Ê≠£‰∫§Áü©Èòµ
 	- orthonormal ÊòØ Q·µÄQ = I, Q·µÄ = Q‚Åª¬π ÁöÑÂâçÊèê‰πã‰∏Ä !
 - Note that Q·µÄQ = I even if Q is rectangular. But then Q·µÄ is only a left-inverse.

**3R** Multiplication by any Q preserves lengths:

```
Lengths unchanged: 
	‚ÄñQx‚Äñ = ‚Äñx‚Äñ  for ever vector x.  (2)
```

 - eg. rotations and reflection matrix.

It also preserves innerproducts and angles, since (Qx)·µÄ(Qy) = `x·µÄQ·µÄQy = x·µÄy`.

The preservation of lengths comes directly from Q·µÄQ = I:

```
		 ‚ÄñQx‚Äñ¬≤ = ‚Äñx‚Äñ¬≤  
because  (Qx)·µÄ(Qx) = x·µÄQ·µÄQx = x·µÄx.  (3)
```

We come now to the calculation that uses the special property Q·µÄ is Q‚Åª¬π. If we have a basis, then any vector is a combination of the basis vectors. This is exceptionally simple for an orthonormal basis, which will be a key idea behind Fourier series. The problem is to *find the coefficients of the basis vectors*:

&nbsp;&nbsp;***Write b as a combination b = x‚ÇÅq‚ÇÅ + x‚ÇÇq‚ÇÇ + ... + x<sub>n</sub>q<sub>n</sub>***

To compute x‚ÇÅ there is a neat trick. *Multiply both sides of the equation by q‚ÇÅ·µÄ*.  We are left with q‚ÇÅ·µÄb = x‚ÇÅq‚ÇÅ·µÄq‚ÇÅ. Since q‚ÇÅ·µÄq‚ÇÅ = 1, we have found x‚ÇÅ = q‚ÇÅb. 

Each piece of b has a simple formula, and recombining the pieces gives back b:

&nbsp;&nbsp;&nbsp;&nbsp;***Every vector b is equal to (q‚ÇÅ·µÄb)q‚ÇÅ + (q‚ÇÇ·µÄb)q‚ÇÇ + ... + (q<sub>n</sub>·µÄb)q<sub>n</sub> .  &nbsp;&nbsp;&nbsp;&nbsp;(4)***

I can't resist putting this orthonormal basis into a square matrix Q. The vector equation x‚ÇÅq‚ÇÅ + ... + x<sub>n</sub>q<sub>n</sub>= b is identical to Qx = b.  Its solution is x = Q‚Åª¬πb. But since Q‚Åª¬π = Q·µÄ -- this is where orthonormality enters -- the solution is also x = Q·µÄb:

```
		  ‚é°-- q‚ÇÅ·µÄ --‚é§‚é° ‚é§   ‚é°q‚ÇÅ·µÄb‚é§
x = Q·µÄb = ‚é¢         ‚é•‚é¢b‚é• = ‚é¢    ‚é•    (5)
		  ‚é£-- qn·µÄ --‚é¶‚é£ ‚é¶   ‚é£qn·µÄb‚é¶
```

***The components of x are the inner products*** **q·µ¢·µÄb** , as in equation (4) . 

**Remark 1**: The ratio a·µÄb/a·µÄa appeared earlier, when we projected b onto a line. Here a is q‚ÇÅ, the denominator is 1, and the projection is (q‚ÇÅ·µÄb)q‚ÇÅ. Thus we have a new interpretation for formula (4): *Every vector b is the sum of its one-dimensional projections onto the lines through the q's*.

 - b Á≠â‰∫é b Âú®ÂêÑ‰∏™Âü∫ÂêëÈáè‰∏äÊäïÂΩ±ÁöÑÂíå„ÄÇ

**Remark 2**: ***The rows of a square matrix are orthonormal whenever the columns are. ***

```
		‚é°1/‚àö3   1/‚àö2   1/‚àö6‚é§
	Q = ‚é¢1/‚àö3    0	  -2/‚àö6‚é•
		‚é£1/‚àö3  -1/‚àö2   1/‚àö6‚é¶  
```


---

**Rectangular Matrices with Orthonormal Columns**

This chapter is about Ax = b, when A is not necessarily square.

For Qx = b we now admit the same possibility -- there may be more rows than columns. The n orthonormal vectors q·µ¢ in the columns of Q have m > n components. Then Q is an m x n matrix and we cannot expect to solve Qx = b exactly. *We solve it by least squares*.

If there is any justice, orthonormal columns should make the problem simple. It worked for square matrices, and now it will work for rectangular matrices. The key is to notice that we still have Q·µÄQ = I. So Q·µÄ is still the ***left-inverse*** of Q.

For least squares that is all we need. Now the normal equations are Q·µÄQxÃÇ = Q·µÄb. But Q·µÄQ is the identity matrix!  Therefore xÃÇ = Q·µÄb, whether Q is square and xÃÇ is an exact solution, or Q is rectangular and we need least squares.

**3S** If Q has orthonormal columns, the least-squares problem becomes easy:

```
  Qx = b   , rectangular system with no solution for most b.
Q·µÄQxÃÇ = Q·µÄb , normal equation for the best xÃÇ -- in which Q·µÄQ = I.
   xÃÇ = Q·µÄb , xÃÇ·µ¢ = q·µ¢·µÄb 
   p = QxÃÇ  , the projection of b is (q‚ÇÅ·µÄb)q‚ÇÅ + ... + (qn·µÄb)qn.
   p = QQ·µÄb, the projection matrix is P = QQ·µÄ ( here QQ·µÄ ‚â† I )
```

PS. For rectangular matrics , QQ·µÄ ‚â† I :

```octave
octave:11> Q = [1 0 ; 0 1 ; 0 0 ]
Q =

   1   0
   0   1
   0   0

octave:12> Q'*Q
ans =

   1   0
   0   1

octave:13> Q*Q'
ans =

   1   0   0
   0   1   0
   0   0   0
```

```
The projection matrix: 
	P = Q(Q·µÄQ)‚Åª¬πQ·µÄ ,  or P = QQ·µÄ     (7)
```

QQ·µÄ is the zero matrix on the othogonal complement ( the nullspace of Q·µÄ ).  -- ‰∏çÁêÜËß£

**Example 3** The following case is simple but typical. Suppose we project a point b = (x,y,z) onto the x-y plane. It's projection is p = (x,y,0), and this is the sum of the separate projections on the x- and y-axes:

```
		 ‚é°1‚é§				  ‚é°x‚é§	
	q‚ÇÅ = ‚é¢0‚é•  and  (q‚ÇÅ·µÄb)q‚ÇÅ = ‚é¢0‚é•; 	
		 ‚é£0‚é¶  				  ‚é£0‚é¶  

		 ‚é°0‚é§				  ‚é°0‚é§	
	q‚ÇÇ = ‚é¢1‚é•  and  (q‚ÇÇ·µÄb)q‚ÇÇ = ‚é¢y‚é•.
		 ‚é£0‚é¶  				  ‚é£0‚é¶  
```

The overall projection matrix is :

```
					‚é°1 0 0‚é§			‚é°x‚é§	  ‚é°x‚é§	
P = q‚ÇÅq‚ÇÅ·µÄ + q‚ÇÇq‚ÇÇ·µÄ = ‚é¢0 1 0‚é•,  and  P‚é¢y‚é• = ‚é¢y‚é• 
					‚é£0 0 0‚é¶  		‚é£z‚é¶	  ‚é£0‚é¶  
```

***Projection onto a plane = sum of projections onto orthonormal q‚ÇÅ and q‚ÇÇ***.

Orthogonal columns are so much better that it is worth changing to that case. And it leads to a factorization A = QR that is nearly as famous as A = LU.

---

**The Gram-Schmidt Process**

Now we propose to find a way to ***make*** them orthogonal.

The method is simple. We are given a,b,c and we want q‚ÇÅ,q‚ÇÇ,q‚ÇÉ. There is no problem with q‚ÇÅ: it can go in the direction of a. We divide by the length , so that q‚ÇÅ = a/‚Äña‚Äñ is a unit vector. The real problem begins with q‚ÇÇ -- which has to be orthogonal to q‚ÇÅ. If the second vector b has any component in the direction of q‚ÇÅ (or a) , ***that component has to be subtracted***:

```
Second Vector  	
	B = b - (q‚ÇÅ·µÄb)q‚ÇÅ , and q‚ÇÇ = B/‚ÄñB‚Äñ.     (9)
```

B is orthogonal to q‚ÇÅ. It is the part of b that goes in a new direction , and not in the direction of a.

![](https://raw.githubusercontent.com/mebusy/notes/master/imgs/LA_F3.10.png)

At this point q‚ÇÅ and q‚ÇÇ are set. The third orthogonal direction starts with c. It will not be in the plance of q‚ÇÅ and q‚ÇÇ, which is the plane of a and b. However, it may have a component in that plane, and that has to be substracted. What is left is the component C we want, the part that is in a new direction perpendicular to the plane:

```
Third Vector 	
C = c -(q‚ÇÅ·µÄc)q‚ÇÅ -(q‚ÇÇ·µÄc)q‚ÇÇ , and q‚ÇÉ = C/‚ÄñC‚Äñ.  (10)
```

This is the one idea of the whole Gram-Schmidt process, ***to subtract from every new vector  its components in the directions that are already settled***. That idea is used over and over again. When there is a 4th vector, we subtract away its components in the direction of q‚ÇÅ,
q‚ÇÇ,q‚ÇÉ.


**Example 5 Gram-Schmidt** Suppose the independent vectors are a, b, c:

```
	‚é°1‚é§      ‚é°1‚é§      ‚é°2‚é§         
a = ‚é¢0‚é•, b = ‚é¢0‚é•, c = ‚é¢1‚é•.
	‚é£1‚é¶      ‚é£0‚é¶      ‚é£0‚é¶         

	 ‚é°1/‚àö2‚é§
q‚ÇÅ = ‚é¢ 0  ‚é•.
	 ‚é£1/‚àö2‚é¶
```

```
	‚é°1‚é§       ‚é°1/‚àö2‚é§       ‚é° 1‚é§
B = ‚é¢0‚é• - 1/‚àö2‚é¢ 0  ‚é•  = 1/2‚é¢ 0‚é•.
	‚é£0‚é¶       ‚é£1/‚àö2‚é¶       ‚é£-1‚é¶

	 ‚é° 1/‚àö2‚é§
q‚ÇÇ = ‚é¢  0  ‚é•.
	 ‚é£-1/‚àö2‚é¶	
```

```
	‚é°2‚é§     ‚é°1/‚àö2‚é§     ‚é° 1/‚àö2‚é§    ‚é°0‚é§
C = ‚é¢1‚é• - ‚àö2‚é¢ 0  ‚é• - ‚àö2‚é¢  0  ‚é•  = ‚é¢1‚é•.
	‚é£0‚é¶     ‚é£1/‚àö2‚é¶     ‚é£-1/‚àö2‚é¶    ‚é£0‚é¶   
```


**3T** The Gram-Schmidt process starts with independent vectors a‚ÇÅ, ... , a<sub>n</sub>	and end with orthogonal vectors q‚ÇÅ, ... , q<sub>n</sub>. At step j it substracts from a‚±º its components in the directions q‚ÇÅ, ... , q‚±º‚Çã‚ÇÅ that are already settled:

```
A‚±º = a‚±º -(q‚ÇÅ·µÄa‚±º)q‚ÇÅ - ... -(q‚±º‚Çã‚ÇÅ·µÄa‚±º)q‚±º‚Çã‚ÇÅ.  (11)
```

*Remark on the calculations*  I think it is easier to compute the orthogonal a, B, C, without forcing their lengths to equal one. Then square roots enter only at the end, when dividing by those lengths. The example above would have the same B and C, without using square roots.

*Notice the 1/2 from a·µÄb/a·µÄa instead of 1/‚àö2 from q·µÄb*.

```
	‚é°1‚é§      ‚é°1‚é§    ‚é° 1/2‚é§
B = ‚é¢0‚é• - 1/2‚é¢0‚é•  = ‚é¢  0 ‚é•,
	‚é£0‚é¶      ‚é£1‚é¶    ‚é£-1/2‚é¶

	‚é°2‚é§   ‚é°1‚é§    ‚é° 1/2‚é§    ‚é°0‚é§
C = ‚é¢1‚é• - ‚é¢0‚é• - 2‚é¢  0 ‚é•  = ‚é¢1‚é•.
	‚é£0‚é¶   ‚é£1‚é¶    ‚é£-1/2‚é¶    ‚é£0‚é¶   
```



---

**The Factorization A = QR**

We started with a matrix A, whose columns were a, b, c. We ended with a matrix Q, whose columns are q‚ÇÅ,q‚ÇÇ,q‚ÇÉ. What is the relation between those matrices? The matrices A and Q are m x n when the n vectors are in m-dimensional space, and there has to be a third matrix that connects them.

The idea is to write the a's as combinations of the q's. The vector b in Figure 3.10 is a combination of the orthonormal q‚ÇÅ and q‚ÇÇ, and we know what combination it is:

```
	b = (q‚ÇÅ·µÄb)q‚ÇÅ + (q‚ÇÇ·µÄb)q‚ÇÇ.
```

Every vector in the plane is the sum of its q‚ÇÅ and q‚ÇÇ components. Similarly c is the sum of its q‚ÇÅ,q‚ÇÇ,q‚ÇÉ components: 

`c = (q‚ÇÅ·µÄc)q‚ÇÅ + (q‚ÇÇ·µÄc)q‚ÇÇ + (q‚ÇÉ·µÄc)q‚ÇÉ`. 

If we express that in matrix form we have ***the new factorization A = QR***:

```
QR factors :

	‚é°     ‚é§   ‚é°        ‚é§‚é°q‚ÇÅ·µÄa q‚ÇÅ·µÄb q‚ÇÅ·µÄc‚é§
A = ‚é¢a b c‚é• = ‚é¢q‚ÇÅ q‚ÇÇ q‚ÇÉ‚é•‚é¢     q‚ÇÇ·µÄb q‚ÇÇ·µÄc‚é• = QR.	(12)
	‚é£     ‚é¶   ‚é£        ‚é¶‚é£          q‚ÇÉ·µÄc‚é¶
```

R is upper triangular because of the way Gram-Schmidt was done. The first vectors a and q‚ÇÅ fell on the same line. Then q‚ÇÅ, q‚ÇÇ were in the same plane as a, b. The third vectors c and q‚ÇÉ were not involved until step 3.

The QR factorization is like A = LU, except that the first factor Q has orthonormal columns. The second factor is called R, because the nonzeros are to the *right* of the diagonal (and the letter U is already taken). 

```
	‚é°1 1 2‚é§   ‚é°1/‚àö2  1/‚àö2  0 ‚é§‚é°‚àö2  1/‚àö2  ‚àö2‚é§
A = ‚é¢0 0 1‚é• = ‚é¢  0    0    1 ‚é•‚é¢    1/‚àö2  ‚àö2‚é• = QR.
	‚é£1 0 0‚é¶   ‚é£1/‚àö2 -1/‚àö2  0 ‚é¶‚é£          1 ‚é¶
```

*You see the lengths of a, B, C on the diagonal of R*. The orthonormal vectors q‚ÇÅ,q‚ÇÇ,q‚ÇÉ, which are the whole object of orthogonalization, are in the first factor Q. 

The entries r·µ¢‚±º = q·µ¢·µÄa‚±º.


**3U** Every m x n matrix with independent columns canbe factored into A = QR. The columns of Q are orthonormal, and R is upper triangular and invertible. When m = n and all matrices are square, Q becomes an orthogonal matrix.

I must not forget the main point of orthogonalization. It simplifies the least-squares problem Ax = b. The normal equations are still correct, but A·µÄA becomes easier:

```
A·µÄA = R·µÄQ·µÄQR = R·µÄR. 	(14)
```

The fundamental equation A·µÄAxÃÇ = A·µÄb simplifies to a triangular system:

```
R·µÄRxÃÇ = R·µÄQ·µÄb ,   or  RxÃÇ = Q·µÄb.	 (15)
```
Instead of solving QRx = b, which can't be done, we solve RxÃÇ = Q·µÄb, which is just back-substitution because R is triangular. The real cost is the mn¬≤ operations of Gram-Schmidt, which are needed to find Q and R in the first place.

The same idea of orthogonality applies to functions. The sines and cosines are orthogonal; the powers 1, x, x¬≤ are not. When f(x) is written as a combination of sines and cosines, that is a ***Fourier series***. Each term is a projection onto a line -- the line in function space containing multiples of cosnx or sinnx. It is completely parallel to the vector case, and very important. And finally we have a job for Schmidt: To orthogonalize the powers of x and produce the Legendre polynomials.

---

**Function Spaces and Fourier Series**

This is a brief and optional section, but it has a number of good intentions:

 1. to introduce the most famous infinite-dimensional vector space (Hilbert space);
 2. to extend the ideas of length and inner product from vectors v to functions f(x);
 3. to recoginze the Fourier series as a sum of one-dimensional projections ( the orthogonal "columns" are the sines and cosines);
 4. to apply Gram-Schmidt orthogonalization to the polynomials 1, x, x¬≤, ... ; and
 5. to find the best approximation to f(x) by a straight line.

We will try to follow this outline, which opens up a range of new-applications for linear algebra, in a systematic way.

 1. Hilbert space
 	- After studying R‚Åø, it is natural to think of the space R<sup>‚àû</sup>. The infinite series must converge to a finite sum. This leaves (1, 1/2, 1/3, ...) but not (1, 1, 1, ...).  
 	- Hilbert space is the natural way to let the number of dimensions become infinite, and at the same time to keep the geometry of ordinary Euclidean space. Ellipses "..." become infinite-dimensional ellipsoids, and perpendicular lines are recognized exactly as before. The vectors v and w are orthogonal when their inner product is zero: 
 	- `v·µÄw = v‚ÇÅw‚ÇÅ + v‚ÇÇw‚ÇÇ + v‚ÇÉw‚ÇÉ + ... = 0`.
 	- This snm is guaranteed to converge, and for any two vectors it still obeys the Schwarz inequality `‚Äñv·µÄw‚Äñ < ‚Äñv‚Äñ‚Äñw‚Äñ` . The cosine, even in Hilbert space, is never larger than 1.
 	- There is another remarkable thing about this space: It is found under a great many different disguises. Its "vectors" can turn into functions, which is the second point.

 2. Lengths and Inner Products.
 	- Suppose f(x) = sin x on the interval 0 < x < 2ùúã.




TODO


---





















